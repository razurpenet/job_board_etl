{"status": "OK", "request_id": "96e84d08-0a38-4a34-8c1b-1c2152fba414", "parameters": {"query": "data engineer jobs in usa", "page": 2, "num_pages": 4, "date_posted": "today"}, "data": [{"employer_name": "The Walt Disney Company", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Disney_wordmark.svg/640px-Disney_wordmark.svg.png", "employer_website": "https://thewaltdisneycompany.com", "employer_company_type": "Entertainment", "job_publisher": "Geebo", "job_id": "iNM1iaIzALZGCAyWAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer at The Walt Disney Company in Orlando, FL", "job_apply_link": "https://geebo.com/jobs-online/view/id/1081399461-data-engineer-at-the-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4537, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1081399461-data-engineer-at-the-/", "is_direct": true}], "job_description": "Disney Data Engineering teams leverage cutting-edge technology, immersive storytelling and imaginative thinking to create innovative, one-of-a-kind creations for Guests and consumers around the globe. As part of the Data Engineering team within Disney Parks, Experiences and Products, you will be using some of the most unique and robust data in the world! All of this incredible data is a key enabler behind:\nDisney's world class, personalized Guest experience Interactive and immersive experiences Optimizing Parks & Resorts operations Vacation planning Disney Cruise Line - pricing, booking and experience Keeping our Guests safe Keeping our attractions running Product development and pricing Marketing and communication Consumer Product ecommerce Responsibilities :\nAs a Data Engineer, you will develop custom data pipelines across a wide variety of technologies. You will have the opportunity to engineer high-performance and large-scale data engineer projects, ensure solutions support all functional and non-functional requirements and develop production grade, consumable data views. The Data Engineer will also participate in operational support and maintenance of products/services as well as coordinate and collaborate with offshore teams. Design and engineer high-performance/large scale data engineering projects, producing maintainable and secure code with automated testing in a continuous integration environment. Develop production grade, consumable data views Ensure solutions support all functional and non-functional requirements Participate in operational support and maintenance of products and services Ability to participate in discovery processes with stakeholders to identify business requirements and expected outcomes. Coordinate and collaborate with offshore teams Basic Qualifications :\nRelevant cloud data engineering work experience Ability to perform across multiple phases of development for multiple complex projects, including technical design, build, and end-to-end testing. Passionate about delivering data engineering projects and features in a team environment. Demonstrate the ability to quickly learn new technologies Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and ability to exercise sound judgment and initiative in stressful situations. Strong oral and written communication and interpersonal skills Must have technical\nQualifications:\nFundamentals of data pipelining, ELT/ETL, data architecture, and the overall data lifecycle Cross-platform development languages:\nPython preferred (Java specialty OK) SQL and scripting proficiency Relational database and NoSQL (ex:\nMongoDB, DynamoDB, Redis, HBase, Cassandra) database experience. Cloud technologies including AWS or Google Cloud Platform (GCP) Preferred\nQualifications:\nSnowflake data warehouse exposure and experience Queuing Technology - Kafka, RabbitMQ, Redis, SQS, Kinesis Streams, Kinesis Firehose Data Processing - EMR, Spark, Glue, Spark Streaming/Flink Containers - Docker, Docker Swarm, Docker Applications CICD - Jenkins/Codebuild/GitLab Security - IAM roles, wire encryption, KMS, Kerberos, Authz, AD Infrastructure as Code - Terraform, Cloud Formation, CDK Required Education :\nBachelor's degree in Computer Science, Engineering, Information Technology, or related field OR equivalent work experience Additional Information :\nDISNEYTECH\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Orlando", "job_state": "FL", "job_country": "US", "job_latitude": 28.538383, "job_longitude": -81.37893, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=iNM1iaIzALZGCAyWAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Relevant cloud data engineering work experience Ability to perform across multiple phases of development for multiple complex projects, including technical design, build, and end-to-end testing", "Passionate about delivering data engineering projects and features in a team environment", "Demonstrate the ability to quickly learn new technologies Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and ability to exercise sound judgment and initiative in stressful situations", "Strong oral and written communication and interpersonal skills Must have technical", "Fundamentals of data pipelining, ELT/ETL, data architecture, and the overall data lifecycle Cross-platform development languages:", "MongoDB, DynamoDB, Redis, HBase, Cassandra) database experience", "Snowflake data warehouse exposure and experience Queuing Technology - Kafka, RabbitMQ, Redis, SQS, Kinesis Streams, Kinesis Firehose Data Processing - EMR, Spark, Glue, Spark Streaming/Flink Containers - Docker, Docker Swarm, Docker Applications CICD - Jenkins/Codebuild/GitLab Security - IAM roles, wire encryption, KMS, Kerberos, Authz, AD Infrastructure as Code - Terraform, Cloud Formation, CDK Required Education :", "Bachelor's degree in Computer Science, Engineering, Information Technology, or related field OR equivalent work experience Additional Information :", "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"], "Responsibilities": ["As a Data Engineer, you will develop custom data pipelines across a wide variety of technologies", "You will have the opportunity to engineer high-performance and large-scale data engineer projects, ensure solutions support all functional and non-functional requirements and develop production grade, consumable data views", "The Data Engineer will also participate in operational support and maintenance of products/services as well as coordinate and collaborate with offshore teams", "Design and engineer high-performance/large scale data engineering projects, producing maintainable and secure code with automated testing in a continuous integration environment", "Develop production grade, consumable data views Ensure solutions support all functional and non-functional requirements Participate in operational support and maintenance of products and services Ability to participate in discovery processes with stakeholders to identify business requirements and expected outcomes", "Coordinate and collaborate with offshore teams Basic Qualifications :"], "Benefits": ["$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "713", "job_naics_name": "Amusement, Gambling, and Recreation Industries"}, {"employer_name": "Insight Global", "employer_logo": "https://images.squarespace-cdn.com/content/5f7f984c3ca20d1d55b276f7/1619793549819-L54X3W9Z5RD277UPNH9S/IGLogoPublic.png?content-type=image%2Fpng", "employer_website": "http://www.insightglobal.com", "employer_company_type": "Staffing", "job_publisher": "Jobs Trabajo.org", "job_id": "9HHPMaNLuHc4X5B3AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer", "job_apply_link": "https://us.trabajo.org/job-640-20231221-7ba628ebb7aacfa2cb6102e3b7ea8693", "job_apply_is_direct": false, "job_apply_quality_score": 0.4387, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-640-20231221-7ba628ebb7aacfa2cb6102e3b7ea8693", "is_direct": false}], "job_description": "We are now seeking an experienced Data Engineer to support the day-to-day operations of DISH Media Data & Analytics. As a Data Engineer you will work collaboratively with internal teams and external partners to design, develop, and maintain tools and automation workflows to drive our Media Sales Operations. You will gather and document your own requirements from the stakeholders and business, design and build ETL jobs and Python-based tools and processes that solve operational goals and automate for efficiencies.\n\n- AWS; glue, redshift (database application)\n- Undergraduate Degree in Computer Science, Mathematics or a STEM related field\n\n- 5+ years of data engineering or software development experience (2+ years with strong AWS experience or AWS certification)\n\n- 5+ years hands-on development experience using Python, including extensive use of the standard module library and advanced modules such as Pandas and Numpy\n\n- Ability and experience running full SDLC on small Data Projects; gathering requirements, documenting into business and technical, Design, Engineer, Test, Deploy, Maintain\n\n- Experience with AWS services, including but not limited to S3, EC2, Data Pipeline, Lambda, CloudFormation, EventBridge, SNS, SQS and Athena\n\n- Strong knowledge of SQL-based analysis and data architecture\n\n- Experience with version control tools such as GitLab\n\n- Experience working in a Linux environment doing command line scripting", "job_is_remote": false, "job_posted_at_timestamp": 1703125680, "job_posted_at_datetime_utc": "2023-12-21T02:28:00.000Z", "job_city": "Denver", "job_state": "CO", "job_country": "US", "job_latitude": 39.739235, "job_longitude": -104.99025, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=9HHPMaNLuHc4X5B3AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Undergraduate Degree in Computer Science, Mathematics or a STEM related field", "5+ years of data engineering or software development experience (2+ years with strong AWS experience or AWS certification)", "5+ years hands-on development experience using Python, including extensive use of the standard module library and advanced modules such as Pandas and Numpy", "Ability and experience running full SDLC on small Data Projects; gathering requirements, documenting into business and technical, Design, Engineer, Test, Deploy, Maintain", "Experience with AWS services, including but not limited to S3, EC2, Data Pipeline, Lambda, CloudFormation, EventBridge, SNS, SQS and Athena", "Strong knowledge of SQL-based analysis and data architecture", "Experience with version control tools such as GitLab", "Experience working in a Linux environment doing command line scripting"], "Responsibilities": ["As a Data Engineer you will work collaboratively with internal teams and external partners to design, develop, and maintain tools and automation workflows to drive our Media Sales Operations", "You will gather and document your own requirements from the stakeholders and business, design and build ETL jobs and Python-based tools and processes that solve operational goals and automate for efficiencies", "AWS; glue, redshift (database application)"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "561311", "job_naics_name": "Employment Placement Agencies"}, {"employer_name": "Mastercard", "employer_logo": "https://www.mastercard.us/content/dam/public/mastercardcom/na/us/en/homepage/Home/mc-logo-52.svg", "employer_website": "http://www.mastercard.com", "employer_company_type": "Finance", "job_publisher": "Careers At Mastercard", "job_id": "jAh1475wbj5ruKpSAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Lead Software Engineer/Data Engineer", "job_apply_link": "https://careers.mastercard.com/us/en/job/R-206857/Lead-Software-Engineer-Data-Engineer", "job_apply_is_direct": false, "job_apply_quality_score": 0.8995, "apply_options": [{"publisher": "Careers At Mastercard", "apply_link": "https://careers.mastercard.com/us/en/job/R-206857/Lead-Software-Engineer-Data-Engineer", "is_direct": false}, {"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Mastercard,-Inc./Job/Lead-Software-Engineer-Data-Engineer/-in-O'fallon,MO?jid=32f5dd7147694ec9", "is_direct": false}, {"publisher": "Professional Diversity Network", "apply_link": "https://www.prodivnet.com/job/lead-software-engineer-data-engineer-o-fallon-missouri-13675033", "is_direct": false}, {"publisher": "The Muse", "apply_link": "https://www.themuse.com/jobs/mastercardinternationalincorporated/lead-software-engineerdata-engineer", "is_direct": false}, {"publisher": "WIRED HIRED", "apply_link": "https://jobs.wired.co.uk/job/lead-software-engineerdata-engineer-at-mastercard-2", "is_direct": false}, {"publisher": "Adzuna", "apply_link": "https://www.adzuna.com/details/4357576691", "is_direct": false}, {"publisher": "Fairygodboss", "apply_link": "https://fairygodboss.com/jobs/mastercard-incorporated/lead-software-engineer-data-engineer-b4245e0c29619534c1c5971800844da5", "is_direct": false}, {"publisher": "Ladders", "apply_link": "https://www.theladders.com/job/lead-software-engineer-data-engineer-mastercard-o-fallon-mo_65172359", "is_direct": false}], "job_description": "Job Title:\n\nLead Software Engineer/Data Engineer\n\nOverview:\n\nOverview\n\nMastercard is the global technology company behind the world\u2019s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless\u00ae. We ensure every employee can be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\nData Engineering and platform under Data Platform and Engineering service program is focused on enabling insights into Mastercard network and help build data-driven products by curating and preparing data in a secure and reliable manner. Moving to a \u201cUnified and Fault-Tolerant Architecture for Data Ingestion and Processing\u201d is critical to achieving this mission.\nAs a Lead Site Reliability / DevOps Engineer in Data Platform and Engineering service program, you will have the opportunity to build high performance data pipelines to load into Mastercard Data Warehouse. Our Data Warehouse provides analytical capabilities to number of business users who help different customers provide answer to their business problems through data. You will play a vital role within a rapidly growing organization, while working closely with experienced and driven engineers to solve challenging problems.\n\nYour Role\n\u2022 Partner on the design the next implementation of Mastercard secure, global data and insight architecture, building the data integration and processing capabilities and operationalizing \u201cUnified Data Acquisition and Processing (UDAP) platform\u201d\n\u2022 Identify and resolve performance bottlenecks either proactively\n\u2022 Work with the customer support group as needed to resolve performance issues in the field\n\u2022 Explore automation opportunity and develop tools to automate some of the day to day operations tasks\n\u2022 Provide performance metrics and maintain dashboards to reflect production systems health\n\u2022 Conceptualize and implement proactive monitoring where possible to catch issues early\n\u2022 Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.\n\u2022 Work with cross functional agile teams to drive projects through full development cycle.\n\u2022 Help the team improve with the usage of data engineering best practices.\n\u2022 Collaborate with other data engineering teams to improve the data engineering ecosystem and talent within Mastercard.\n\u2022 Creatively solve problems when facing constraints, whether it is the number of developers, quality or quantity of data, compute power, storage capacity or just time.\n\u2022 Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.\n\nAll About You\n\u2022 At least Bachelor's degree in Computer Science, Computer Engineering or Technology related field or equivalent work experience\n\u2022 Experience in Data Warehouse related projects in product or service-based organization\n\u2022 Experience as a Site Reliability Engineering or DevOps Engineer\n\u2022 Experience overall with experience as a software engineer or software architect\n\u2022 Experience solving for scalability, Performance and stability\n\u2022 Expert knowledge of Linux operating systems and environment and Scripting (Shell and Python preferred)\n\u2022 A deep expertise in your field of Software Engineering\n\u2022 Expert at troubleshooting complex system and application stacks\n\u2022 Operational Experience in Big Data Stacks ( Hadoop ecosystem, Spark is a plus)\n\u2022 Operational Experience in real-time ,streaming and data pipelines relevant frameworks ( Kafka and NiFi is a plus)\n\u2022 Operational experience troubleshooting network/server communication\n\u2022 Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts\n\u2022 Expertise in enterprise metrics/monitoring with frameworks such as Splunk, Druid, Grafana\n\u2022 Experience with cloud computing services, particularly deploying and running services in Azure or AWS\n\u2022 A belief in data driven analysis and problem solving and a proven track record in applying these principles\n\u2022 An organized approach the planning and execution of major projects", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "O'Fallon", "job_state": "MO", "job_country": "US", "job_latitude": 38.810608, "job_longitude": -90.699844, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=jAh1475wbj5ruKpSAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": null, "job_offer_expiration_timestamp": null, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["At least Bachelor's degree in Computer Science, Computer Engineering or Technology related field or equivalent work experience", "Experience in Data Warehouse related projects in product or service-based organization", "Experience as a Site Reliability Engineering or DevOps Engineer", "Experience overall with experience as a software engineer or software architect", "Experience solving for scalability, Performance and stability", "A deep expertise in your field of Software Engineering", "Expert at troubleshooting complex system and application stacks", "Operational experience troubleshooting network/server communication", "Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts", "Expertise in enterprise metrics/monitoring with frameworks such as Splunk, Druid, Grafana", "Experience with cloud computing services, particularly deploying and running services in Azure or AWS", "A belief in data driven analysis and problem solving and a proven track record in applying these principles", "An organized approach the planning and execution of major projects"], "Responsibilities": ["Moving to a \u201cUnified and Fault-Tolerant Architecture for Data Ingestion and Processing\u201d is critical to achieving this mission", "As a Lead Site Reliability / DevOps Engineer in Data Platform and Engineering service program, you will have the opportunity to build high performance data pipelines to load into Mastercard Data Warehouse", "Our Data Warehouse provides analytical capabilities to number of business users who help different customers provide answer to their business problems through data", "You will play a vital role within a rapidly growing organization, while working closely with experienced and driven engineers to solve challenging problems", "Partner on the design the next implementation of Mastercard secure, global data and insight architecture, building the data integration and processing capabilities and operationalizing \u201cUnified Data Acquisition and Processing (UDAP) platform\u201d", "Identify and resolve performance bottlenecks either proactively", "Work with the customer support group as needed to resolve performance issues in the field", "Explore automation opportunity and develop tools to automate some of the day to day operations tasks", "Provide performance metrics and maintain dashboards to reflect production systems health", "Conceptualize and implement proactive monitoring where possible to catch issues early", "Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines", "Work with cross functional agile teams to drive projects through full development cycle", "Help the team improve with the usage of data engineering best practices", "Collaborate with other data engineering teams to improve the data engineering ecosystem and talent within Mastercard", "Creatively solve problems when facing constraints, whether it is the number of developers, quality or quantity of data, compute power, storage capacity or just time", "Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_occupational_categories": ["Software Engineering"], "job_naics_code": "522110", "job_naics_name": "Commercial Banking"}, {"employer_name": "Prescient Edge", "employer_logo": null, "employer_website": "http://www.prescientedge.com", "employer_company_type": null, "job_publisher": "ZipRecruiter", "job_id": "VxoA0o_1chGNszviAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer", "job_apply_link": "https://www.ziprecruiter.com/c/Prescient-Edge/Job/Data-Engineer/-in-Fort-Bragg,NC?jid=1b9e10357de40ac1", "job_apply_is_direct": false, "job_apply_quality_score": 0.7187, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Prescient-Edge/Job/Data-Engineer/-in-Fort-Bragg,NC?jid=1b9e10357de40ac1", "is_direct": false}, {"publisher": "Salary.com", "apply_link": "https://www.salary.com/job/prescient-edge/data-engineer/j202305301632402160298", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=db8fec20eba9dde6", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-prescient-edge-JV_IC1138721_KO0,13_KE14,28.htm?jl=1007689666226", "is_direct": false}, {"publisher": "SimplyHired", "apply_link": "https://www.simplyhired.com/job/R_oGwvSfGx9Edza4TBiK2cU9KM2stoXnfiyGPw0tI_V9OKtFjADLyg", "is_direct": false}, {"publisher": "Prescient Edge - Talentify", "apply_link": "https://prescient-edge.talentify.io/job/data-engineer-fort-bragg-north-carolina-prescient-edge-8190", "is_direct": false}, {"publisher": "BeBee", "apply_link": "https://us.bebee.com/job/20231212-ddf57f5a869120904ca1ecda6cd83b6c", "is_direct": false}, {"publisher": "Fort Bragg, NC - Geebo", "apply_link": "https://fortbragg-nc.geebo.com/jobs-online/view/id/1124243136-data-engineer-/", "is_direct": true}], "job_description": "Job Title\n\nData Engineer\n\nLocation\n\nFort Liberty, NC 28307 US (Primary)\n\nCategory\n\nResearch, Development, and Engineering\n\nJob Type\n\nFull-Time\n\nCareer Level\n\nStaff\n\nEducation\n\nBachelor's Degree\n\nTravel\n\nNone\n\nSecurity Clearance Required\n\nTS/SCI\n\nJob Description\n\nPrescient Edge is seeking a Data Engineerto support a federal government client.\n\nBenefits:\n\nAt Prescient Edge, we believe that acting with integrity and serving our employees is the key to everyone's success. To that end, we provide employees with a best-in-class benefits package that includes:\n\u2022 A competitive salary with performance bonus opportunities.\n\u2022 Comprehensive healthcare benefits, including medical, vision, dental, and orthodontia coverage.\n\u2022 A substantial retirement plan with no vesting schedule.\n\u2022 Career development opportunities, including on-the-job training, tuition reimbursement, and networking.\n\u2022 A positive work environment where employees are respected, supported, and engaged.\n\nSecurity Clearance:\n\u2022 Current DOD TS/SCI security clearance.\n\nJob Requirements\n\nExperience:\n\u2022 5+ years of experience in one or more of the following: Business Analysis, Army Special Operations, Intelligence and/ or Information Management/ Knowledge Management.\n\u2022 5+ years of Experience supporting the United States Military, preferably SOF elements.\n\u2022 5+ years of experience with Single Page Application Development and client-side coding, including Jscript, React, Angular, Aurelia, Vue, Ajax, JSON, or REST, such as Odata, HTML, or CSS.\n\u2022 5+ years of experience with two or more of the following: C#, Python, PHP, or Java.\n\u2022 Knowledge of database architecture and data transformations.\n\nEducation:\n\u2022 BA/BS from an accredited institution, or former Officer, NCO or Warrant Officer with Military Experience or Intelligence/Knowledge Management background.\n\nLocation:\n\u2022 Fort Liberty, NC.\n\nPrescient Edge is a Veteran-Owned Small Business (VOSB) founded as a counterintelligence (CI) and Human Intelligence (HUMINT) company in 2008. We are a global operations and solutions integrator delivering full-spectrum intelligence analysis support, training, security, and RD&E support solutions to the Department of Defense and throughout the intelligence community. Prescient Edge is an Equal Opportunity Employer (EEO). All applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other characteristic that is protected by law. We strive to foster equity and inclusion throughout our organization because we believe that diversity of thought is critical for creating a safe and engaging work environment while also enabling the organization's success.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Fort Liberty", "job_state": "NC", "job_country": "US", "job_latitude": 35.139416, "job_longitude": -79.00517, "job_benefits": ["dental_coverage", "retirement_savings", "health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=VxoA0o_1chGNszviAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-20T00:00:00.000Z", "job_offer_expiration_timestamp": 1705708800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor's Degree", "Current DOD TS/SCI security clearance", "5+ years of experience in one or more of the following: Business Analysis, Army Special Operations, Intelligence and/ or Information Management/ Knowledge Management", "5+ years of Experience supporting the United States Military, preferably SOF elements", "5+ years of experience with Single Page Application Development and client-side coding, including Jscript, React, Angular, Aurelia, Vue, Ajax, JSON, or REST, such as Odata, HTML, or CSS", "5+ years of experience with two or more of the following: C#, Python, PHP, or Java", "Knowledge of database architecture and data transformations", "BA/BS from an accredited institution, or former Officer, NCO or Warrant Officer with Military Experience or Intelligence/Knowledge Management background"], "Benefits": ["A competitive salary with performance bonus opportunities", "Comprehensive healthcare benefits, including medical, vision, dental, and orthodontia coverage", "A substantial retirement plan with no vesting schedule", "Career development opportunities, including on-the-job training, tuition reimbursement, and networking", "A positive work environment where employees are respected, supported, and engaged"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_occupational_categories": ["15-2041.00: Statisticians"]}, {"employer_name": "Google", "employer_logo": "https://kgo.googleusercontent.com/profile_vrt_raw_bytes_1587515358_10512.png", "employer_website": "http://www.google.com", "employer_company_type": "Information", "job_publisher": "LinkedIn", "job_id": "UCyfcHx_G0RJiHLYAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer, Google Customer Solutions", "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-google-customer-solutions-at-google-3774339875", "job_apply_is_direct": false, "job_apply_quality_score": 0.7785, "apply_options": [{"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-google-customer-solutions-at-google-3774339875", "is_direct": false}, {"publisher": "LocalJobs.com", "apply_link": "https://www.localjobs.com/job/redwood-city-ca-data-engineer-google-customer-solutions", "is_direct": false}, {"publisher": "OBIO Job Board - Ontario Bioscience Innovation Organization", "apply_link": "https://careers.obio.ca/companies/google-24698/jobs/27177096-data-engineer-iii-google-customer-solutions", "is_direct": false}, {"publisher": "Fairygodboss", "apply_link": "https://fairygodboss.com/jobs/google/data-engineer-google-customer-solutions-1dadc861689e8524737f1a1353660711", "is_direct": false}, {"publisher": "Levels.fyi", "apply_link": "https://www.levels.fyi/jobs?jobId=114376571952734918", "is_direct": false}, {"publisher": "ASU+GSV Summit Job Board", "apply_link": "https://jobs.asugsvsummit.com/companies/google-24698/jobs/27177096-data-engineer-iii-google-customer-solutions", "is_direct": false}, {"publisher": "Jora", "apply_link": "https://us.jora.com/job/Data-Engineer-88211bd26369fdf1a8fce47d5608786e", "is_direct": false}, {"publisher": "BeBee", "apply_link": "https://us.bebee.com/job/20231205-3b7187ddd919706ef7bdd6c1404ad2e2", "is_direct": false}], "job_description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Redwood City, CA, USA; New York, NY, USA; San Francisco, CA, USA.Minimum qualifications:\n\u2022 Bachelor\u2019s degree or equivalent practical experience.\n\u2022 Experience designing data models, data warehouses, and using SQL and NoSQL database management systems along with data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow).\n\u2022 Experience in one or more object oriented programming languages (e.g., Java, C++, or Python, etc.).\n\nPreferred qualifications:\n\u2022 Master\u2019s degree in Business, Statistics, Mathematics, Economics, Engineering or Applied Science, or a related field.\n\u2022 2 years of experience managing projects and working with analytics, software coding, or customer-side web technologies.\n\u2022 2 years of experience in a customer-facing role.\n\u2022 Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources.\n\u2022 Experience in distributed data processing and Unix or GNU/Linux systems.\n\nAbout The Job\n\nPLACEHOLDER JOB DESCRIPTION\n\nGoogle Customer Solutions is harnessing the power of artificial intelligence to build intelligent solutions that enable GCS to grow smarter and faster. AI-driven tools and systems can help automate repetitive tasks, improve decision-making processes, and unlock valuable insights from large volumes of data. Embracing AI as a strategic cornerstone for growth will empower GCS to thrive in an increasingly competitive market and unlock new opportunities for success.\n\nAs a Data Engineer, you will take on data challenges in an agile way. In this role, you will use an analytical, data-driven approach to drive a deep understanding of fast changing business. You will build data pipelines and reporting tools that enable engineers, analysts, and data science teams through feature engineering, automated data extracts, wrangling needs, and scaled insights for both the data science team and their users.\n\nWhen our millions of advertisers and publishers are happy, so are we! Our Google Customer Solutions (GCS) team of entrepreneurial, enthusiastic and client-focused members are the \"human face\" of Google, helping entrepreneurs both individually and broadly build their online presence and grow their businesses. We are dedicated to growing the unique needs of advertising companies. Our teams of strategists, analysts, advisers and support specialists collaborate closely to spot and analyze customer needs and trends. In collaboration, we create and implement business plans broadly for all types of businesses.\n\nThe US base salary range for this full-time position is $93,500-$135,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\n\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .\n\nResponsibilities\n\u2022 Build data pipelines, reports, and frameworks that enable analysts and other stakeholders across the organization.\n\u2022 Recognize and adopt best practices in developing pipelines, analytical insights, data integrity, test design, analysis, validation, and documentation.\n\u2022 Design and develop scalable and actionable solutions (e.g., dashboards, automated collateral, web applications) that tell a story and provide insights to help advertisers grow.\n\u2022 Work with various stakeholders to understand feature/tooling gaps and innovate on behalf of customers.\n\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .", "job_is_remote": false, "job_posted_at_timestamp": 1703154368, "job_posted_at_datetime_utc": "2023-12-21T10:26:08.000Z", "job_city": "Redwood City", "job_state": "CA", "job_country": "US", "job_latitude": 37.484795, "job_longitude": -122.22814, "job_benefits": ["health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=UCyfcHx_G0RJiHLYAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-29T05:50:37.000Z", "job_offer_expiration_timestamp": 1706507437, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree or equivalent practical experience", "Experience designing data models, data warehouses, and using SQL and NoSQL database management systems along with data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow)", "Experience in one or more object oriented programming languages (e.g., Java, C++, or Python, etc.)"], "Responsibilities": ["As a Data Engineer, you will take on data challenges in an agile way", "In this role, you will use an analytical, data-driven approach to drive a deep understanding of fast changing business", "You will build data pipelines and reporting tools that enable engineers, analysts, and data science teams through feature engineering, automated data extracts, wrangling needs, and scaled insights for both the data science team and their users", "Build data pipelines, reports, and frameworks that enable analysts and other stakeholders across the organization", "Recognize and adopt best practices in developing pipelines, analytical insights, data integrity, test design, analysis, validation, and documentation", "Design and develop scalable and actionable solutions (e.g., dashboards, automated collateral, web applications) that tell a story and provide insights to help advertisers grow", "Work with various stakeholders to understand feature/tooling gaps and innovate on behalf of customers"], "Benefits": ["The US base salary range for this full-time position is $93,500-$135,000 + bonus + equity + benefits", "Our salary ranges are determined by role, level, and location"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "41903100", "job_onet_job_zone": "4", "job_naics_code": "519130", "job_naics_name": "Internet Publishing and Broadcasting and Web Search Portals"}, {"employer_name": "Senior Data Engineer jobs in VA", "employer_logo": null, "employer_website": null, "employer_company_type": null, "job_publisher": "USA Staffing", "job_id": "c_6aaDG1HdS5-HvEAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Senior Data Engineer || Reston, VA || Onsite", "job_apply_link": "https://usstaffinginc.org/job/senior-data-engineer-jobs-in-va-reston-va-14-senior-data-engineer-reston-va-onsite/", "job_apply_is_direct": true, "job_apply_quality_score": 0.3986, "apply_options": [{"publisher": "USA Staffing", "apply_link": "https://usstaffinginc.org/job/senior-data-engineer-jobs-in-va-reston-va-14-senior-data-engineer-reston-va-onsite/", "is_direct": true}], "job_description": "Senior Data Engineer\nDuration 6 months with the right to hire\n\nRemote but will have to travel to Reston, VA week of 1/8.\n\nWe are seeking a Technical Consultant to join our newly formed Technical Consulting practice. In this role, you will have the opportunity to master your data engineering and architecture skills while working on challenging projects with diverse clients and make a meaningful impact on their business success. We value diversity and strive to create an inclusive and supportive work environment where everyone can thrive. We offer competitive compensation, benefits, and professional development opportunities.\n\nResponsibilities:\n\n\u00b7 Data Integration and Management: Design, implement, and maintain data pipelines and integration processes\n\n\u00b7 Programming and Scripting: Develop and optimize scripts in Python/Javascript, to facilitate data management and integration\n\n\u00b7 Contact Center Technologies Integration: Work with various contact center technologies to ensure seamless data flow\n\n\u00b7 CRM Technology Integration: Integrate CRM systems with other tools and platforms to ensure efficient data exchange and processing\n\n\u00b7 API Development and Management: Design, develop, and maintain APIs for data exchange and integration between different systems\n\n\u00b7 Agile Project Management: Work within a sprint schedule, adapting to agile methodologies for efficient project execution and timely delivery\n\n\u00b7 Cross-functional Collaboration: Collaborate with different teams to understand requirements and provide technical solutions that align with business goals\n\n\u00b7 Quality Assurance and Testing: Ensure the reliability and efficiency of integrations and data pipelines through rigorous testing and quality assurance practices\n\n\u00b7 Documentation and Reporting: Maintain comprehensive documentation of processes, integrations, and systems, and report on project progress and outcomes\n\n\u00b7 Consulting: Build and maintain strong relationships with clients, understanding their needs, and providing exceptional client service to ensure long-term partnerships\n\nQualifications:\n\n\u00b7 Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, Information Technology, or related field\n\n\u00b7 Technical Expertise: Strong proficiency in data management technologies, including relational and NoSQL databases, with advanced skills in programming languages, especially Python. Experience with TypeScript is a plus\n\n\u00b7 Agile Methodology: Experience working in an agile environment, preferably with knowledge of sprint schedules and agile project management tools\n\n\u00b7 Cloud computing: Comfortable with AWS and other cloud environments\n\n\u00b7 Problem-Solving Skills: Strong analytical and problem-solving abilities to develop creative solutions for data integration challenges\n\n\u00b7 Communication Skills: Excellent verbal and written communication skills for effective collaboration and documentation\n\n\u00b7 Adaptability and Learning: Ability to quickly adapt to new technologies and continuously learn to stay abreast of industry developments\n\n\u00b7 Teamwork: Strong team player with the ability to work collaboratively in a cross-functional team environment\n\n\u00b7 Quality Focus: Commitment to quality and attention to detail in all aspects of data integration and pipeline development\n\n\u00b7 Experience with APIs: Proven experience in developing and managing APIs for data integration\n\n\u00b7 Familiarity with CCaaS and Qualtrics is a plus: Knowledge of Contact Center as a Service technologies and experience with Qualtrics, particularly its analytics and NLU features\n\nClick here for More remote and onsite Contract / Fulltime USA JOBS", "job_is_remote": false, "job_posted_at_timestamp": 1703177260, "job_posted_at_datetime_utc": "2023-12-21T16:47:40.000Z", "job_city": "Reston", "job_state": "VA", "job_country": "US", "job_latitude": 38.95863, "job_longitude": -77.357, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=c_6aaDG1HdS5-HvEAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": null, "job_offer_expiration_timestamp": null, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s or Master\u2019s degree in Computer Science, Engineering, Information Technology, or related field", "Technical Expertise: Strong proficiency in data management technologies, including relational and NoSQL databases, with advanced skills in programming languages, especially Python", "Agile Methodology: Experience working in an agile environment, preferably with knowledge of sprint schedules and agile project management tools", "Cloud computing: Comfortable with AWS and other cloud environments", "Problem-Solving Skills: Strong analytical and problem-solving abilities to develop creative solutions for data integration challenges", "Communication Skills: Excellent verbal and written communication skills for effective collaboration and documentation", "Adaptability and Learning: Ability to quickly adapt to new technologies and continuously learn to stay abreast of industry developments", "Teamwork: Strong team player with the ability to work collaboratively in a cross-functional team environment", "Quality Focus: Commitment to quality and attention to detail in all aspects of data integration and pipeline development", "Experience with APIs: Proven experience in developing and managing APIs for data integration"], "Responsibilities": ["In this role, you will have the opportunity to master your data engineering and architecture skills while working on challenging projects with diverse clients and make a meaningful impact on their business success", "Data Integration and Management: Design, implement, and maintain data pipelines and integration processes", "Programming and Scripting: Develop and optimize scripts in Python/Javascript, to facilitate data management and integration", "Contact Center Technologies Integration: Work with various contact center technologies to ensure seamless data flow", "CRM Technology Integration: Integrate CRM systems with other tools and platforms to ensure efficient data exchange and processing", "API Development and Management: Design, develop, and maintain APIs for data exchange and integration between different systems", "Agile Project Management: Work within a sprint schedule, adapting to agile methodologies for efficient project execution and timely delivery", "Cross-functional Collaboration: Collaborate with different teams to understand requirements and provide technical solutions that align with business goals", "Quality Assurance and Testing: Ensure the reliability and efficiency of integrations and data pipelines through rigorous testing and quality assurance practices", "Documentation and Reporting: Maintain comprehensive documentation of processes, integrations, and systems, and report on project progress and outcomes", "Consulting: Build and maintain strong relationships with clients, understanding their needs, and providing exceptional client service to ensure long-term partnerships"], "Benefits": ["We offer competitive compensation, benefits, and professional development opportunities"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "Intel", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Intel_logo_%282006-2020%29.svg/1005px-Intel_logo_%282006-2020%29.svg.png", "employer_website": "http://www.intel.com", "employer_company_type": "Manufacturing", "job_publisher": "Jobs Trabajo.org", "job_id": "mt80oP1fXST1gKY1AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer - Data Engineering -Big Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-3bfe69642012cca80f149b9256bc2069", "job_apply_is_direct": false, "job_apply_quality_score": 0.4321, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-3bfe69642012cca80f149b9256bc2069", "is_direct": false}], "job_description": "Fab Sort Manufacturing (FSM) is responsible for the production of all Intel silicon using some of the world's most advanced manufacturing processes in fabs in Arizona, Ireland, Israel, Oregon and 2 new greenfield sites in Ohio and Germany. 0 strategy, FSM is rapidly expanding its operation to deliver output for both internal and foundry customers with state-of-the-art technologies arriving in high-volume manufacturing at a 2-year cadence going forward. Intel recently created HVM Global Yield organization in FSM to strengthen its yield operation and enable fast-paced yield ramp-up in early HVM phases for each technology in collaboration with Technology Development team and FSM fab managers.\nThis job requisition is to seek Data Science (DS) team engineering roles in FSM HVM Global Yield organization, reporting to Data Science team manager. Selected candidates will work with other members in Global Yield org including Process Integration, Device and Defect engineering teams, fab module/yield teams and TD team members to achieve yield ramp-up and process optimization in early production stage, supporting internal and external customers.\nData Science engineers' responsibilities include (but are not limited to):\n+ Identify valuable data sources and set automated data collection process to build and manage the dataset used for yield analysis.\n+ Processing of structured and unstructured data, and building predictive models and machine learning algorithms.\n+ Work with Yield Modelling team to develop new yield analysis methods and algorithms to deliver world class yield analytics and machine-learning solutions in high-volume manufacturing environment.\n+ Collaborate with Technology Development, Program Managers, Process Integration, Device Integration and Defect teams to identify yield and performance detractors and enhancement opportunities and support fast paced yield ramp-up in high-volume manufacturing phase.\n+ Engineering support for technical interactions with internal and external customers.\nBachelor's degree in science and engineering major.\n+ 3+ years' experience in advanced node semiconductor industry in yield analysis and data science.\n+ 3+ years' experience in program languages and big data to develop a new analysis method and algorithms using large amount of fab data. Expertise in big data analysis and machine-learning.\n+ 3+ years' experience in Device Physics and overall FinFET process flow.\nin science and engineering major.\n+ Experience in working or leading a TFT project.\n+ Experience in serving external Foundry customers through technical interactions.\n+ Experience in new semiconductor technology development.\n+ Basic understanding on module processes including lithography, dry etch, wet etch, CMP, diffusion, implant, thin films and metrology.\n\\#foundry\n\u2022 *As the world's largest chip manufacturer, Intel strives to make every facet of semiconductor manufacturing state-of-the-art -- from semiconductor process development and manufacturing, through yield improvement to packaging, final test and optimization, and world class Supply Chain and facilities support. Employees in the Technology Development and Manufacturing Group are part of a worldwide network of design, development, manufacturing, and assembly/test facilities, all focused on utilizing the power of Moore\u2019s Law to bring smart, connected devices to every person on Earth.\n\u2022 *All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.\n\u2022 *It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation.", "job_is_remote": false, "job_posted_at_timestamp": 1703130258, "job_posted_at_datetime_utc": "2023-12-21T03:44:18.000Z", "job_city": "San Juan", "job_state": "TX", "job_country": "US", "job_latitude": 26.189241, "job_longitude": -98.15529, "job_benefits": ["health_insurance", "retirement_savings", "paid_time_off"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=mt80oP1fXST1gKY1AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor's degree in science and engineering major", "3+ years' experience in advanced node semiconductor industry in yield analysis and data science", "3+ years' experience in program languages and big data to develop a new analysis method and algorithms using large amount of fab data", "Expertise in big data analysis and machine-learning", "3+ years' experience in Device Physics and overall FinFET process flow", "Experience in working or leading a TFT project", "Experience in serving external Foundry customers through technical interactions", "Experience in new semiconductor technology development"], "Responsibilities": ["Selected candidates will work with other members in Global Yield org including Process Integration, Device and Defect engineering teams, fab module/yield teams and TD team members to achieve yield ramp-up and process optimization in early production stage, supporting internal and external customers", "Identify valuable data sources and set automated data collection process to build and manage the dataset used for yield analysis", "Processing of structured and unstructured data, and building predictive models and machine learning algorithms", "Work with Yield Modelling team to develop new yield analysis methods and algorithms to deliver world class yield analytics and machine-learning solutions in high-volume manufacturing environment", "Collaborate with Technology Development, Program Managers, Process Integration, Device Integration and Defect teams to identify yield and performance detractors and enhancement opportunities and support fast paced yield ramp-up in high-volume manufacturing phase", "Engineering support for technical interactions with internal and external customers"], "Benefits": ["*It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "334413", "job_naics_name": "Semiconductor and Related Device Manufacturing"}, {"employer_name": "Chewy", "employer_logo": "https://static1.squarespace.com/static/5e0fb6a85a52fd5fb32a83c1/t/5ea5d85bf9a9ba212f12ce58/1587927131703/Chewy_Logo_RGB_Blue.jpg", "employer_website": "http://www.chewy.com", "employer_company_type": "Retail", "job_publisher": "Geebo", "job_id": "YSufnz5oOO6JlFVSAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer II at Chewy in Dallas, TX", "job_apply_link": "https://geebo.com/jobs-online/view/id/1070861941-data-engineer-ii-at-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4353, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1070861941-data-engineer-ii-at-/", "is_direct": true}], "job_description": "Our Opportunity:\nChewy's Planning and Analytics team has an exciting opportunity for a Data Engineer II to join the pack. With a background in data engineering, data analysis and reporting you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization. What You'll Do:\nDesign, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data & forecasting products by working with analytics and data scientist team members to speed to market and to enrich data and model options. Assist in creating Proof of Concepts and advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Code, test, and document new or modified data systems to create robust and scalable applications for data analytics. Partner with Business Analysts and Solutions Architects to develop technical architectures for strategic enterprise projects and initiatives. Document technical details of your work using industry standard protocols like Jira, Confluence etc. Respect for Teammates - encouraging diverse identities, backgrounds, talent and perspectives. Collaboration - working with all team members, and across teams, to integrate perspectives and encourage contributions. Adaptability - quickly applying feedback and circumstances to team priorities. What You'll Need:\nBachelor of Science or Master's degree in Computer Science, Engineering, Information Systems, Mathematics or related field 7\nyears of enterprise experience as a data engineer and/or software engineer Strong software development skills in Python and Advanced SQL, tuning SQL queries and data pipelines Proven ability to demonstrate developing modern data pipelines and applications for analytics (e.g., BI, reporting, dashboards) and advanced analytics (e.g., machine learning, deep learning) use cases. Knowledge of industry best practices, strong understanding how data is extracted, transformed, scrubbed, and loaded in a large Data Warehouse environment. Ability to effectively operate both independently and as part of a team. Self-motivated with strong problem-solving and self-learning skills. E-com, Retail or startup experience is a plus. Excellent written and oral communication skills and ability to work with development teams. Bonus:\nExposure building dimensional models in the data warehouse Experience with MPP/Columnar databases like Snowflake, Redshift, etc Exposure to data streaming tools and technologies like kafka or similar technologies Experience with developing solutions on cloud computing services and infrastructure with AWS AWS Components like EC2, S3, Athena, Lambda etc Experience with Sagemaker/Sagemaker Studio AWS Certified Developer/AWS Machine Learning 4\nyears of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems 2\nyears of experience working with workflow orchestration tools (Preferred:\nAirflow, Prefect, Luigi, AutoSys) Experience designing and building infrastructure in cloud data environments (Preferred:\nAWS, GCP, Azure).\nSalary Range:\n$100K -- $150K\nMinimum Qualification\nData Science & Machine Learning, Systems Architecture & EngineeringEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Dallas", "job_state": "TX", "job_country": "US", "job_latitude": 32.776665, "job_longitude": -96.79699, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=YSufnz5oOO6JlFVSAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 84, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Bachelor of Science or Master's degree in Computer Science, Engineering, Information Systems, Mathematics or related field 7", "years of enterprise experience as a data engineer and/or software engineer Strong software development skills in Python and Advanced SQL, tuning SQL queries and data pipelines Proven ability to demonstrate developing modern data pipelines and applications for analytics (e.g., BI, reporting, dashboards) and advanced analytics (e.g., machine learning, deep learning) use cases", "Knowledge of industry best practices, strong understanding how data is extracted, transformed, scrubbed, and loaded in a large Data Warehouse environment", "Ability to effectively operate both independently and as part of a team", "Self-motivated with strong problem-solving and self-learning skills", "Excellent written and oral communication skills and ability to work with development teams", "Exposure building dimensional models in the data warehouse Experience with MPP/Columnar databases like Snowflake, Redshift, etc Exposure to data streaming tools and technologies like kafka or similar technologies Experience with developing solutions on cloud computing services and infrastructure with AWS AWS Components like EC2, S3, Athena, Lambda etc Experience with Sagemaker/Sagemaker Studio AWS Certified Developer/AWS Machine Learning 4", "years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems 2"], "Responsibilities": ["With a background in data engineering, data analysis and reporting you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning", "This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity", "You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization", "Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals", "Solve complex data problems to deliver insights that helps our business to achieve their goals", "Create data & forecasting products by working with analytics and data scientist team members to speed to market and to enrich data and model options", "Assist in creating Proof of Concepts and advise, consult, mentor and coach other data and analytic professionals on data standards and practices", "Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team", "Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes", "Code, test, and document new or modified data systems to create robust and scalable applications for data analytics", "Partner with Business Analysts and Solutions Architects to develop technical architectures for strategic enterprise projects and initiatives", "Document technical details of your work using industry standard protocols like Jira, Confluence etc", "Respect for Teammates - encouraging diverse identities, backgrounds, talent and perspectives", "Collaboration - working with all team members, and across teams, to integrate perspectives and encourage contributions", "Adaptability - quickly applying feedback and circumstances to team priorities"], "Benefits": ["$100K -- $150K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "454111", "job_naics_name": "Electronic Shopping"}, {"employer_name": "Datadog", "employer_logo": "https://meroxa.com/static/dd326ae62365893b92a8c695910f44bd/datadog_full_logo_.png", "employer_website": "https://www.datadog.com", "employer_company_type": "Information", "job_publisher": "Mogul", "job_id": "DUO8HuBnMVx43hyYAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer - Data Science Engineering", "job_apply_link": "https://onmogul.com/jobs/data-engineer-data-science-engineering", "job_apply_is_direct": false, "job_apply_quality_score": 0.537, "apply_options": [{"publisher": "Mogul", "apply_link": "https://onmogul.com/jobs/data-engineer-data-science-engineering", "is_direct": false}, {"publisher": "New York City NY Geebo.com Free Classifieds Ads - Geebo", "apply_link": "https://newyorkcity-ny.geebo.com/jobs-online/view/id/1081387294-data-engineer-data-science-/", "is_direct": true}], "job_description": "About Datadog:\n\nWe're on a mission to build the best platform in the world for engineers to understand and scale their systems, applications, and teams. We operate at high scale\u2014trillions of data points per day\u2014providing always-on alerting, metrics visualization, logs, and application tracing for tens of thousands of companies. Our engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way.\n\nOur engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way. We need you to design and build machine learning-powered products that help our customers learn from their data and make better decisions in real-time.\n\nThe team:\nWe extract and manage data and events from our core products and live systems to make them centrally available for our Data Science team in both batch and real-time ways. We enable Data Scientists to productionize their models and expose their data assets to the rest of the company.\n\nIf you\u2019re excited to work on a fast-moving data engineering team with the best open-source data tools at high scale, we want to meet you.\n\nYou will:\n\u2022 Build distributed, real-time, high-volume data pipelines and work together with others to enable high-scale Data Science\n\u2022 Do it with Spark, Luigi, Kafka and other open-source technologies\n\u2022 Work all over the stack, moving fluidly between programming languages: Scala, Java, Python, Go, and more\n\u2022 Join a tightly knit team solving hard problems the right way\n\u2022 Own meaningful parts of our service, have an impact, grow with the company\n\nRequirements:\n\u2022 You have a BS/MS/PhD in a scientific field or equivalent experience\n\u2022 You have built and operated data pipelines for real customers in production systems\n\u2022 You are fluent in several programming languages (JVM & otherwise)\n\u2022 You enjoy wrangling huge amounts of data and exploring new data sets\n\u2022 You value code simplicity and performance\n\u2022 You want to work in a fast, high growth startup environment that respects its engineers and customers\n\u2022 You are preferably familiar with Spark and/or Hadoop and know how to put machine learning models in production\n\nIs this you? Send your resume and link to your GitHub if available.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "New York", "job_state": "NY", "job_country": "US", "job_latitude": 40.712776, "job_longitude": -74.005974, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=DUO8HuBnMVx43hyYAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["You have a BS/MS/PhD in a scientific field or equivalent experience", "You have built and operated data pipelines for real customers in production systems", "You are fluent in several programming languages (JVM & otherwise)", "You enjoy wrangling huge amounts of data and exploring new data sets", "You value code simplicity and performance", "You want to work in a fast, high growth startup environment that respects its engineers and customers", "You are preferably familiar with Spark and/or Hadoop and know how to put machine learning models in production"], "Responsibilities": ["Build distributed, real-time, high-volume data pipelines and work together with others to enable high-scale Data Science", "Join a tightly knit team solving hard problems the right way", "Own meaningful parts of our service, have an impact, grow with the company"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15111100", "job_onet_job_zone": "5", "job_naics_code": "511210", "job_naics_name": "Software Publishers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Jobs Trabajo.org", "job_id": "p-SIJEFbyVvRZMgkAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer, Data Engineering", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-42b8626cab747d76e21c5b14f827bb9d", "job_apply_is_direct": false, "job_apply_quality_score": 0.4508, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-42b8626cab747d76e21c5b14f827bb9d", "is_direct": false}], "job_description": "Quantitative Data Engineer\n\nQuantitative Investment Platforms and Developer Operations Team\n\nFull-time\n\nAs a Data Engineer you will work in a fast paced, innovative, and collaborative environment on exciting technology directives. A Data Engineer embodies a unique combination of both technical and people skills. Both emotional intelligence and the ability to solve complex problems and communicate effectively are key to success. Ideally, you have a strong background in Data Engineering as well as workflow orchestration and CI/CD pipelines. Additionally, it is preferred that you have robust experience in cloud platforms and system architectures.\n\nAs a Quantitative Operations Engineer, you will join the Quantitative Investment & Developer Operations team, part of MassMutual\u2019s Investment Management organization. The Quantitative development team is comprised of highly skilled, financial industry savvy professionals who render collaborative and portfolio risk solutions to our portfolio managers. The team culture is collaborative, cross-functional, and fosters high performance results with an emphasis on encouraging a healthy work/life balance.\n\nOur ideal Quant Data Engineer is someone who is passionate about data and deploying solutions and workflow automations. You enjoy building data projects from the ground up and are equally comfortable working with business partners to understand requirements as you are developing and delivering robust solutions that meet the highest standards. Learning new technologies and working in the cloud excite you. Ingest data from disparate sources into the various data stores.\n\nCleanse and enrich data and apply adequate data quality controls.\n\nProvide insight and direction to guide the future development of MassMutual\u2019s data platform.\n\nDevelop re-usable tools to help streamline the delivery of new projects.\n\nWork in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements.\n\nBachelor\u2019s degree in Computer Science, Finance, Business or a related field.\n\n2+ years of experience in the IT, and/or finance industry\n\nUnderstanding of ETL methodologies and proficient in tools via Python\n\nHands on experience developing with Python and advanced data processing using Python libraries & AWS services.\n\nExperience working in a cloud environment and basic administration (e.g. Experience in modeling and tuning relational databases (SQL Server/PostgreSQL) and NOSQL databases (Mongo)\n\nExperience with GIT and code review/deployment & scheduling part of operations\n\nExperience in relational databases (SQLServer/PostgreSQL) and NOSQL databases (Mongo)\n\nExperience in Agile/SCRUM-specifically in Story/Acceptance criteria development\n\nMaster\u2019s degree in computer science, engineering or a related field\n\n4+ Years of experience in the IT and/or quantitative, investment or finance industry\n\nExperience with troubleshooting and root cause analysis to determine and remediate potential issues\n\nExperience with data reporting (e.g. Micro strategy, Tableau, Looker) or via python libraries\n\nExperience delivering mobile apps and app store ecosystems\n\nA Data-driven mindset and the ability to use quantitative tools and analyze unstructured feedback to inform the development of solutions\n\nCuriosity regarding emerging digital and technology trends can translate into excellent customer experiences\n\nRegular meetings with the Quantitative and ETX project teams\n\nNetworking opportunities including access to Asian, Hispanic/Latinx, African American, women, LGBTQ, Veteran and disability-focused Business Resource Groups\n\nAccess to learning content on Degreed and other informational platforms\n\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. Note: Veterans are welcome to apply, regardless of their discharge status.\n\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703120689, "job_posted_at_datetime_utc": "2023-12-21T01:04:49.000Z", "job_city": "Boston", "job_state": "MA", "job_country": "US", "job_latitude": 42.36008, "job_longitude": -71.05888, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=p-SIJEFbyVvRZMgkAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Both emotional intelligence and the ability to solve complex problems and communicate effectively are key to success", "Ideally, you have a strong background in Data Engineering as well as workflow orchestration and CI/CD pipelines", "You enjoy building data projects from the ground up and are equally comfortable working with business partners to understand requirements as you are developing and delivering robust solutions that meet the highest standards", "Bachelor\u2019s degree in Computer Science, Finance, Business or a related field", "Understanding of ETL methodologies and proficient in tools via Python", "Hands on experience developing with Python and advanced data processing using Python libraries & AWS services", "Experience working in a cloud environment and basic administration (e.g. Experience in modeling and tuning relational databases (SQL Server/PostgreSQL) and NOSQL databases (Mongo)", "Experience with GIT and code review/deployment & scheduling part of operations", "Experience in Agile/SCRUM-specifically in Story/Acceptance criteria development", "Master\u2019s degree in computer science, engineering or a related field", "4+ Years of experience in the IT and/or quantitative, investment or finance industry", "Experience with troubleshooting and root cause analysis to determine and remediate potential issues", "Experience with data reporting (e.g. Micro strategy, Tableau, Looker) or via python libraries", "Experience delivering mobile apps and app store ecosystems", "A Data-driven mindset and the ability to use quantitative tools and analyze unstructured feedback to inform the development of solutions", "Curiosity regarding emerging digital and technology trends can translate into excellent customer experiences"], "Responsibilities": ["Ingest data from disparate sources into the various data stores", "Cleanse and enrich data and apply adequate data quality controls", "Provide insight and direction to guide the future development of MassMutual\u2019s data platform", "Develop re-usable tools to help streamline the delivery of new projects", "Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "The Hanover Insurance Group", "employer_logo": "https://149448277.v2.pressablecdn.com/wp-content/uploads/2020/02/the-hanover-insurance-group-logo.jpg", "employer_website": "http://www.hanover.com", "employer_company_type": "Finance", "job_publisher": "ZipRecruiter", "job_id": "rMoeSJFbPBnrPPISAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer", "job_apply_link": "https://www.ziprecruiter.com/c/The-Hanover-Insurance-Group/Job/Data-Engineer/-in-Worcester,MA?jid=ccc8b71f7470b95a", "job_apply_is_direct": false, "job_apply_quality_score": 0.7079, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/The-Hanover-Insurance-Group/Job/Data-Engineer/-in-Worcester,MA?jid=ccc8b71f7470b95a", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=7ff766e605347d55", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/data-engineer-the-hanover-insurance-group-JV_IC1154962_KO0,13_KE14,41.htm?jl=1009042120280", "is_direct": false}], "job_description": "Data Engineer\n\nWorcester, MA, USA Req #17440\nThursday, December 21, 2023\n\nFor more than 170 years, The Hanover has been committed to delivering on our promises and being there when it matters the most. We live our values every day, demonstrating we CARE through our values, ESG initiatives and IDE journey.\n\nOur Corporate Actuarial Department is seeking a Data Engineer to join our growing team. This is a full time, exempt role. This is a fully remote or hybrid opportunity.\n\nPOSTION OVERVIEW:\n\nData engineering is the aspect of data science that focuses on practical applications of data collection and analysis. This role primarily will become proficient with all internal & external data produced and consumed by THG. The engineer will understand where the data is, basic data models and architecture, how to access and obtain data and how to manipulate and work with data to produce output - which may be reports, datasets or self-service reports.\n\nIN THIS ROLE, YOU WILL:\n\u2022 Design, develop, and maintain ETL processes using SQL, Python, and Spark to ensure efficient and reliable data integration.\n\u2022 Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and implement data solutions that meet those needs.\n\u2022 Implement and optimize data warehousing solutions on Azure Cloud, utilizing services such as Azure SQL Data Warehouse and Azure Synapse Analytics.\n\u2022 Monitor and troubleshoot data pipelines to ensure data accuracy, completeness, and timely delivery.\n\u2022 Stay up-to-date with industry trends and emerging technologies to continuously enhance and improve data engineering processes.\nWHAT YOU NEED TO APPLY:\n\u2022 2-5 years related experience and bachelor's degree.\n\u2022 Degrees and/or related experience in Applied Data Science, Data Science, Information and Data Science, Information Management Analytics, Data Mining, Predictive Analytics are most sought-after.\n\u2022 Degrees in Statistics, Mathematics, Computer Science, Information Systems are also preferred.\nSkills:\n\u2022 Analytical Skills: Data Engineers work with large amounts of data that will include facts, figures, and number crunching. You will need to see through the data and analyze it to find conclusions.\n\u2022 Communication Skills: Data engineers are often called to present their findings or translate the data into an understandable document. You will need to write and speak clearly, easily communicating complex ideas.\n\u2022 Critical Thinking: Data engineers must look at the numbers, trends, and data and come to new conclusions based on the findings.\n\u2022 Attention to Detail: Data is precise. Data engineers must make sure they are vigilant in their analysis to come to correct conclusions.\n\u2022 Math Skills: Data engineers intermediate need math skills to estimate numerical data.\n\nCAREER DEVELOPMENT:\n\nIt's not just a job, it's a career, and we are here to support you every step of the way. We want you to be successful and fulfilled. Through on-the-job experiences, personalized coaching and our robust learning and development programs, we encourage you - at every level - to grow and develop.\n\nBENEFITS:\n\nWe offer comprehensive benefits to help you be healthy, build financial security, and balance work and home life. At The Hanover, you'll enjoy what you do and have the support you need to succeed.\n\nBenefits include:\n\u2022 Medical, dental, vision, life, and disability insurance\n\u2022 401K with a company match\n\u2022 Tuition reimbursement\n\u2022 PTO\n\u2022 Company paid holidays\n\u2022 Flexible work arrangements\n\u2022 Cultural Awareness Day in support of IDE\n\u2022 On-site medical/wellness center (Worcester only)\n\u2022 Click here for the full list of Benefits\n\nEEO statement:\n\nThe Hanover values diversity in the workplace and among our customers. The company provides equal opportunity for employment and promotion to all qualified employees and applicants on the basis of experience, training, education, and ability to do the available work without regard to race, religion, color, age, sex/gender, sexual orientation, national origin, gender identity, disability, marital status, veteran status, genetic information, ancestry or any other status protected by law.\n\nFurthermore, The Hanover Insurance Group is committed to providing an equal opportunity workplace that is free of discrimination and harassment based on national origin, race, color, religion, gender, ancestry, age, sexual orientation, gender identity, disability, marital status, veteran status, genetic information or any other status protected by law.\"\n\nAs an equal opportunity employer, Hanover does not discriminate against qualified individuals with disabilities. Individuals with disabilities who wish to request a reasonable accommodation to participate in the job application or interview process, or to perform essential job functions, should contact us at: HRServices@hanover.com and include the link of the job posting in which you are interested.\n\nPrivacy Policy:\n\nTo view our privacy policy and online privacy statement, click here.\n\nApplicants who are California residents: To see the types of information we may collect from applicants and employees and how we use it, please click here.\n\nOther details\n\u2022 Pay Type Salary\nApply Now", "job_is_remote": true, "job_posted_at_timestamp": 1703145600, "job_posted_at_datetime_utc": "2023-12-21T08:00:00.000Z", "job_city": "Worcester", "job_state": "MA", "job_country": "US", "job_latitude": 42.262592, "job_longitude": -71.80229, "job_benefits": ["retirement_savings", "paid_time_off", "health_insurance", "dental_coverage"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=rMoeSJFbPBnrPPISAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-20T00:00:00.000Z", "job_offer_expiration_timestamp": 1705708800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["2-5 years related experience and bachelor's degree", "Degrees and/or related experience in Applied Data Science, Data Science, Information and Data Science, Information Management Analytics, Data Mining, Predictive Analytics are most sought-after", "Analytical Skills: Data Engineers work with large amounts of data that will include facts, figures, and number crunching", "You will need to write and speak clearly, easily communicating complex ideas", "Critical Thinking: Data engineers must look at the numbers, trends, and data and come to new conclusions based on the findings", "Data engineers must make sure they are vigilant in their analysis to come to correct conclusions"], "Responsibilities": ["Data engineering is the aspect of data science that focuses on practical applications of data collection and analysis", "This role primarily will become proficient with all internal & external data produced and consumed by THG", "The engineer will understand where the data is, basic data models and architecture, how to access and obtain data and how to manipulate and work with data to produce output - which may be reports, datasets or self-service reports", "Design, develop, and maintain ETL processes using SQL, Python, and Spark to ensure efficient and reliable data integration", "Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and implement data solutions that meet those needs", "Implement and optimize data warehousing solutions on Azure Cloud, utilizing services such as Azure SQL Data Warehouse and Azure Synapse Analytics", "Monitor and troubleshoot data pipelines to ensure data accuracy, completeness, and timely delivery", "Stay up-to-date with industry trends and emerging technologies to continuously enhance and improve data engineering processes", "Math Skills: Data engineers intermediate need math skills to estimate numerical data"], "Benefits": ["We offer comprehensive benefits to help you be healthy, build financial security, and balance work and home life", "Medical, dental, vision, life, and disability insurance", "401K with a company match", "Tuition reimbursement", "PTO", "Company paid holidays", "Flexible work arrangements", "Cultural Awareness Day in support of IDE"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_occupational_categories": ["15-2041.00: Statisticians"], "job_naics_code": "524126", "job_naics_name": "Direct Property and Casualty Insurance Carriers"}, {"employer_name": "Booking.com", "employer_logo": "https://cf.bstatic.com/static/img/booking_logo_knowledge_graph/247454a990efac1952e44dddbf30c58677aa0fd8.png", "employer_website": "http://www.booking.com", "employer_company_type": null, "job_publisher": "LinkedIn", "job_id": "nGN54_vqmpPJoYGKAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineering Manager - Data & Machine Learning Platform", "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineering-manager-data-machine-learning-platform-at-booking-com-3775651352", "job_apply_is_direct": false, "job_apply_quality_score": 0.8024, "apply_options": [{"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/data-engineering-manager-data-machine-learning-platform-at-booking-com-3775651352", "is_direct": false}, {"publisher": "The Muse", "apply_link": "https://www.themuse.com/jobs/bookingcom/data-engineering-manager-data-machine-learning-platform", "is_direct": false}, {"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Booking.com/Job/Data-Engineering-Manager-Data-&-Machine-Learning-Platform/-in-Washington,DC?jid=0cfa83df2b497c61", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=03933ad2eebc7268", "is_direct": false}, {"publisher": "Adzuna", "apply_link": "https://www.adzuna.com/details/4457408851", "is_direct": false}, {"publisher": "Fairygodboss", "apply_link": "https://fairygodboss.com/jobs/booking/data-engineering-manager-data-machine-learning-platform-e159e9a6d98602a947d9501622f7066a", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/data-engineering-manager-data-and-machine-learning-platform-booking-com-JV_IC1138213_KO0,59_KE60,71.htm?jl=1009003821091", "is_direct": false}, {"publisher": "Ladders", "apply_link": "https://www.theladders.com/job/data-engineering-manager-data-machine-learning-platform-booking-washington-dc_65933910", "is_direct": false}], "job_description": "At Booking.com, we want to empower everyone to experience the world. Through our products, partners, and people is how we do it. There's a whole planet of possibilities out there, and we bring it all together, in one place. Booking.com (USA), Inc, one of the support companies in the United States, is seeking a full time Data Engineering Manager.\n\nOur department mission is to radically accelerate innovation across all data, analytics & Machine Learning teams by providing a self-service, end-to-end platform, enabling the creation of connected, and repeatable, machine learning and analytics solutions. In the team you would join, we envision and set product direction for Data Analytics and investments powering the world\u2019s largest online accommodation reservations website.\n\nYour charter is to build Booking\u2019s innovative Analytical Platform for a wide range of use cases including sophisticated reporting, ML pipelines and data warehousing. Our customers are the Data Scientists, ML and Data Engineers, and Analyst community of Booking. The platform scales to millions of bookings a day, demonstrating several petabytes of data to optimise the end-user experience. In short, you would play a critical role on the team building the Analytics experience that solves Booking\u2019s hardest data consumption challenges.\n\nYou will join the Workflow Management Developer Experience team in Big Data scope to support Migration from Hadoop / Oozie (Spark, Perl, etc) legacy setup to a modern platform (AWS / S3 / Snowflake / Airflow). The Workflow Management Developer Experience (WFME) team is an addition to the existing Workflow Management (WFM) to support the customer with new functionality and processes built on top of the platform. Existing team is building the platform capability, while the WFME team will be responsible for supporting customers with migration from the legacy scope, building processes, new operators and integrations required to make WFM service truly customer centric.\n\nWhat You Will Be Doing\n\nWe\u2019re looking for an experienced Tech and People Leader to join the Workflow Management Developer Experience team. The candidate should be able to quickly build an e2e view of the legacy and modern tech stacks. Having prior experience in Hadoop / Oozie and AWS / Airflow is a must.\n\nTo support the migration we need to analyze the existing workflows and build tools for their migration. This involved reading code to understand nuances and translate them into technical requirements and implementations for the new platform, with a strong focus on Migration Tooling and building operators. The scope is huge, around 4000 workflows. The ideal candidate should have some good examples where they would address a similar problem in the past and enjoy using the \u201cDivide and Conquer\u201d principle while doing this kind of work. They should be able to articulate their thinking to a wide range of collaborators such as Product, Engineering management, team members and users.\n\u2022 Lead people\u2019s performance and wellbeing\n\u2022 Help setting up quarterly and yearly objectives, estimate work, lead team and personal performance\n\u2022 Mentor and coach team members\n\u2022 Improve team routines and processes for better performance based on the state of the team and the service. Make sure the team follows the standard processes, learn and embrace\n\u2022 Handle hiring and compensations\n\u2022 Help the team to communicate externally and internally, resolve conflicts\n\u2022 Own end-to-end data and data applications by defining, monitoring and adjusting relevant SLIs and SLOs\n\u2022 Handle, mitigate and learn from incidents in a manner that improves the overall system health\n\u2022 Continuously evolve your craft by keeping up to date with the latest developments in data engineering and related technologies, introducing them to the community and promoting their application in areas where they can generate impact\n\u2022 Actively chip in to Data Engineering at Booking.com through training, exploration of new technologies, interviewing, onboarding and mentoring colleagues\n\u2022 Push for improvements, scaling and extending data engineering tooling and infrastructure, collaborating with central teams\n\nYou Are a Role Model Of The Booking.com Values\n\u2022 Thrive with change and gets things done\n\u2022 Demand a high standard of excellence in their craft\n\u2022 Accept the opportunity to improve\n\u2022 Understand success starts with accountability and ownership\n\u2022 Care more about being successful and reaching goals together than individually\n\u2022 Curious, experiments and continuously learns\n\u2022 Humble, open, friendly and remember that diversity gives us strength\n\nTechnology, Craft & Delivery\n\u2022 Take leadership in driving tactical and strategic technical decisions in order to fill gaps in the modern Workflow Management solution and migrate from the legacy stack\n\u2022 Develop vital operators and integrations\n\u2022 Maintain and share the E2E vision on the scope of the service and migration\n\u2022 Analysing oozie jobs, finding in scheduling and compute usage\n\u2022 Proactively chip in to the migration strategy\n\u2022 Drive requirements for the migration tooling\n\u2022 Implement user friendly Tooling to support oozie migration to Airflow taking into account the nuances of the legacy structures and the modern tech stack\n\u2022 Train and support users with the migration stack:\n\u2022 AWS / Airflow / Snowflake\n\u2022 K8S\n\u2022 Hadoop / Hive / Spark / Java / Python / Scala\n\u2022 SQL\n\u2022 Terraform\n\u2022 APIs\n\nArchitecture & Product Strategy\n\u2022 Define, shape and deliver the roadmap\n\u2022 Build new products, processes and operational plans\n\u2022 Negotiate on the strategic importance of own product roadmap features\n\u2022 Drive innovation in the team\n\u2022 Lead the architecture across the team\n\nWhat You Will Bring\n\u2022 6+ years of experience in Data Engineering\n\u2022 At least 3 years of experience leading and developing a team of engineers in a fast-paced and sophisticated environment\n\u2022 SRE experience is a plus\n\u2022 Working in Developer Experience team is a plus\n\u2022 Experience working with Kubernetes\n\u2022 Experience deploying data processing solutions on cloud providers is a plus\n\u2022 Experience with big data tools (eg Spark) and ML frameworks (eg TensorFlow) commonly used for ML is a plus\n\u2022 Experience working with workflow management tools such as Apache Airflow is a plus\n\u2022 Ability to lead multiple teams is a plus\n\u2022 A deep understanding of software development in a team, and a track record of developing and shipping software\n\u2022 Strong technical skills (Coding & System design) with ability to get hands-on with your team if needed\n\u2022 Excellent communicator with strong collaborator management experience, good commercial awareness and technical vision\n\u2022 You have driven successful technical, business and people related initiatives that improved productivity, performance and quality\n\u2022 You are a humble and thoughtful technology leader, you lead by example and gain your teammates\u2019 respect through actions, not the title\n\u2022 Analytical skills and data-driven mentality\n\u2022 You are required to live within a commutable distance from your assigned office location\n\nWhat We'll Provide\n\nBooking.com\u2019s Total Rewards Philosophy is not only about compensation but also about benefits. Our Total Rewards are aimed to make it easier for you to experience all that life has to offer on your terms, so you can focus on what really matters. We offer competitive compensation as well as thoughtful, valuable, and even fun benefits which include:\n\u2022 Medical, life, and disability insurance*\n\u2022 Annual paid time off and generous paid leave scheme including: parent, grandparent, bereavement, sick and care leave\n\u2022 Industry leading product discounts for yourself, friends, and family, including automatic Genius Level 3 status and quarterly Booking.com wallet credit\n\u2022 Free access to online learning platforms, mentorship programs, and a complimentary Headspace membership\n\u2022 Collaborative, friendly and diverse culture\n\u2022 Referral Program\n\u2022 For this role will have a salary range of: $173,700 - $191,000\n\u2022 Additional Annual or Quarterly bonus potential (role dependent)\n\u2022 *Please note that while our philosophy is the same in every location, benefits may differ by office/country.\n\nShould you require accommodation to meet the essential functions of this job, please let us know.\n\nPre- Employment Screening\n\nIf your application is successful, your personal data may be used for a pre-employment screening check by a third party as permitted by applicable law. Depending on the vacancy and applicable law, a pre-employment screening may include employment history, education and other information (such as media information) that may be necessary for determining your qualifications and suitability for the position.", "job_is_remote": false, "job_posted_at_timestamp": 1703153415, "job_posted_at_datetime_utc": "2023-12-21T10:10:15.000Z", "job_city": "Washington", "job_state": "DC", "job_country": "US", "job_latitude": 38.907192, "job_longitude": -77.03687, "job_benefits": ["health_insurance", "paid_time_off"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=nGN54_vqmpPJoYGKAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-28T21:26:05.000Z", "job_offer_expiration_timestamp": 1706477165, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 72, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["The candidate should be able to quickly build an e2e view of the legacy and modern tech stacks", "Having prior experience in Hadoop / Oozie and AWS / Airflow is a must", "They should be able to articulate their thinking to a wide range of collaborators such as Product, Engineering management, team members and users", "Understand success starts with accountability and ownership", "Care more about being successful and reaching goals together than individually", "Curious, experiments and continuously learns", "Humble, open, friendly and remember that diversity gives us strength", "Hadoop / Hive / Spark / Java / Python / Scala", "SQL", "6+ years of experience in Data Engineering", "At least 3 years of experience leading and developing a team of engineers in a fast-paced and sophisticated environment", "Experience working with Kubernetes", "A deep understanding of software development in a team, and a track record of developing and shipping software", "Strong technical skills (Coding & System design) with ability to get hands-on with your team if needed", "Excellent communicator with strong collaborator management experience, good commercial awareness and technical vision", "You have driven successful technical, business and people related initiatives that improved productivity, performance and quality", "You are a humble and thoughtful technology leader, you lead by example and gain your teammates\u2019 respect through actions, not the title", "Analytical skills and data-driven mentality", "You are required to live within a commutable distance from your assigned office location"], "Responsibilities": ["In short, you would play a critical role on the team building the Analytics experience that solves Booking\u2019s hardest data consumption challenges", "The Workflow Management Developer Experience (WFME) team is an addition to the existing Workflow Management (WFM) to support the customer with new functionality and processes built on top of the platform", "Existing team is building the platform capability, while the WFME team will be responsible for supporting customers with migration from the legacy scope, building processes, new operators and integrations required to make WFM service truly customer centric", "This involved reading code to understand nuances and translate them into technical requirements and implementations for the new platform, with a strong focus on Migration Tooling and building operators", "Lead people\u2019s performance and wellbeing", "Help setting up quarterly and yearly objectives, estimate work, lead team and personal performance", "Mentor and coach team members", "Improve team routines and processes for better performance based on the state of the team and the service", "Make sure the team follows the standard processes, learn and embrace", "Handle hiring and compensations", "Help the team to communicate externally and internally, resolve conflicts", "Own end-to-end data and data applications by defining, monitoring and adjusting relevant SLIs and SLOs", "Handle, mitigate and learn from incidents in a manner that improves the overall system health", "Continuously evolve your craft by keeping up to date with the latest developments in data engineering and related technologies, introducing them to the community and promoting their application in areas where they can generate impact", "Actively chip in to Data Engineering at Booking.com through training, exploration of new technologies, interviewing, onboarding and mentoring colleagues", "Push for improvements, scaling and extending data engineering tooling and infrastructure, collaborating with central teams", "You Are a Role Model Of The Booking.com Values", "Take leadership in driving tactical and strategic technical decisions in order to fill gaps in the modern Workflow Management solution and migrate from the legacy stack", "Develop vital operators and integrations", "Maintain and share the E2E vision on the scope of the service and migration", "Analysing oozie jobs, finding in scheduling and compute usage", "Proactively chip in to the migration strategy", "Drive requirements for the migration tooling", "AWS / Airflow / Snowflake", "Define, shape and deliver the roadmap", "Build new products, processes and operational plans", "Negotiate on the strategic importance of own product roadmap features", "Drive innovation in the team", "Lead the architecture across the team"], "Benefits": ["Booking.com\u2019s Total Rewards Philosophy is not only about compensation but also about benefits", "We offer competitive compensation as well as thoughtful, valuable, and even fun benefits which include:", "Medical, life, and disability insurance*", "Annual paid time off and generous paid leave scheme including: parent, grandparent, bereavement, sick and care leave", "Industry leading product discounts for yourself, friends, and family, including automatic Genius Level 3 status and quarterly Booking.com wallet credit", "Free access to online learning platforms, mentorship programs, and a complimentary Headspace membership", "Collaborative, friendly and diverse culture", "Referral Program", "For this role will have a salary range of: $173,700 - $191,000", "Additional Annual or Quarterly bonus potential (role dependent)"]}, "job_job_title": "Engineering manager", "job_posting_language": "en", "job_onet_soc": "15111100", "job_onet_job_zone": "5"}, {"employer_name": "Diverse Lynx", "employer_logo": "https://c.smartrecruiters.com/sr-company-images-prod-aws-dc5/5525418ee4b08c20c6910cd4/default_social_logo/300x300?r=s3-eu-central-1&_1534874151146", "employer_website": "http://www.diverselynx.com", "employer_company_type": null, "job_publisher": "Jobs Trabajo.org", "job_id": "31FcQMXuMzGHU6IeAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer - Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-5039c2fe6cf05ffe8a902a285fc20c79", "job_apply_is_direct": false, "job_apply_quality_score": 0.4424, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-5039c2fe6cf05ffe8a902a285fc20c79", "is_direct": false}], "job_description": "Job Title: Data Engineer\nLocation: Bay Area, CA(Sunnyvale)\nDuration: Contract / FullTime Position\n\nJob Description:\nWe're seeking data engineers to work on scalable data workflows to enrich knowledge graph. As an expert in developing software to manage large, dynamic datasets, you'll be building and optimizing pipelines for data ingestion, cleaning, transformation and evaluation to support a rapidly scaling organization.\n\nKey Qualifications\n~2+ years of experience as a Software Engineer processing large-scale datasets\n~ Excellent programming skills - e.g. Python, Go, Java, Scala.\n~ Excellent problem-solving, analytic, and debugging skills\n~ Solid computer science and systems foundations; ability to quickly learn new domains\n~ Proven software development skills in UNIX-type OS (e.g. Linux, Mac OS)\n~ Experience working with large data sets and pipelines, ideally using the Apache software stack (e.g. Spark, HBase)\n~ Experience with continuous integration and continuous development solutions (e.g. Jenkins, etc.)\n~ Experience with cloud-native deployment is a good plus (e.g. Kubernetes)\n~ Good communication skills and teamwork\n~ Passion for building great products\n~ Experience in tooling and streamlining workflows in complex processes\n\nDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.\n#J-18808-Ljbffr", "job_is_remote": false, "job_posted_at_timestamp": 1703117566, "job_posted_at_datetime_utc": "2023-12-21T00:12:46.000Z", "job_city": "San Francisco", "job_state": "CA", "job_country": "US", "job_latitude": 37.77493, "job_longitude": -122.41942, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=31FcQMXuMzGHU6IeAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["~2+ years of experience as a Software Engineer processing large-scale datasets", "~ Excellent programming skills - e.g. Python, Go, Java, Scala", "~ Excellent problem-solving, analytic, and debugging skills", "~ Solid computer science and systems foundations; ability to quickly learn new domains", "~ Proven software development skills in UNIX-type OS (e.g. Linux, Mac OS)", "~ Experience working with large data sets and pipelines, ideally using the Apache software stack (e.g", "Spark, HBase)", "~ Experience with continuous integration and continuous development solutions (e.g", "Jenkins, etc.)", "~ Experience with cloud-native deployment is a good plus (e.g", "Kubernetes)", "~ Good communication skills and teamwork", "~ Passion for building great products", "~ Experience in tooling and streamlining workflows in complex processes"], "Responsibilities": ["We're seeking data engineers to work on scalable data workflows to enrich knowledge graph", "As an expert in developing software to manage large, dynamic datasets, you'll be building and optimizing pipelines for data ingestion, cleaning, transformation and evaluation to support a rapidly scaling organization"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "Niagara Bottling", "employer_logo": "https://dg6qn11ynnp6a.cloudfront.net/wp-content/uploads/2017/10/02102317/Niagara-Logo-1020x510.png", "employer_website": "http://www.niagarawater.com", "employer_company_type": null, "job_publisher": "Niagara Bottling", "job_id": "hEnYuczjvlRrRyUZAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "AI/Machine Learning Engineer II", "job_apply_link": "https://careers.niagarawater.com/us/en/job/R40188/AI-Machine-Learning-Engineer-II", "job_apply_is_direct": false, "job_apply_quality_score": 0.9116, "apply_options": [{"publisher": "Niagara Bottling", "apply_link": "https://careers.niagarawater.com/us/en/job/R40188/AI-Machine-Learning-Engineer-II", "is_direct": false}, {"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Niagara-Bottling,-LLC/Job/AI-Machine-Learning-Engineer-II/-in-Diamond-Bar,CA?jid=0915cfecae1ae6ef", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/ai-machine-learning-engineer-ii-niagara-bottling-JV_IC1146774_KO0,31_KE32,48.htm?jl=1008889727794", "is_direct": true}, {"publisher": "Careers In Food", "apply_link": "https://m.careersinfood.com/aimachine-learning-engineer-ii-job-1594254.htm", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=c19f2fd54e1b03fa", "is_direct": false}, {"publisher": "Monster", "apply_link": "https://www.monster.com/job-openings/ai-machine-learning-engineer-ii-diamond-bar-ca--2387a45c-1e0b-42c6-bf82-3f7cf705adde", "is_direct": false}, {"publisher": "Robots.Jobs", "apply_link": "https://robots.jobs/jobs/aimachine-learning-engineer-ii-in-diamond-bar-california-us/", "is_direct": false}, {"publisher": "Levels.fyi", "apply_link": "https://www.levels.fyi/jobs?jobId=128591730036351686", "is_direct": false}], "job_description": "At Niagara, we\u2019re looking for Team Members who want to be part of achieving our mission to provide our customers the highest quality most affordable bottled water.\n\nConsider applying here, if you want to:\n\u2022 Work in an entrepreneurial and dynamic environment with a chance to make an impact.\n\u2022 Develop lasting relationships with great people.\n\u2022 Have the opportunity to build a satisfying career.\n\nWe offer competitive compensation and benefits packages for our Team Members.\n\nAI/Machine Learning Engineer II\n\nAs an AI/Machine Learning Engineer, you'll work on training, evaluating, and serving large AI models, internet-scale dataset building, and prototype new research and product ideas. Your responsibilities include pre-training, optimizing inference throughput, continual learning, implementing interfaces, designing, testing, and optimizing new neural net architectures, and internet-scale data scraping. Additionally, you will work with Product Management and data scientists to build and constantly lead excellence in our products.\n\nDetailed Description\n\u2022 Develop & Design Predictive analytic systems through Continuous Monitoring of critical asset parameters\n\u2022 Data Analysis: Conduct in-depth analysis of large and complex datasets related to equipment sensor data, maintenance logs, and other relevant sources using machine learning programs and libraries such as Python with libraries like NumPy, Pandas, and SciPy.\n\u2022 Model Development: Design and implement advanced predictive maintenance models using machine learning algorithms and techniques from libraries such as scikit-learn, TensorFlow, or PyTorch. Apply regression, classification, clustering, and time series analysis algorithms to develop accurate predictive models.\n\u2022 Feature Engineering: Utilize machine learning libraries to extract, transform, and engineer relevant features from raw data. Use feature selection techniques and data preprocessing methods available in libraries like scikit-learn to optimize model performance.\n\u2022 Model Training and Evaluation: Utilize machine learning frameworks to train and fine-tune predictive maintenance models using historical data. Evaluate model performance using metrics such as accuracy, precision, recall, and F1-score, leveraging libraries like scikit-learn or Keras.\n\u2022 Data Visualization and Reporting: Utilize data visualization libraries like Matplotlib or Plotly to create interactive visualizations and reports that effectively communicate insights derived from predictive maintenance models.\n\u2022 Collaboration and Cross-functional Communication: Collaborate with maintenance engineers, data engineers, domain experts, and stakeholders from different departments, using tools like Jupyter Notebooks or Git, to share code, insights, and results. Foster effective communication and knowledge sharing within the team.\n\u2022 Data Governance and Quality Assurance: Ensure data integrity, quality, and security throughout the predictive maintenance process. Implement data cleansing, validation, and quality control measures using libraries like Pandas or PySpark to ensure accurate and reliable model outputs.\n\u2022 Research and Innovation: Stay updated with the latest advancements in machine learning and predictive maintenance. Explore new machine learning algorithms, libraries, and techniques that can enhance predictive maintenance capabilities.\n\u2022 Documentation: Maintain clear and concise documentation of methodologies, code implementations, and model specifications using tools like Markdown or Sphinx. Document experiments, findings, and best practices for future reference and knowledge sharing.\n\u2022 Continuous Improvement: Continuously explore ways to improve model performance, scalability, and efficiency using machine learning libraries and frameworks. Keep up with the latest research papers and developments to incorporate cutting-edge techniques into predictive maintenance models.\n\u2022 Training and Knowledge Transfer: Share expertise and insights with colleagues, stakeholders, and other team members. Conduct training sessions or workshops to promote understanding and effective utilization of machine learning programs and libraries.\n\u2022 Develop and design advanced systems for asset reliability for all manufacturing assets across all Niagara plants.\n\u2022 Machine Learning Pipeline Development (ML Ops): Design, develop, and implement end-to-end machine learning pipelines, from data ingestion and preprocessing to model training, evaluation, and deployment. Implement automation and orchestration techniques to ensure reproducibility and scalability.\n\u2022 Data Analysis and Modeling: Apply advanced statistical analysis and machine learning techniques to analyze large, complex datasets. Develop predictive models, classification algorithms, and optimization algorithms to solve business problems and generate actionable insights.\n\u2022 Data Cleaning and Preprocessing: Clean, transform, and preprocess data to ensure data quality and suitability for analysis. Handle missing data, outliers, and noise to ensure accurate modeling results.\n\u2022 Feature Engineering: Identify relevant features and variables from raw data and create new features to enhance model performance. Conduct feature selection and extraction techniques to improve model accuracy and interpretability.\n\u2022 Model Development and Evaluation: Develop and implement predictive models using machine learning algorithms such as regression, decision trees, random forests, neural networks, or deep learning. Evaluate model performance, conduct hypothesis testing, and fine-tune models to optimize accuracy and generalizability.\n\u2022 Data Visualization and Communication: Visualize and communicate complex data analysis and model results to non-technical stakeholders effectively. Prepare clear and concise reports, dashboards, and presentations to convey insights and recommendations.\n\u2022 Collaborative Problem Solving: Collaborate with cross-functional teams including data engineers, business analysts, and domain experts to understand business challenges and develop data-driven solutions. Participate in brainstorming sessions and provide technical expertise to drive innovative problem-solving approaches.\n\u2022 Experimental Design and A/B Testing: Design and execute experiments to test hypotheses, measure the impact of interventions, and optimize business outcomes. Conduct A/B testing and analyze experimental results to provide insights and recommendations for improvement.\n\u2022 Continuous Integration and Deployment: Implement CI/CD (Continuous Integration/Continuous Deployment) practices to automate the testing, integration, and deployment of machine learning models. Establish version control processes for models and associated code.\n\u2022 Security and Compliance: Ensure the security and compliance of machine learning systems, including data privacy, access controls, and regulatory requirements. Implement secure storage and transmission of sensitive data.\n\u2022 Performance Optimization: Optimize the performance and efficiency of machine learning models and infrastructure, including resource allocation, parallel processing, and distributed computing. Identify and resolve bottlenecks and scalability challenges.\n\u2022 Documentation and Knowledge Sharing: Document the design, implementation, and maintenance processes of machine learning pipelines and infrastructure. Share knowledge and best practices with the team and stakeholders to foster a culture of learning and improvement.\n\u2022 Systems Reliability Engineer is estimated to travel 10-20%\n\u2022 Please note this job description is not a full list of activities, duties, or responsibilities required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without prior notice.\n\nWork Experience/KSA\u2019s\n\u2022 Required:\n\u2022 2-4 years \u2013 Experience in Industrial ML/Automation/Data Science or other related fields\n\u2022 2-4 years \u2013 Experience in a position in the Industrial ML/Automation/Data Science field in manufacturing\n\u2022 Experience may include a combination of work experience and education\n\u2022 Preferred:\n\u2022 3-5 years \u2013 Experience in Industrial ML/Automation/Data Science or other related fields\n\u2022 3-5 years \u2013 Experience in a position in the Industrial ML/Automation/Data Science field in manufacturing\n\u2022 Experience may include a combination of work experience and education\n\nPreferred Competencies and Skills\n\u2022 3+ years of industry experience in developing and productionizing applied machine learning for solving business problems\n\u2022 Proficiency in Azure ML Studio, AWS and related tools for model development, deployment, and monitoring.\n\u2022 Expertise and experience in both supervised and unsupervised learning\n\u2022 Expertise and experience in neural networks or reinforcement learning\n\u2022 Expertise and experience in modern NLP, large language models, or generative AI\n\u2022 Proficiency with Python\n\u2022 Proficiency in using query languages such as SQL, Hive, Pig. Etc.\n\u2022 Experience working with machine learning frameworks such as TensorFlow, PyTorch, Spark ML, scikit-learn, or related frameworks\n\u2022 Preferred experience with common data science toolkits, such as R, Weka, Python with focus on NumPy, Matplotlib and Pandas, MATLAB, etc.\n\u2022 Curious, passion for learning, self-motivated, and excited about solving open-ended challenges.\n\u2022 The ability to explain complex findings and technical approaches to a variety of audiences\n\u2022 Desire to work with amazing, passionate all kinds of partners who care about solving challenging problems to improve productivity for its users\n\u2022 Proficiency in, but not limited to:\n\u2022 Microsoft Office Applications \u2013 Word, Excel, PowerPoint, Outlook, Project, Visio, etc.\n\u2022 Project Management tools\n\u2022 Able to translate data into recommendable actions to senior management\n\u2022 Strong analytical and problem-solving skills\n\u2022 Able to work with minimal supervision\n\u2022 Detail-oriented with excellent oral and written communication skills\n\u2022 Able to execute tasks in a very dynamic and ever-changing environment\n\nEducation\n\u2022 Minimum Required:\n\u2022 Bachelor's Degree in Computer/Industrial/Automation/Data Science Engineering or other related fields or equivalent experience\n\u2022 Preferred:\n\u2022 Master's Degree in Computer/Industrial/Automation/Data Science Engineering\n\nTypical Compensation Range\nPay Rate Type: Salary\n\n$95,301.00 - $138,186.00 / Yearly\n\nBenefits\n\nhttps://careers.niagarawater.com/us/en/benefits\n\nAny employment agency, person or entity that submits a r\u00e9sum\u00e9 into this career site or to a hiring manager does so with the understanding that the applicant's r\u00e9sum\u00e9 will become the property of Niagara Bottling, LLC. Niagara Bottling, LLC will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.\n\nEmployment agencies that have fee agreements with Niagara Bottling, LLC and have been engaged on a search shall submit r\u00e9sum\u00e9 to the designated Niagara Bottling, LLC recruiter or, upon authorization, submit r\u00e9sum\u00e9 into this career site to be eligible for placement fees.\nNiagara Plant Name\nCORP-MAIN", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Diamond Bar", "job_state": "CA", "job_country": "US", "job_latitude": 34.000996, "job_longitude": -117.8112, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=hEnYuczjvlRrRyUZAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": null, "job_offer_expiration_timestamp": null, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["2-4 years \u2013 Experience in Industrial ML/Automation/Data Science or other related fields", "Experience may include a combination of work experience and education", "3-5 years \u2013 Experience in a position in the Industrial ML/Automation/Data Science field in manufacturing", "Experience working with machine learning frameworks such as TensorFlow, PyTorch, Spark ML, scikit-learn, or related frameworks", "Curious, passion for learning, self-motivated, and excited about solving open-ended challenges", "The ability to explain complex findings and technical approaches to a variety of audiences", "Desire to work with amazing, passionate all kinds of partners who care about solving challenging problems to improve productivity for its users", "Proficiency in, but not limited to:", "Microsoft Office Applications \u2013 Word, Excel, PowerPoint, Outlook, Project, Visio, etc", "Project Management tools", "Able to translate data into recommendable actions to senior management", "Strong analytical and problem-solving skills", "Able to work with minimal supervision", "Detail-oriented with excellent oral and written communication skills", "Able to execute tasks in a very dynamic and ever-changing environment", "Bachelor's Degree in Computer/Industrial/Automation/Data Science Engineering or other related fields or equivalent experience"], "Responsibilities": ["As an AI/Machine Learning Engineer, you'll work on training, evaluating, and serving large AI models, internet-scale dataset building, and prototype new research and product ideas", "Your responsibilities include pre-training, optimizing inference throughput, continual learning, implementing interfaces, designing, testing, and optimizing new neural net architectures, and internet-scale data scraping", "Additionally, you will work with Product Management and data scientists to build and constantly lead excellence in our products", "Develop & Design Predictive analytic systems through Continuous Monitoring of critical asset parameters", "Data Analysis: Conduct in-depth analysis of large and complex datasets related to equipment sensor data, maintenance logs, and other relevant sources using machine learning programs and libraries such as Python with libraries like NumPy, Pandas, and SciPy", "Model Development: Design and implement advanced predictive maintenance models using machine learning algorithms and techniques from libraries such as scikit-learn, TensorFlow, or PyTorch", "Apply regression, classification, clustering, and time series analysis algorithms to develop accurate predictive models", "Feature Engineering: Utilize machine learning libraries to extract, transform, and engineer relevant features from raw data", "Use feature selection techniques and data preprocessing methods available in libraries like scikit-learn to optimize model performance", "Model Training and Evaluation: Utilize machine learning frameworks to train and fine-tune predictive maintenance models using historical data", "Evaluate model performance using metrics such as accuracy, precision, recall, and F1-score, leveraging libraries like scikit-learn or Keras", "Data Visualization and Reporting: Utilize data visualization libraries like Matplotlib or Plotly to create interactive visualizations and reports that effectively communicate insights derived from predictive maintenance models", "Collaboration and Cross-functional Communication: Collaborate with maintenance engineers, data engineers, domain experts, and stakeholders from different departments, using tools like Jupyter Notebooks or Git, to share code, insights, and results", "Data Governance and Quality Assurance: Ensure data integrity, quality, and security throughout the predictive maintenance process", "Implement data cleansing, validation, and quality control measures using libraries like Pandas or PySpark to ensure accurate and reliable model outputs", "Research and Innovation: Stay updated with the latest advancements in machine learning and predictive maintenance", "Explore new machine learning algorithms, libraries, and techniques that can enhance predictive maintenance capabilities", "Documentation: Maintain clear and concise documentation of methodologies, code implementations, and model specifications using tools like Markdown or Sphinx", "Document experiments, findings, and best practices for future reference and knowledge sharing", "Continuous Improvement: Continuously explore ways to improve model performance, scalability, and efficiency using machine learning libraries and frameworks", "Keep up with the latest research papers and developments to incorporate cutting-edge techniques into predictive maintenance models", "Training and Knowledge Transfer: Share expertise and insights with colleagues, stakeholders, and other team members", "Conduct training sessions or workshops to promote understanding and effective utilization of machine learning programs and libraries", "Develop and design advanced systems for asset reliability for all manufacturing assets across all Niagara plants", "Machine Learning Pipeline Development (ML Ops): Design, develop, and implement end-to-end machine learning pipelines, from data ingestion and preprocessing to model training, evaluation, and deployment", "Implement automation and orchestration techniques to ensure reproducibility and scalability", "Data Analysis and Modeling: Apply advanced statistical analysis and machine learning techniques to analyze large, complex datasets", "Develop predictive models, classification algorithms, and optimization algorithms to solve business problems and generate actionable insights", "Data Cleaning and Preprocessing: Clean, transform, and preprocess data to ensure data quality and suitability for analysis", "Handle missing data, outliers, and noise to ensure accurate modeling results", "Feature Engineering: Identify relevant features and variables from raw data and create new features to enhance model performance", "Conduct feature selection and extraction techniques to improve model accuracy and interpretability", "Model Development and Evaluation: Develop and implement predictive models using machine learning algorithms such as regression, decision trees, random forests, neural networks, or deep learning", "Evaluate model performance, conduct hypothesis testing, and fine-tune models to optimize accuracy and generalizability", "Data Visualization and Communication: Visualize and communicate complex data analysis and model results to non-technical stakeholders effectively", "Prepare clear and concise reports, dashboards, and presentations to convey insights and recommendations", "Collaborative Problem Solving: Collaborate with cross-functional teams including data engineers, business analysts, and domain experts to understand business challenges and develop data-driven solutions", "Participate in brainstorming sessions and provide technical expertise to drive innovative problem-solving approaches", "Experimental Design and A/B Testing: Design and execute experiments to test hypotheses, measure the impact of interventions, and optimize business outcomes", "Conduct A/B testing and analyze experimental results to provide insights and recommendations for improvement", "Continuous Integration and Deployment: Implement CI/CD (Continuous Integration/Continuous Deployment) practices to automate the testing, integration, and deployment of machine learning models", "Establish version control processes for models and associated code", "Security and Compliance: Ensure the security and compliance of machine learning systems, including data privacy, access controls, and regulatory requirements", "Implement secure storage and transmission of sensitive data", "Performance Optimization: Optimize the performance and efficiency of machine learning models and infrastructure, including resource allocation, parallel processing, and distributed computing", "Identify and resolve bottlenecks and scalability challenges", "Documentation and Knowledge Sharing: Document the design, implementation, and maintenance processes of machine learning pipelines and infrastructure", "Share knowledge and best practices with the team and stakeholders to foster a culture of learning and improvement"], "Benefits": ["Have the opportunity to build a satisfying career", "We offer competitive compensation and benefits packages for our Team Members", "Pay Rate Type: Salary", "$95,301.00 - $138,186.00 / Yearly"]}, "job_job_title": "Learning engineer", "job_posting_language": "en", "job_onet_soc": "15111100", "job_onet_job_zone": "5", "job_occupational_categories": ["Engineering"]}, {"employer_name": "Oracle", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png", "employer_website": "http://www.oracle.com", "employer_company_type": "Information", "job_publisher": "Jobs Trabajo.org", "job_id": "Pd9uwY1qYydZtx3NAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer - Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-601e9e4c1df4dfdd3f93c84c5eec21ee", "job_apply_is_direct": false, "job_apply_quality_score": 0.4489, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-601e9e4c1df4dfdd3f93c84c5eec21ee", "is_direct": false}], "job_description": "Must have the ability to obtain federal security clearance necessary for this role which requires being a US citizen**\nBuilding off our Cloud momentum, Oracle has formed a new organization - Oracle Health Applications & Infrastructure. This team will focus on product development and product strategy for Oracle Health while building out a complete platform supporting modernized, automated healthcare. This is a net new line of business, constructed with an entrepreneurial spirit that promotes an energetic and creative environment. We are unencumbered and will need your contribution to make it a world-class engineering center with a focus on excellence.Identify and help document for the Data Center Engineering knowledge base.\nAssist with process improvements and standard methodologies.\nEducation Qualifications: 4-year degree or higher education, computer science or technology, or equivalent work experience.\nReceipt of the appropriate government security clearance card applicable for your position.\nDue to the client contract you will be assigned, this position requires you to be a U.The night shifts are four 10 hour shifts from 7pm to 5am Monday evenings to Tuesday mornings thru Thursday evenings to Friday mornings. The Data Center Engineering team is a small, close-knit team that comes from a wide range of backgrounds and experiences. Oracle Health Mission Statement:\nOracle Health is putting humans at the heart of the conversation and what the healthcare experience needs to look like - for patients, providers, payers, and the population. This will allow a patient to get point of care from anyone, anywhere or any device by providing the practitioner medical information leveraging global data.\nLife at Oracle and Equal Opportunity\nAn Oracle career can span industries, roles, Countries and cultures, giving you the opportunity to flourish in new roles and innovate, while blending work life in. Oracle has thrived through 40+ years of change by innovating and operating with integrity while delivering for the top companies in almost every industry.\nOracle offers a highly competitive suite of Employee Benefits designed on the principles of parity, consistency, and affordability. The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more. We also encourage our employees to engage in the culture of giving back to the communities where we live and do business.\nAt Oracle, we believe that innovation starts with diversity and inclusion and to create the future we need talent from various backgrounds, perspectives, and abilities. We ensure that individuals with disabilities are provided reasonable accommodation to successfully participate in the job application, interview process, and in potential roles to perform crucial job functions.\nPerform performance trend analysis and manage the server/network capacity.\n+ React to potential problems using automation, scheduling, and monitoring tools -- escalating to management where appropriate.\n+ Participate in configuration and implement technical solutions to enhance and/or troubleshoot the system.\n+ Replacing failed server components, upgrading servers to meet the demands of applications, and proactively maintaining hardware to ensure efficient performance for its entire lifecycle.\n+ Responding to client emergencies in Oracle Health\u2019s data centers as the Data Center Engineering team is the very last line of defense for clients before the server hardware itself.\n+ The Data Center Engineering team is the bridge between Oracle Health Application & Infrastructure and third-party contractors and vendor companies who interact with the data centers.\n+ Maintaining a spare parts inventory with Data Center Capacity and Asset Management.\n08 per hour; Oracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle\u2019s differing products, industries and lines of business.\nOracle offers a comprehensive benefits package which includes the following:\nMedical, dental, and vision insurance, including expert medical opinion\nShort term disability and long term disability\nSupplemental life insurance (Employee/Spouse/Child)\nHealth care and dependent care Flexible Spending Accounts\nPre-tax commuter and parking benefits\nFlexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position. 11 paid holidays\nEmployee Stock Purchase Plan\nFinancial planning and group legal\nAn Oracle career can span industries, roles, Countries and cultures, giving you the opportunity to flourish in new roles and innovate, while blending work life in. Oracle has thrived through 40+ years of change by innovating and operating with integrity while delivering for the top companies in almost every industry.\nIn order to nurture the talent that makes this happen, we are committed to an inclusive culture that celebrates and values diverse insights and perspectives, a workforce that inspires thought leadership and innovation.\nOracle offers a highly competitive suite of Employee Benefits designed on the principles of parity, consistency, and affordability. The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more. We also encourage our employees to engage in the culture of giving back to the communities where we live and do business.\nAt Oracle, we believe that innovation starts with diversity and inclusion and to create the future we need talent from various backgrounds, perspectives, and abilities. We ensure that individuals with disabilities are provided reasonable accommodation to successfully participate in the job application, interview process, and in potential roles. Oracle is an Equal Employment Opportunity Employer . All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans\u2019 status, or any other characteristic protected by law. Oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law.\n\u2022 **", "job_is_remote": false, "job_posted_at_timestamp": 1703129811, "job_posted_at_datetime_utc": "2023-12-21T03:36:51.000Z", "job_city": "Raleigh", "job_state": "NC", "job_country": "US", "job_latitude": 35.77959, "job_longitude": -78.638176, "job_benefits": ["retirement_savings", "dental_coverage", "health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Pd9uwY1qYydZtx3NAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Must have the ability to obtain federal security clearance necessary for this role which requires being a US citizen**", "Education Qualifications: 4-year degree or higher education, computer science or technology, or equivalent work experience"], "Responsibilities": ["Identify and help document for the Data Center Engineering knowledge base", "Assist with process improvements and standard methodologies", "This will allow a patient to get point of care from anyone, anywhere or any device by providing the practitioner medical information leveraging global data", "Perform performance trend analysis and manage the server/network capacity", "React to potential problems using automation, scheduling, and monitoring tools -- escalating to management where appropriate", "Participate in configuration and implement technical solutions to enhance and/or troubleshoot the system", "Replacing failed server components, upgrading servers to meet the demands of applications, and proactively maintaining hardware to ensure efficient performance for its entire lifecycle", "Responding to client emergencies in Oracle Health\u2019s data centers as the Data Center Engineering team is the very last line of defense for clients before the server hardware itself", "The Data Center Engineering team is the bridge between Oracle Health Application & Infrastructure and third-party contractors and vendor companies who interact with the data centers", "Maintaining a spare parts inventory with Data Center Capacity and Asset Management"], "Benefits": ["The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more", "Medical, dental, and vision insurance, including expert medical opinion", "Short term disability and long term disability", "Supplemental life insurance (Employee/Spouse/Child)", "Health care and dependent care Flexible Spending Accounts", "Pre-tax commuter and parking benefits", "Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position", "11 paid holidays", "Employee Stock Purchase Plan"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "511210", "job_naics_name": "Software Publishers"}, {"employer_name": "PACCAR", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/PACCAR-logo.svg/2560px-PACCAR-logo.svg.png", "employer_website": "http://www.paccar.com", "employer_company_type": "Manufacturing", "job_publisher": "Talent.com", "job_id": "w1LPJcjUfwL_UkkwAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "It intern summer 2024", "job_apply_link": "https://www.talent.com/view?id=39c5d578f7a8", "job_apply_is_direct": false, "job_apply_quality_score": 0.5973, "apply_options": [{"publisher": "Talent.com", "apply_link": "https://www.talent.com/view?id=39c5d578f7a8", "is_direct": false}, {"publisher": "Washington Jobs - Tarta.ai", "apply_link": "https://washington.tarta.ai/j/wOl8b4wBcRajQcTjXqUB1223-it-intern-summer-2024-in-renton-washington-at-paccar", "is_direct": false}, {"publisher": "Jobilize", "apply_link": "https://www.jobilize.com/job/us-wa-renton-it-intern-summer-2024-project-management-job-paccar-winch", "is_direct": false}], "job_description": "About PACCAR\n\nPACCAR, Inc. is a Fortune 500 company established in 1905 and is recognized as a global leader in the commercial vehicle, financial, and customer service fields.\n\nPACCAR is a global technology leader in the design, manufacture, and customer support of high-quality light-, medium- and heavy-duty trucks under its internationally recognized brands Kenworth, Peterbilt and DAF nameplates.\n\nPACCAR designs and manufactures advanced diesel engines and provides customized financial services, information technology and truck parts related to its principal business.\n\nYour next career move is waiting at PACCAR whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services.\n\nJoin the PACCAR team today!\n\nDivision Information\n\nPACCAR's Information Division (ITD), located in Renton, WA, utilizes cutting-edge technology to provide systems development, consulting, voice and data communications services to the entire corporation, which has high visibility in the technology sector.\n\nThe Role\n\nDoes empowering teams to makedatadriven decisions excite you? Do you wake up in the morning wondering what possibilities could be unlocked with moredata?\n\nPACCAR is looking for adataengineer to join the team.DataEngineering focuses on making possible fast, accurate, and reliable access todata.\n\nWe builddatapipelines, manage adatalake, and support the production use of ourdata. We advocate for leading datapractices and make sure that our business users can makedatadriven decisions.\n\nAbout the Team\n\nAs a Data Engineer Intern, you will work with a highly celebrative team of data engineers with different backgrounds. This data engineering team focuses on solving PACCAR's most important challenges by making data available to our data science and analytics teams.\n\nAs part of a diversified global Fortune 500 company, the data engineering team works on a huge variety of projects, from building data ingestion pipelines to data quality products, data integration services and data platforms.\n\nCome join this dynamic, growing, and pioneering team!\n\nJob Responsibilities\n\u2022 Build and support automateddatapipelines from a wide range ofdatasource\n\u2022 Working on ways to automated and improve development and release processes\n\u2022 Develop solutions to measure, improve, and monitordataquality based on business requirements.\n\u2022 Proactively support product health by building solutions that are automated, scalable, and sustainable - be relentlessly focused on minimizing defects and technical debt\n\nRequired Experience & Skills\n\u2022 Basic proficiency with a dialect of ANSI SQL and Python\n\u2022 Working knowledge of RDBMS databases such as MS SQL Server, Oracle, PostgreSQL preferred.\n\u2022 Experience with participating in projects in a highly collaborative, multi-discipline team environment.\n\nEducation\n\nBachelors' degree in Computer Science, Informatics, or a related field required\n\nBenefits of working at PACCAR\n\nAs a U.S. PACCAR intern, you have a full range of benefit options including :\n\u2022 401k with up to a 5% company match\n\u2022 Sick Leave\n\u2022 Medical, dental, and vision plans for you and your family\n\u2022 Flexible spending accounts (FSA) and health savings account (HSA)\n\u2022 Life and accidental death and dismemberment insurance\n\u2022 EAP services including wellness plans, estate planning, financial counseling and more\n\u2022 Global Fortune 500 company with a wide array of growth, training, and development opportunities\n\u2022 Work alongside experienced goal-oriented colleagues recognized as experts in their field\n\nDiversity & Inclusion\n\nPACCAR has success with diverse teams of employees working together to achieve excellent results. Having a diverse and inclusive work environment ensures PACCAR has the talent needed to conduct business today and in the future by leveraging different backgrounds, skills, and viewpoints.\n\nPACCAR increases awareness through the efforts of global company-wide Diversity Councils and various employee resource groups that support initiatives & activities including multi-cultural events, outreach, mentorship programs, and more.\n\nPACCAR has been awarded the Top Company for Women to Work for in Transportation since 2018 by the Women in Trucking Association and Newsweek 2023 America's Greatest Workplaces for Women.\n\nFor more information, please visit our Diversity and Inclusion Commitment page : Diversity and Inclusion (paccar.com)\n\nAdditional Information\n\nPACCAR is an Equal Opportunity Employer / Protected Veteran / Disability. At PACCAR, we value talent and promote growth and development.\n\nWe carefully consider numerous compensation factors including your education, training, or experience. If hired, you will be required to provide proof of authorization to work in the United States.\n\nApplicants and employees for this position will not be sponsored for work authorization, including, but not limited to H-1B visas, now or in the future.\n\nThe salaries for intern positions are as follows :\n\u2022 Undergraduate Enrollment - $25 / hour\n\u2022 Graduate Enrollment - $30 / hour\n\nLast updated : 2023-12-21", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Renton", "job_state": "WA", "job_country": "US", "job_latitude": 47.479694, "job_longitude": -122.207924, "job_benefits": ["retirement_savings", "dental_coverage", "health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=w1LPJcjUfwL_UkkwAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-03-10T00:00:00.000Z", "job_offer_expiration_timestamp": 1710028800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 30, "job_max_salary": 30, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Basic proficiency with a dialect of ANSI SQL and Python", "Experience with participating in projects in a highly collaborative, multi-discipline team environment", "Bachelors' degree in Computer Science, Informatics, or a related field required", "If hired, you will be required to provide proof of authorization to work in the United States"], "Responsibilities": ["Build and support automateddatapipelines from a wide range ofdatasource", "Working on ways to automated and improve development and release processes", "Develop solutions to measure, improve, and monitordataquality based on business requirements", "Proactively support product health by building solutions that are automated, scalable, and sustainable - be relentlessly focused on minimizing defects and technical debt"], "Benefits": ["As a U.S. PACCAR intern, you have a full range of benefit options including :", "401k with up to a 5% company match", "Sick Leave", "Medical, dental, and vision plans for you and your family", "Flexible spending accounts (FSA) and health savings account (HSA)", "Life and accidental death and dismemberment insurance", "EAP services including wellness plans, estate planning, financial counseling and more", "Global Fortune 500 company with a wide array of growth, training, and development opportunities", "Undergraduate Enrollment - $25 / hour", "Graduate Enrollment - $30 / hour"]}, "job_job_title": null, "job_posting_language": "en", "job_onet_soc": "11103100", "job_onet_job_zone": "4", "job_occupational_categories": ["41-9031.00"], "job_naics_code": "336111", "job_naics_name": "Automobile Manufacturing"}, {"employer_name": "PayPal", "employer_logo": "https://www.paypalobjects.com/webstatic/i/logo/rebrand/ppcom.png", "employer_website": "http://www.paypal.com", "employer_company_type": null, "job_publisher": "Austin, TX - Geebo", "job_id": "1cuiQ08dkGzS1pWmAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer (Jr And Mid Level) at PayPal in Austin, TX", "job_apply_link": "https://austin-tx.geebo.com/jobs-online/view/id/1081966754-data-engineer-jr-and-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4362, "apply_options": [{"publisher": "Austin, TX - Geebo", "apply_link": "https://austin-tx.geebo.com/jobs-online/view/id/1081966754-data-engineer-jr-and-/", "is_direct": true}], "job_description": "Job Description At PayPal (NASDAQ:\nPYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives. We're a purpose-driven company, and our beliefs are the foundation of how we conduct business every day. We're guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Collectively, these values inspire us to work together as One Team with our customers at the center of everything we do, and to take care of ourselves, each other, and the communities in which we live and work. We challenge the status quo, ask questions, and find solutions. Join us as we enable the hopes, dreams, and ambitions of millions of people around the world. The candidate will be part of PayPal Data Engineering Team. The candidate will be responsible for analysis, design, coding, and testing of the application code. The candidate has to interact with global cross-functional teams to address complex business requirements. The role demands the individual to possess technical skills required to perform the job in an effective manner. The individual should be self-motivated, possess creative problem-solving skills and have the ability to handle multiple projects at the same time. The candidate should have passion for data & analytics.\nResponsibilities:\nIn this role, the individual will be part of the Data engineering team and will be responsible for. o Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale. o Planning the execution of the project in an effective and efficient manner. o Approaching the problem, taking into account all possibilities. o Creativity and out of the box thinking is required. o Proactively anticipating problems and appropriately communicating to the team and management in a timely manner. o Being flexible and being able to support all functions of product life cycle when required. o Ability to work in a fast-paced environment Hiring Jr and Mid-Level DATA ENGINEERS! Location:\nAustin, TX Come Join Us! Required Skills:\n3-7 years of experience in the IT industry, experience in Data Technology space is preferred. Advanced Shell or Perl scripting experience or proficiency in any programming language like Java, Python, Scala, C+\nand Spark Working experience in any MPP systems, should have strong SQL programming skills Knowledge of data warehousing concepts Working knowledge on Big Data, Streaming Integrations Excellent written and oral communication skills Strong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusions Expertise in database programming and performance tuning techniques Familiar with data movement techniques and best practices to handle large volumes of data Experience with data warehousing architecture and data modeling best practices Experience with File Systems, server architectures, and distributed systems Strong communication skills and willingness to take initiative to contribute beyond basic responsibilities Working experience in an Agile methodology is highly preferred Knowledge of Hadoop, Spark, HBase and Hive is highly preferred Knowledge and Working experience on Cloud platforms is highly preferred For more than 20 years, PayPal has remained at the forefront of the digital payment revolution. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, our open digital payments platform gives PayPal's 400 million active account holders the confidence to connect and transact in new and powerful ways. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying, or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Austin", "job_state": "TX", "job_country": "US", "job_latitude": 30.267153, "job_longitude": -97.74306, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=1cuiQ08dkGzS1pWmAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "The Cigna Group", "employer_logo": "https://cigna.gg/static/www-cigna-gg/img/cigna-rebrand-logo-blue-with-green.png", "employer_website": "https://www.thecignagroup.com", "employer_company_type": "Finance", "job_publisher": "Cigna - The Cigna Group", "job_id": "CJdR7_aAck1-TIhMAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Senior AI Engineer - Hybrid", "job_apply_link": "https://jobs.thecignagroup.com/us/en/job/23009983/Senior-AI-Engineer-Hybrid", "job_apply_is_direct": false, "job_apply_quality_score": 0.9022, "apply_options": [{"publisher": "Cigna - The Cigna Group", "apply_link": "https://jobs.thecignagroup.com/us/en/job/23009983/Senior-AI-Engineer-Hybrid", "is_direct": false}, {"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/senior-ai-engineer-hybrid-at-the-cigna-group-3787352183", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=4075e2d110391dbd", "is_direct": false}, {"publisher": "National Labor Exchange Veterans Jobs", "apply_link": "https://veterans.usnlx.com/st-louis-mo/senior-ai-engineer-hybrid/EBED4B6A7FE248C1BDD228F64903044D/job/?vs=28", "is_direct": false}, {"publisher": "Cigna Jobs", "apply_link": "https://cigna.dejobs.org/st-louis-mo/senior-ai-engineer-hybrid/EBED4B6A7FE248C1BDD228F64903044D/job/?vs=28", "is_direct": false}, {"publisher": "SimplyHired", "apply_link": "https://www.simplyhired.com/job/dYnXd59qrd8NODRfQv-pSdScNpWrlR6rBCc7Z-ays3S4_JtSXduYHQ", "is_direct": false}, {"publisher": "Cigna - Talentify", "apply_link": "https://cigna.talentify.io/job/senior-ai-engineer-saint-louis-missouri-cigna-23009983", "is_direct": false}, {"publisher": "Jooble", "apply_link": "https://jooble.org/jdp/-1610515414124069202/Senior-engineer-Saint-Louis%2C-MO", "is_direct": false}], "job_description": "The job profile for this position is Machine Learning Senior Advisor, which is a Band 4 Senior Contributor Career Track Role.\n\nExcited to grow your career?\n\nWe value our talented employees, and whenever possible strive to help one of our associates grow professionally before recruiting new talent to our open positions. If you think the open position you see is right for you, we encourage you to apply!\n\nOur people make all the difference in our success.\n\nAs a Senior AI Engineer, you will be joining a team transforming healthcare and improving the lives and vitality of the millions of members we serve. We leverage cutting edge Artificial Intelligence (AI) and Machine Learning (ML) algorithms to develop solutions for automated document processing and customer service chat bots. We are looking for Sr AI Engineers with strong engineering, full stack expertise to build the best fit solutions leveraging Large Language Models (LLMs) and Generative AI solutions. The work you do will impact millions of customers, members, and employers that rely on Cigna every day. Extreme focus on speed to market and getting Products and Services in the hands of customer and passion to transform healthcare is key to the success of this role.\n\nResponsibilities\n\u2022 Build scalable software solutions using LLM\u2019s and other ML models to solve challenges in healthcare\n\u2022 Build enterprise grade AI solutions with focus on privacy, security, fairness.\n\u2022 Work with Product Development as a Generative Artificial Intelligence (AI) subject matter expert and architect and develop scalable, resilient, ethical AI solutions\n\u2022 Strong engineering skills to design the output from the AI with nodes and nested nodes in JSON or array, HTML formats as required \u2013 This is critical so that the AI output can be consumed as is and displayed on the dashboard for accelerated development cycles.\n\u2022 Build extensible API Integrations, low code UI/UX solutions, with extremely short cycle times, to extract information from sources, integrate with GPT4, receive insights and make them available in intuitive, high performing dashboards\n\u2022 Build solutions that align with responsible AI practices.\n\u2022 Envision the solution outcomes to solve for the business problem with actionable insights and design viable solutions to meet the outcomes.\n\u2022 Understand how AI is interpreting the data set and use that understanding to build prompts that lead to expected outcomes\n\u2022 Architect and develop software or infrastructure for scalable, distributed systems and with machine learning technologies.\n\u2022 Work with frameworks(Tensorflow, PyTorch) and open source platforms like Hugging Face to deliver the best solutions\n\u2022 Optimize existing generative AI models for improved performance, scalability, and efficiency.\n\u2022 Develop and maintain AI pipelines, including data preprocessing, feature extraction, model training, and evaluation.\n\u2022 Develop clear and concise documentation, including technical specifications, user guides, and presentations, to communicate complex AI concepts to both technical and non-technical stakeholders.\n\u2022 Contribute to the establishment of best practices and standards for generative AI development within the organization.\n\nQualifications:\n\u2022 Degree in Computer Science, Artificial Intelligence, or a related field.\n\u2022 5 years of Full stack engineering expertise with languages like C#, Python and Proficiency in designing architecture, building API Integrations, configuring and deploying cloud services, setting up authentication, monitoring and logging\n\u2022 Experience in implementing enterprise systems in production setting for AI, computer vision, natural language processing. Exposure to self-supervised learning, transfer learning, and reinforcement learning is a plus.\n\u2022 Experience with information storage/retrieval using vector databases like pinecone.\n\u2022 Strong understanding and exposure in natural language generation or Gen AI like transformers, LLM\u2019s, text embedding\u2019s.\n\u2022 Experience with designing scalable software systems for classification, text extraction/summary, data connectors for different formats(pdf, csv, doc, etc)\n\u2022 Experience with machine learning libraries and frameworks such as PyTorch or TensorFlow, Hugging Face, Lang chain, Llama Index.\n\u2022 3 years of experience in a technical leadership role leading project teams and setting technical direction.\n\u2022 3 years of experience working in a complex, matrixed organization involving cross-functional or cross-business projects.\n\u2022 Programming experience in C/C++, Java, Python.\n\u2022 Strong knowledge of data structures, algorithms, and software engineering principles.\n\u2022 Familiarity with cloud-based platforms and services, such as AWS, GCP, or Azure.\n\u2022 Excellent problem-solving skills, with the ability to think critically and creatively to develop innovative AI solutions.\n\u2022 Strong communication skills, with the ability to effectively convey complex technical concepts to a diverse audience.\n\u2022 Possess a proactive mindset, with the ability to work independently and collaboratively in a fast-paced, dynamic environmen\n\u2022 This is a hybrid role and will require the ability to work in-person\n\nIf you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload.\n\nQualified applicants will be considered without regard to race, color, age, disability, sex, childbirth (including pregnancy) or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.\n\nPlease note that you must meet our posting guidelines to be eligible for consideration. Policy can be reviewed at this link.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Washington", "job_state": "DC", "job_country": "US", "job_latitude": 38.907192, "job_longitude": -77.03687, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=CJdR7_aAck1-TIhMAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": null, "job_offer_expiration_timestamp": null, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Degree in Computer Science, Artificial Intelligence, or a related field", "5 years of Full stack engineering expertise with languages like C#, Python and Proficiency in designing architecture, building API Integrations, configuring and deploying cloud services, setting up authentication, monitoring and logging", "Experience in implementing enterprise systems in production setting for AI, computer vision, natural language processing", "Experience with information storage/retrieval using vector databases like pinecone", "Strong understanding and exposure in natural language generation or Gen AI like transformers, LLM\u2019s, text embedding\u2019s", "Experience with designing scalable software systems for classification, text extraction/summary, data connectors for different formats(pdf, csv, doc, etc)", "Experience with machine learning libraries and frameworks such as PyTorch or TensorFlow, Hugging Face, Lang chain, Llama Index", "3 years of experience in a technical leadership role leading project teams and setting technical direction", "3 years of experience working in a complex, matrixed organization involving cross-functional or cross-business projects", "Programming experience in C/C++, Java, Python", "Strong knowledge of data structures, algorithms, and software engineering principles", "Familiarity with cloud-based platforms and services, such as AWS, GCP, or Azure", "Excellent problem-solving skills, with the ability to think critically and creatively to develop innovative AI solutions", "Strong communication skills, with the ability to effectively convey complex technical concepts to a diverse audience", "Possess a proactive mindset, with the ability to work independently and collaboratively in a fast-paced, dynamic environmen", "This is a hybrid role and will require the ability to work in-person", "If you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload"], "Responsibilities": ["Extreme focus on speed to market and getting Products and Services in the hands of customer and passion to transform healthcare is key to the success of this role", "Build scalable software solutions using LLM\u2019s and other ML models to solve challenges in healthcare", "Build enterprise grade AI solutions with focus on privacy, security, fairness", "Work with Product Development as a Generative Artificial Intelligence (AI) subject matter expert and architect and develop scalable, resilient, ethical AI solutions", "Strong engineering skills to design the output from the AI with nodes and nested nodes in JSON or array, HTML formats as required \u2013 This is critical so that the AI output can be consumed as is and displayed on the dashboard for accelerated development cycles", "Build extensible API Integrations, low code UI/UX solutions, with extremely short cycle times, to extract information from sources, integrate with GPT4, receive insights and make them available in intuitive, high performing dashboards", "Build solutions that align with responsible AI practices", "Envision the solution outcomes to solve for the business problem with actionable insights and design viable solutions to meet the outcomes", "Understand how AI is interpreting the data set and use that understanding to build prompts that lead to expected outcomes", "Architect and develop software or infrastructure for scalable, distributed systems and with machine learning technologies", "Work with frameworks(Tensorflow, PyTorch) and open source platforms like Hugging Face to deliver the best solutions", "Optimize existing generative AI models for improved performance, scalability, and efficiency", "Develop and maintain AI pipelines, including data preprocessing, feature extraction, model training, and evaluation", "Develop clear and concise documentation, including technical specifications, user guides, and presentations, to communicate complex AI concepts to both technical and non-technical stakeholders", "Contribute to the establishment of best practices and standards for generative AI development within the organization"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15111100", "job_onet_job_zone": "5", "job_occupational_categories": ["Data & Analytics"], "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Children's Hospital Los Angeles", "employer_logo": null, "employer_website": null, "employer_company_type": null, "job_publisher": "Geebo", "job_id": "ITy_3qc5S4dENaPmAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer at Children's Hospital Los Angeles in Glendale, CA", "job_apply_link": "https://geebo.com/jobs-online/view/id/1062594853-data-engineer-at-children-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4506, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1062594853-data-engineer-at-children-/", "is_direct": true}], "job_description": "Job Description NATIONAL LEADERS IN PEDIATRIC CARE Ranked No. 5 in the nation by U.S. News & World Report, Children's Hospital Los Angeles (CHLA) provides the best care in California. Here world-class experts in medicine, education and research work together to deliver family-centered care half a million times each year. From primary to complex critical care, more than 350 programs and services are offered, each one specially designed for children. The CHLA of the future is brighter than can be imagined. Investments in technology, research and innovation will create care that is personal, convenient and empowering. Our scientists will work with clinical experts to take laboratory discoveries and create treatments that are a perfect match for every patient. And together, CHLA team members will turn health care into health transformation. Join a hospital where the work you do will matter--to you, to your colleagues, and above all, to our patients and families. The work will be challenging, but always rewarding. It's Work That Matters. Overview Data Engineer I is responsible for administrating, managing, and scaling the Enterprise Data Lake (EDL), expanding the EDL to utilize Cloud resources, and supporting data pipelines, products, and visualization. Minimum Qualifications/Work\nExperience:\n3\nyears' data engineer experience in various big data and data lake technologies, e.g., Cloudera, Hortonworks, Databricks, etc. Proficiency in SQL, Linux, shell-script programming, Spark, Python and Tableau Experience with AD, Hadoop authentication/authorization and security frameworks is preferred Education/Licensure/Certification:\nBachelor's Degree in computer science, Information Systems, or equivalent. Graduate Degree preferred.\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine Learning, Systems Architecture & EngineeringEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Glendale", "job_state": "CA", "job_country": "US", "job_latitude": 34.14251, "job_longitude": -118.25507, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=ITy_3qc5S4dENaPmAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["years' data engineer experience in various big data and data lake technologies, e.g., Cloudera, Hortonworks, Databricks, etc", "Bachelor's Degree in computer science, Information Systems, or equivalent"], "Responsibilities": ["The work will be challenging, but always rewarding", "Overview Data Engineer I is responsible for administrating, managing, and scaling the Enterprise Data Lake (EDL), expanding the EDL to utilize Cloud resources, and supporting data pipelines, products, and visualization"], "Benefits": ["$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "nmxXrP8NZHym6ODUAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/everett/mdm-data-engineer-65800fa789fe4a04c7f331c1", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/everett/mdm-data-engineer-65800fa789fe4a04c7f331c1", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113751, "job_posted_at_datetime_utc": "2023-12-20T23:09:11.000Z", "job_city": "Everett", "job_state": "MA", "job_country": "US", "job_latitude": 42.40843, "job_longitude": -71.053665, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=nmxXrP8NZHym6ODUAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:09:11.000Z", "job_offer_expiration_timestamp": 1705532951, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Jenius Bank", "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQwDHvr5CmxHZWM_xQiOUk3Sp3fSNFEmLUdvK0R&s=0", "employer_website": null, "employer_company_type": null, "job_publisher": "LinkedIn", "job_id": "Sg7i32qaVI-hzdEAAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "QA Automation Engineer - Data (Hybrid)", "job_apply_link": "https://www.linkedin.com/jobs/view/qa-automation-engineer-data-hybrid-at-jenius-bank-3740222577", "job_apply_is_direct": false, "job_apply_quality_score": 0.782, "apply_options": [{"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/qa-automation-engineer-data-hybrid-at-jenius-bank-3740222577", "is_direct": false}, {"publisher": "Careers At SMBC", "apply_link": "https://careers.smbcgroup.com/job/Scottsdale-Software-Engineer-QA-Data-%28Hybrid%29-AZ-85255/1087145600/", "is_direct": false}, {"publisher": "Salary.com", "apply_link": "https://www.salary.com/job/sumitomo-mitsui-banking-corporation-smbc/qa-automation-engineer-data-hybrid/j202311211243411486107", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=18d9b441c3b73fe6", "is_direct": false}, {"publisher": "Milwaukee Jobs", "apply_link": "https://www.milwaukeejobs.com/job/detail/77522035/Software-Engineer-Data-Hybrid?keywords=Planning%2Bor%2BProject%2BManagement%2BInformation%2BTechnology%2BSpecialist", "is_direct": false}, {"publisher": "Monster", "apply_link": "https://www.monster.com/job-openings/software-engineer-data-hybrid-scottsdale-az--9091c6c4-22f5-46ea-94f6-6827dfee7d02", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/software-engineer-data-hybrid-sumitomo-mitsui-banking-corporation-JV_IC1133911_KO0,29_KE30,65.htm?jl=1009021441058", "is_direct": false}, {"publisher": "Ladders", "apply_link": "https://www.theladders.com/job-listing/qa-automation-engineer-data-hybrid-sumitomo-mitsui-banking-corporation-smbc-scottsdale-az-_v2_-7-3389790573.html", "is_direct": false}], "job_description": "Join us on our mission to create a completely new, 100% digital bank that truly serves customers' best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).\n\nThe anticipated salary range for this role is between $78,000.00 and $130,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees.\n\nWe work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the second largest bank in Japan and the 12th largest bank in the world with operations in over forty countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.\n\nIt is the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the 'patient capital' necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.\n\nSummary\n\nWe are seeking a QA Automation Engineer - Data to provide continuous automated and manual testing of data sets for use in internal data systems and for delivery from internal systems to users within Jenius Bank. Data sets can be in several formats, depending on supplier systems and client requirements, including JSON, XML, CSV or RDF. The QA Automation Engineer - Data works in the technology team with the data engineers.\n\nPrincipal Duties And Responsibilities\n\u2022 Partner with Data Engineering development teams to enable code delivery, automated testing, and assurance of product reliability.\n\u2022 Create, communicate, and enforce data quality management policies, processes, and procedures.\n\u2022 Create effective test plans and data sets related to functional testing and end-to-end testing for ETL, database and reports.\n\u2022 Review requirements, specifications, and technical documentation to provide meaningful feedback.\n\u2022 Articulate test results, progress, and milestones to leadership and development teams\n\u2022 Create jobs and scripts to automatically test the quality of data throughout our data warehouse environments.\n\u2022 Create, document, and execute test cases to support product releases.\n\nPosition Specifications\n\u2022 Education: Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field\n\u2022 Highly proficient in Python and SQL\n\u2022 4+ years of experience in a Quality Assurance / Automation Engineering role\n\u2022 3+ years experience with creation, execution, and maintenance of automation frameworks, scripts in either Python\n\u2022 3+ years experience with SQL\n\u2022 Experience in DevOps and CI/CD related technologies\n\u2022 Preferred experience in a public cloud environment, preferably GCP\n\u2022 Preferred experience in testing data pipelines built in Python.\n\u2022 Preferred experience in Data governance and Metadata Management\n\u2022 Preferred experience working in a Big Data environment, dealing with large diverse data sets.\n\u2022 Have understanding of Spark/Pyspark to test using Notebooks\n\u2022 Ability to work independently, solve problems, update the stake holders.\n\u2022 Analyze, design, develop and deploy solutions as per business requirements.\n\u2022 Ability to identify defects in edge-case scenarios.\n\u2022 Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives.\n\u2022 Excellent analytical, problem solving, and troubleshooting skills in large data warehousing environments.\n\nEOE STATEMENT\n\nWe are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law.\n\nCCPA DISCLOSURE\n\nPersonal Information Collection Notice: This notice contains information under the California Consumer Privacy Act (CCPA) about the categories of personal information (PI) of California residents that Manufacturers Bank collects and the business or commercial purpose(s) for which the PI may be used. We do not sell PI. More information about our collection and use of PI may be found in our CCPA Privacy Policy at https://www.manufacturersbank.com/CCPA-Privacy. Persons with disabilities may contact our Customer Contact Center toll-free at (877) 560-9812 to request the information in this Notice in an alternative format.", "job_is_remote": false, "job_posted_at_timestamp": 1703153210, "job_posted_at_datetime_utc": "2023-12-21T10:06:50.000Z", "job_city": "Scottsdale", "job_state": "AZ", "job_country": "US", "job_latitude": 33.494877, "job_longitude": -111.92168, "job_benefits": ["health_insurance", "retirement_savings"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Sg7i32qaVI-hzdEAAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-02-13T16:44:15.000Z", "job_offer_expiration_timestamp": 1707842655, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 48, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Education: Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field", "Highly proficient in Python and SQL", "4+ years of experience in a Quality Assurance / Automation Engineering role", "3+ years experience with creation, execution, and maintenance of automation frameworks, scripts in either Python", "3+ years experience with SQL", "Experience in DevOps and CI/CD related technologies", "Have understanding of Spark/Pyspark to test using Notebooks", "Ability to identify defects in edge-case scenarios", "Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives", "Excellent analytical, problem solving, and troubleshooting skills in large data warehousing environments"], "Responsibilities": ["We are seeking a QA Automation Engineer - Data to provide continuous automated and manual testing of data sets for use in internal data systems and for delivery from internal systems to users within Jenius Bank", "Partner with Data Engineering development teams to enable code delivery, automated testing, and assurance of product reliability", "Create, communicate, and enforce data quality management policies, processes, and procedures", "Create effective test plans and data sets related to functional testing and end-to-end testing for ETL, database and reports", "Review requirements, specifications, and technical documentation to provide meaningful feedback", "Articulate test results, progress, and milestones to leadership and development teams", "Create jobs and scripts to automatically test the quality of data throughout our data warehouse environments", "Create, document, and execute test cases to support product releases", "Analyze, design, develop and deploy solutions as per business requirements"], "Benefits": ["The anticipated salary range for this role is between $78,000.00 and $130,000.00", "The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire", "The role may also be eligible for an annual discretionary incentive award", "In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees"]}, "job_job_title": "Automation engineer", "job_posting_language": "en", "job_onet_soc": "15119900", "job_onet_job_zone": "4"}, {"employer_name": "Insight Global", "employer_logo": "https://images.squarespace-cdn.com/content/5f7f984c3ca20d1d55b276f7/1619793549819-L54X3W9Z5RD277UPNH9S/IGLogoPublic.png?content-type=image%2Fpng", "employer_website": "http://www.insightglobal.com", "employer_company_type": "Staffing", "job_publisher": "Jobs Trabajo.org", "job_id": "RHu0HzNwDwxUdafxAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Lead SR. Data Engineer", "job_apply_link": "https://us.trabajo.org/job-640-20231221-783b1f2ef6a2190ac91ea7585e529c93", "job_apply_is_direct": false, "job_apply_quality_score": 0.4387, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-640-20231221-783b1f2ef6a2190ac91ea7585e529c93", "is_direct": false}, {"publisher": "Jobilize", "apply_link": "https://www.jobilize.com/job/us-oh-new-albany-lead-sr-data-engineer-remote-insight-global-hiring", "is_direct": false}], "job_description": "A client of Insight Global is seeking a Sr. Data Engineer to join their team. The primary responsibility of Senior Data Management Engineer is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self-service data pipelines, assist in operationalizing the AI / ML Engineering solutions. This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards.\n\nThis role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project.\n\nResponsibilities:\n\u2022 Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation\n\u2022 Create and maintain the data ingestion, quality testing and audit framework\n\u2022 Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE.\n\u2022 Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M\n\u2022 Setup and evangelize the metadata driven approach to data pipelines to promote self service\n\u2022 Setup and continuously improve the data quality and audit monitoring as well as alerting\n\u2022 Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design.\n\u2022 Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices\n\u2022 Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects\n\u2022 Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data\n\u2022 Respond SLA driven production data quality or pipeline issues\n\u2022 Work in a fast-paced Agile/Scrum environment\n\u2022 Identify and assist with implementation of DevOps practices in support of fully automated deployments\n\u2022 Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines\n\u2022 Follow the Industry standard data security practices and evangelize the same across the team.\n\nMust Haves:\n\u2022 5+ years of experience in an Enterprise Data Management or Data Engineering role\n\u2022 3+ of hands-on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake\n\u2022 5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks\n\u2022 Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, SQL Server, MySQL, Teradata\n\u2022 Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab\n\u2022 Preferably experienced in SLA driven Production Data Pipeline or Quality support\n\u2022 Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc.\n\u2022 Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)\n\u2022 Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE\n\u2022 Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls\n\u2022 Ability to implement Agile methodologies and work in an Agile DevOps environment\n\u2022 Bachelor's degree in computer science or engineering or mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization\n\u2022 Experience designing and building complex data pipelines in an agile environment\n\u2022 Expertise on data analysis and wrangling using SQL, python, data bricks\n\u2022 Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions\n\u2022 Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms", "job_is_remote": false, "job_posted_at_timestamp": 1703118897, "job_posted_at_datetime_utc": "2023-12-21T00:34:57.000Z", "job_city": "New Albany", "job_state": "OH", "job_country": "US", "job_latitude": 40.08216, "job_longitude": -82.81009, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=RHu0HzNwDwxUdafxAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["5+ years of experience in an Enterprise Data Management or Data Engineering role", "3+ of hands-on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake", "5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks", "Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, SQL Server, MySQL, Teradata", "Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab", "Preferably experienced in SLA driven Production Data Pipeline or Quality support", "Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc", "Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)", "Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE", "Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls", "Ability to implement Agile methodologies and work in an Agile DevOps environment", "Bachelor's degree in computer science or engineering or mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization", "Experience designing and building complex data pipelines in an agile environment", "Expertise on data analysis and wrangling using SQL, python, data bricks", "Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions", "Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms"], "Responsibilities": ["The primary responsibility of Senior Data Management Engineer is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self-service data pipelines, assist in operationalizing the AI / ML Engineering solutions", "This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards", "This role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project", "Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation", "Create and maintain the data ingestion, quality testing and audit framework", "Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE", "Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M", "Setup and evangelize the metadata driven approach to data pipelines to promote self service", "Setup and continuously improve the data quality and audit monitoring as well as alerting", "Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design", "Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices", "Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects", "Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data", "Respond SLA driven production data quality or pipeline issues", "Work in a fast-paced Agile/Scrum environment", "Identify and assist with implementation of DevOps practices in support of fully automated deployments", "Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines", "Follow the Industry standard data security practices and evangelize the same across the team"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "561311", "job_naics_name": "Employment Placement Agencies"}, {"employer_name": "NATIONAL GRID CO USA (NE POWER)", "employer_logo": null, "employer_website": null, "employer_company_type": null, "job_publisher": "Whitman, MA - Geebo", "job_id": "-NVAzz40Cq-7KyQWAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Senior Data Engineer", "job_apply_link": "https://whitman-ma.geebo.com/jobs-online/view/id/862783738-senior-data-engineer-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4311, "apply_options": [{"publisher": "Whitman, MA - Geebo", "apply_link": "https://whitman-ma.geebo.com/jobs-online/view/id/862783738-senior-data-engineer-/", "is_direct": true}], "job_description": "About usEvery day we deliver safe and secure energy to homes, communities, and businesses.\nWe are there when people need us the most.\nWe connect people to the energy they need for the lives they live.\nThe pace of change in society and our industry is accelerating and our expertise and track record puts us in an unparalleled position to shape the sustainable future of our industry.\nTo be successful we must anticipate the needs of our customers, reducing the cost of energy delivery today and pioneering the flexible energy systems of tomorrow.\nThis requires us to deliver on our promises and always look for new opportunities to grow, both ourselves and our business.\nNational Grid is hiring a Senior Data Engineer for our Waltham, MA.\nJob PurposeNational Grid is a gas and electric utility that brings energy to life for 7 million customers across Massachusetts, New York, and Rhode Island through a network of 100,000\nmiles of wires and pipes.\nThe company is aggressively transforming into a 21st century data-driven energy manager for its customers.\nWe are building a team that will be responsible for Data Engineering and MLOps.\nThe team will develop data pipelines, take data science prototype models to production, monitor operations and standup the necessary infrastructure in Azure to perform these functions.\nRead on if you are excited about using best-in-class tools and methodologies to help shape the future of Data Engineering and MLOps at National Grid.\nKey AccountabilitiesWork with data scientists to specify data requirements for data science projects.\nWork with data owners to understand the data schema, the nature of data flow within the enterprise, and develop data dictionaries.\nDevelop a deep understanding of the ingested data from a business perspective.\nDesign robust, scalable, and maintainable data ingestion ETL/ELT pipelines.\nChoose appropriate data models and ensure that ingested data is high quality.\nWrite high quality code that has high test coverage.\nParticipate in code reviews to help improve code quality.\nSupervisory/Interpersonal- Experience RequiredExperience in project management and stakeholder-based delivery.\nQualificationsBachelor's degree in a computer science or another quantitative field.\n5\nyears of experience in a Data Engineer role Minimum of 3-5 years' experience in advanced data processing and software development is desired.\nExperience in deploying pipelines using Airflow.\nProficient/expert level hands-on knowledge of shell scripting, Python, SQL, Azure, and AWS.\nExperience in extracting data using REST APIs and from databases (Oracle, MSSQL)Ideally, experience in administering Windows and/or Linux servers.\nBe intellectually curious and enjoy learningMore InformationThis position has a career path which provides for advancement opportunities within and across bands as you develop and evolve in the position; gaining experience, expertise and acquiring and applying technical skills.\nInternal candidates will be assessed and provided offers against the minimum qualifications of this role and their individual experience.\nNational Grid is an equal opportunity employer that values a broad diversity of talent, knowledge, experience and expertise.\nWe foster a culture of inclusion that drives employee engagement to deliver superior performance to the communities we serve.\nNational Grid is proud to be an affirmative action employer.\nWe encourage minorities, women, individuals with disabilities and protected veterans to join the National Grid team.\n.\nEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Whitman", "job_state": "MA", "job_country": "US", "job_latitude": 42.080658, "job_longitude": -70.9356, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=-NVAzz40Cq-7KyQWAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Key AccountabilitiesWork with data scientists to specify data requirements for data science projects", "QualificationsBachelor's degree in a computer science or another quantitative field", "Experience in deploying pipelines using Airflow", "Proficient/expert level hands-on knowledge of shell scripting, Python, SQL, Azure, and AWS", "Experience in extracting data using REST APIs and from databases (Oracle, MSSQL)Ideally, experience in administering Windows and/or Linux servers", "Be intellectually curious and enjoy learning"], "Responsibilities": ["The team will develop data pipelines, take data science prototype models to production, monitor operations and standup the necessary infrastructure in Azure to perform these functions", "Read on if you are excited about using best-in-class tools and methodologies to help shape the future of Data Engineering and MLOps at National Grid", "Work with data owners to understand the data schema, the nature of data flow within the enterprise, and develop data dictionaries", "Develop a deep understanding of the ingested data from a business perspective", "Design robust, scalable, and maintainable data ingestion ETL/ELT pipelines", "Choose appropriate data models and ensure that ingested data is high quality", "Write high quality code that has high test coverage", "Participate in code reviews to help improve code quality", "Supervisory/Interpersonal- Experience RequiredExperience in project management and stakeholder-based delivery"], "Benefits": ["Estimated Salary: $20 to $28 per hour based on qualifications"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "ZD3bsqoHv_1q1jF8AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/quincy/mdm-data-engineer-65800fa79ade4304c1526d1d", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/quincy/mdm-data-engineer-65800fa79ade4304c1526d1d", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113760, "job_posted_at_datetime_utc": "2023-12-20T23:09:20.000Z", "job_city": "Quincy", "job_state": "MA", "job_country": "US", "job_latitude": 42.252876, "job_longitude": -71.00227, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=ZD3bsqoHv_1q1jF8AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:09:20.000Z", "job_offer_expiration_timestamp": 1705532960, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Genworth Financial, Inc", "employer_logo": "https://media.licdn.com/dms/image/C4D0BAQHtb1Fp9a2vGg/company-logo_200_200/0/1631345359658?e=2147483647&v=beta&t=poEPG_hib1FbV2HaF7Dmm8XAmLlMdicjfcOJqdL_-CM", "employer_website": "http://www.genworth.com", "employer_company_type": "Finance", "job_publisher": "Longmont, CO - Geebo", "job_id": "wOkl5qBYzsokTRImAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer at Genworth Financial, Inc in Remote", "job_apply_link": "https://longmont-co.geebo.com/jobs-online/view/id/1064200213-data-engineer-at-genworth-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4491, "apply_options": [{"publisher": "Longmont, CO - Geebo", "apply_link": "https://longmont-co.geebo.com/jobs-online/view/id/1064200213-data-engineer-at-genworth-/", "is_direct": true}], "job_description": "YOUR ROLE As a leading provider of long term care insurance, life insurance and annuities, Genworth has a substantial amount of data that supports our business operations and decision making. As a data engineer, you will be a key contributor to Genworth's adoption of technologies that move our business into its next phase of growth. You will design and build solutions that meet the changing analytical demands of our internal customers and leverage an evolving technology landscape. YOUR RESPONSIBILITIES o Select technologies, design databases and create data flows based on business requirements o Partner with co-workers to create technology and process solutions o Implement data and design patterns for common business functionality o Leverage solution frameworks integrating large or complex data sets o Integrate cloud-based and citizen-developer capabilities with existing technologies YOUR QUALIFICATIONS o 2\nyears' experience with MPP technologies o 5 or more years of data engineering experience o Hands-on development experience using OLAP and OLTP platforms such as SQL Server, Oracle and Greenplum o Strong communication, analytical, and problem-solving skills o Ability to effectively manage competing priorities o Must be willing to travel to Genworth offices 1 to 2 times a month PREFERRED QUALIFICATIONS o Experience with Greenplum technology and Informatica o Undergraduate or graduate degree in technology field o Knowledge of insurance products WHY GENWORTH? o We have a real impact on the lives of the people we serve o We work on challenging and rewarding projects o We give back to the communities where we live o We offer competitive benefits including:\no Medical, Dental, Vision, Flexible Spending Account options beginning your first day o Generous Choice Time Off your first full year o 12 Paid Holidays o 40 hours of volunteer time off o 401K Account with matching contributions o Tuition Reimbursement and Student Loan Repayment o Paid Family Leave o Child Care Subsidy Program\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Longmont", "job_state": "CO", "job_country": "US", "job_latitude": 40.167206, "job_longitude": -105.10193, "job_benefits": ["dental_coverage", "health_insurance", "retirement_savings", "paid_time_off"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=wOkl5qBYzsokTRImAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 1, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["years' experience with MPP technologies", "5 or more years of data engineering experience", "Hands-on development experience using OLAP and OLTP platforms such as SQL Server, Oracle and Greenplum", "Strong communication, analytical, and problem-solving skills", "Ability to effectively manage competing priorities", "Experience with Greenplum technology and Informatica", "Undergraduate or graduate degree in technology field", "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"], "Responsibilities": ["You will design and build solutions that meet the changing analytical demands of our internal customers and leverage an evolving technology landscape", "Select technologies, design databases and create data flows based on business requirements", "Partner with co-workers to create technology and process solutions", "Implement data and design patterns for common business functionality", "Leverage solution frameworks integrating large or complex data sets", "Integrate cloud-based and citizen-developer capabilities with existing technologies YOUR QUALIFICATIONS"], "Benefits": ["Medical, Dental, Vision, Flexible Spending Account options beginning your first day", "Generous Choice Time Off your first full year", "12 Paid Holidays", "40 hours of volunteer time off", "401K Account with matching contributions", "Tuition Reimbursement and Student Loan Repayment", "Paid Family Leave", "Child Care Subsidy Program", "$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524126", "job_naics_name": "Direct Property and Casualty Insurance Carriers"}, {"employer_name": "Salesforce", "employer_logo": "https://careers.salesforce.com/images/logo.svg", "employer_website": "http://www.salesforce.com", "employer_company_type": "Information", "job_publisher": "Geebo", "job_id": "zszYzSp8RZpIK1UfAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Associate Data Engineer, Enterprise Data Warehouse at Salesforce in Dallas, TX", "job_apply_link": "https://geebo.com/jobs-online/view/id/1051796261-associate-data-engineer-enterprise-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4542, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1051796261-associate-data-engineer-enterprise-/", "is_direct": true}], "job_description": "Job Details Business Technology blazes the trail of enterprise IT. Built on the foundation of our core values, we own more than the traditional IT components with a heavy focus on working closely with our business partners for amazing outcomes. Our goal is to deliver technology that is centered around our business and our collective success. We oversee technology strategy, Salesforce on Salesforce, customer and partner enablement, applications engineering, infrastructure, collaboration, enterprise operations, architecture, and program enablement. We own the world's foremost Salesforce implementation and enable our global Ohana to do their best work by leveraging our platform. The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, Customer success and Employee Success. This platform also includes management of a Big Data Hadoop platform to store unstructured application log data that is generated by the Salesforce product offerings, this data is predominately used by the data science teams. Team also leads all aspects of rolling out Salesforce's analytics tool Wave to internal business partners. The Salesforce IT EDW team is looking for an experienced BI designer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com. This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.\nResponsibilities:\n? Experience in designing and development of analytical solutions including Salesforce analytics solution - Wave and Tableau ? Contribute towards designing data models, ETL mappings and associated objects of analytical solutions. ? Review solution design with Lead members and ensure that the defined EDW standards and framework are followed. ? Review and validate logical and physical design to ensure alignment with the defined solution architecture. ? Create/review technical documentation for all new and modified objects. ? Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly. ? Follow standard practises for QA in the organization ? Support QA, UAT and performance testing phases of the development cycle. ? Understand and incorporate required security framework in the developed data model and ETL objects. ? Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes. ? Evaluate, determine root cause and resolve production issues. ? Work closely with other IT development groups to deliver coordinated software solutions Required Skills:\n? 1+year of experience working as a BI technical resource in a customer-focused IT EDW team ? 2+years of experience in the data warehousing domain as a technical resource ? Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools. ?Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture. ? Good knowledge of SQL and relational database models. ? Hands-on experience creating Unix shell scripts ? Data visualization tool experience like Tableau ? Good understanding of data modeling ? Basic working experience in an agile environment ? Excellent team player able to work with virtual and global across functional teams at all levels. ? Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines. ? Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences. ? Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role. Desired Skills:\n? Salesforce, Data Warehousing, customer success and Data Analysis ? Working experience on Data science or Python scripting is a plus ? Working experience of snowflake ? Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Dallas", "job_state": "TX", "job_country": "US", "job_latitude": 32.776665, "job_longitude": -96.79699, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=zszYzSp8RZpIK1UfAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel", "1+year of experience working as a BI technical resource in a customer-focused IT EDW team ?", "2+years of experience in the data warehousing domain as a technical resource ?", "Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools", "Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture", "Good knowledge of SQL and relational database models", "Hands-on experience creating Unix shell scripts ?", "Data visualization tool experience like Tableau ?", "Good understanding of data modeling ?", "Basic working experience in an agile environment ?", "Excellent team player able to work with virtual and global across functional teams at all levels", "Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines", "Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences", "Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role", "Salesforce, Data Warehousing, customer success and Data Analysis ?", "Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements", "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"], "Responsibilities": ["Contribute towards designing data models, ETL mappings and associated objects of analytical solutions", "Review solution design with Lead members and ensure that the defined EDW standards and framework are followed", "Review and validate logical and physical design to ensure alignment with the defined solution architecture", "Create/review technical documentation for all new and modified objects", "Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly", "Follow standard practises for QA in the organization ?", "Support QA, UAT and performance testing phases of the development cycle", "Understand and incorporate required security framework in the developed data model and ETL objects", "Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes", "Evaluate, determine root cause and resolve production issues", "Work closely with other IT development groups to deliver coordinated software solutions Required Skills:"], "Benefits": ["$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "511210", "job_naics_name": "Software Publishers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "XBLobFI3k9s0CvutAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/middleton/mdm-data-engineer-65800fa72ee8d604cf8487d0", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/middleton/mdm-data-engineer-65800fa72ee8d604cf8487d0", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113750, "job_posted_at_datetime_utc": "2023-12-20T23:09:10.000Z", "job_city": "Middleton", "job_state": "MA", "job_country": "US", "job_latitude": 42.595093, "job_longitude": -71.01617, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=XBLobFI3k9s0CvutAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:09:10.000Z", "job_offer_expiration_timestamp": 1705532950, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "iHerb.com", "employer_logo": "https://images.crunchbase.com/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/bcd0rdavg1nsilncplmf", "employer_website": "http://www.iherb.com", "employer_company_type": "Retail", "job_publisher": "Beavercreek, OR - Geebo", "job_id": "C-Jbj61CPOVCqe8rAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer - AWS at iHerb.COM in Remote", "job_apply_link": "https://beavercreek-or.geebo.com/jobs-online/view/id/1080708174-data-engineer-aws-at-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.5158, "apply_options": [{"publisher": "Beavercreek, OR - Geebo", "apply_link": "https://beavercreek-or.geebo.com/jobs-online/view/id/1080708174-data-engineer-aws-at-/", "is_direct": true}], "job_description": "Job Description We're looking for a Senior Data Engineer to work on our massive data processing pipeline and lead our data lake and data warehouse building, to help us deliver more insights and scale our data infrastructure. You will have a chance to contribute to the company's evolving culture, bring innovative approaches and learn from your talented colleagues. Responsibilities Designs and develops programs and tools to support data ingestion, curation and provisioning of complex enterprise data to achieve analytics & reporting on our current technology stack Designs and builds data extracts, integrations and transformations Provides successful deployment and provisioning of data solutions to required environments Designs and builds data architecture and applications that successfully enable speed, quality and efficient pipelines Interacts with cross-functional customers and development team to gather and define requirements. Develops understanding of the data and builds business acumen. Reviews discrepancies in requirements and resolves with stakeholders. Identifies and recommends appropriate continuous improvement opportunities and ensures integrations are automated and have proper exception handling. Key team member of project team designing and deploying a ground up cloud data pipeline Qualifications Bachelor or Master s degree in technical discipline such as Computer Science, Information Systems or another technical field People person, team player with a strong can-do mentality 5 years as a Data Engineer on a data and analytics team Proficient in data modelling principles Proficiency in Snowflake and other data warehousing solutions (Redshift / BigQuery) Proficiency in Databricks Experience with building data stream pipelines and ETL using:\nSpark, Elastic Advanced experience with AWS cloud services Advanced knowledge in Python Advanced working SQL experience as well as performance tuning Experience with data pipeline workflow management tools such as:\nAirflow, Astronomer Knowledge and ability to write, test, and debug APIs Experience working with agile methodologies and working in cross-functional teams Must be proactive, demonstrate initiative and be a logical thinker Must be an inquisitive learner and have a thirst for improvement Ability to understand and apply customer requirements, including drawing out unforeseen implications and making design recommendations. Strong facilitation and consensus building skills. Strong oral and written communication skills; Ability to communicate by simplifying complexity Ability to provide work guidance to junior level developers Enjoys higher learning and keeps track of industry best practices and trends and through acquired knowledge, takes advantage of process and system improvement opportunities Be proactive requiring minimal supervision, be strong time management and work organization skills Have an ability prioritize workload and handle multiple tasks and at times meet tight deadlines. Strong problem-solving and analytical skills Has worked on data quality improvement projects such as Master Data Management Highly Desired AWS certifications (any):\nAWS Certified Solutions Architect - Associate/Professional AWS Certified Developer - Associate/Professional AWS Certified DevOps Engineer AWS Certified Solutions Architect AWS Certified Data Analytics AWS Certified Security - Specialty AWS Certified Cloud Practitioner.\nSalary Range:\n$100K -- $150K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Beavercreek", "job_state": "OR", "job_country": "US", "job_latitude": 45.28793, "job_longitude": -122.53534, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=C-Jbj61CPOVCqe8rAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Key team member of project team designing and deploying a ground up cloud data pipeline Qualifications Bachelor or Master s degree in technical discipline such as Computer Science, Information Systems or another technical field People person, team player with a strong can-do mentality 5 years as a Data Engineer on a data and analytics team Proficient in data modelling principles Proficiency in Snowflake and other data warehousing solutions (Redshift / BigQuery) Proficiency in Databricks Experience with building data stream pipelines and ETL using:", "Spark, Elastic Advanced experience with AWS cloud services Advanced knowledge in Python Advanced working SQL experience as well as performance tuning Experience with data pipeline workflow management tools such as:", "Airflow, Astronomer Knowledge and ability to write, test, and debug APIs Experience working with agile methodologies and working in cross-functional teams Must be proactive, demonstrate initiative and be a logical thinker Must be an inquisitive learner and have a thirst for improvement Ability to understand and apply customer requirements, including drawing out unforeseen implications and making design recommendations", "Strong facilitation and consensus building skills", "Strong oral and written communication skills; Ability to communicate by simplifying complexity Ability to provide work guidance to junior level developers Enjoys higher learning and keeps track of industry best practices and trends and through acquired knowledge, takes advantage of process and system improvement opportunities Be proactive requiring minimal supervision, be strong time management and work organization skills Have an ability prioritize workload and handle multiple tasks and at times meet tight deadlines", "AWS Certified Solutions Architect - Associate/Professional AWS Certified Developer - Associate/Professional AWS Certified DevOps Engineer AWS Certified Solutions Architect AWS Certified Data Analytics AWS Certified Security - Specialty AWS Certified Cloud Practitioner", "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"], "Responsibilities": ["Responsibilities Designs and develops programs and tools to support data ingestion, curation and provisioning of complex enterprise data to achieve analytics & reporting on our current technology stack Designs and builds data extracts, integrations and transformations Provides successful deployment and provisioning of data solutions to required environments Designs and builds data architecture and applications that successfully enable speed, quality and efficient pipelines Interacts with cross-functional customers and development team to gather and define requirements", "Develops understanding of the data and builds business acumen", "Reviews discrepancies in requirements and resolves with stakeholders", "Identifies and recommends appropriate continuous improvement opportunities and ensures integrations are automated and have proper exception handling"], "Benefits": ["$100K -- $150K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "454111", "job_naics_name": "Electronic Shopping"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "Z9le0-g8NzcyRxJ0AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/taunton/mdm-data-engineer-65800fa6e2a04004aa753568", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/taunton/mdm-data-engineer-65800fa6e2a04004aa753568", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113746, "job_posted_at_datetime_utc": "2023-12-20T23:09:06.000Z", "job_city": "Taunton", "job_state": "MA", "job_country": "US", "job_latitude": 41.9001, "job_longitude": -71.08977, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Z9le0-g8NzcyRxJ0AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:09:06.000Z", "job_offer_expiration_timestamp": 1705532946, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Cognizant", "employer_logo": "https://www.cognizant.com/content/dam/cognizant/en_us/dotcom/logos/COG-Logo-2022.svg", "employer_website": "http://www.cognizant.com", "employer_company_type": "Computer Services", "job_publisher": "Nashville, TN - Geebo", "job_id": "DfgsSQKmvdOLqQ2mAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Azure Data Engineer at Cognizant in Nashville, TN", "job_apply_link": "https://nashville-tn.geebo.com/jobs-online/view/id/1066089664-azure-data-engineer-at-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4486, "apply_options": [{"publisher": "Nashville, TN - Geebo", "apply_link": "https://nashville-tn.geebo.com/jobs-online/view/id/1066089664-azure-data-engineer-at-/", "is_direct": true}], "job_description": "Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks You must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future Job Title - Azure Data Engineer Location - Hartford, CT (Remote until COVID) Roles/\nResponsibilities:\nDesigning and coding Hadoop applications to analyze data collections. Develop and unit test ETL/ ELT data pipelines using ADF V2 and other components like Azure SQL, Azure SQL DW, ADLS Gen2 etc. Maintain and manage code with Azure DevOps. Implement secure integration with data sources using Azure Key Vault, Managed Identity, Private Links etc. Create data pipeline involving diverse systems such Azure SQL DB, Azure SQL DW, ADLS Gen2, Azure Databricks, On premise compute. Engage with onsite-offshore team for daily activities Status reporting - weekly and monthly basis. Document functional and technical design. Required\nQualifications:\nBachelor's degree in Software Engineering or Computer Science. 2-3 Experience working on ETL/ELT tools. Understanding and experience in Azure and Azure Data ecosystem Good to have programming language experience with .NET/Python or Scala/ Spark. 2-3 years' experience working with ADF. 2-3 years' experience working with Azure Data Lake. 2-3 years' experience working with Azure SQL DB or Azure Synapse. Experience in Machine Learning Studio, Stream Analytics, Event/IoT Hubs, and Cosmos. Nice to have Databricks experience. Nice to have Azure Logic Apps, Azure Blog Storage experience. Nice to have data migration experience from on prem to cloud. Knowledge of ADF Fault Tolerance, load balancing, security controls and distributed processing capabilities. 5-8 Years of work experience in ETL tools from designing and implementing data ingestion and processing pipelines using any big data technologies Minimum 3\nyears of experience in designing and implementing a fully operational solution on Snowflake Data Warehouse Excellent understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies Should be having good presentation and communication skills, both written and verbal Ability to problem solve and able to convert the requirements to design Well versed in RDBMS databases Good communication skill Person must have good data analytical skills Hands on python programming is a plus Good knowledge and extensive experience in SQL Should have experience working with any of the cloud and able to load data from cloud storage\nSalary Range:\n$100K -- $150K\nMinimum Qualification\nData Science & Machine Learning, Systems Architecture & Engineering, Software DevelopmentEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Nashville", "job_state": "TN", "job_country": "US", "job_latitude": 36.162663, "job_longitude": -86.7816, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=DfgsSQKmvdOLqQ2mAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "541511", "job_naics_name": "Custom Computer Programming Services"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "JL0uizy3XHYGEmdeAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/athol/mdm-data-engineer-65800fd81bb8db04c085e0e1", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/athol/mdm-data-engineer-65800fd81bb8db04c085e0e1", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703114119, "job_posted_at_datetime_utc": "2023-12-20T23:15:19.000Z", "job_city": "Athol", "job_state": "MA", "job_country": "US", "job_latitude": 42.595932, "job_longitude": -72.22674, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=JL0uizy3XHYGEmdeAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:15:19.000Z", "job_offer_expiration_timestamp": 1705533319, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Koch Industries", "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQJLPxwM4W8LjSXGoaKBgxNiDWrXZE5FSPfBg&usqp=CAU", "employer_website": "http://www.kochind.com", "employer_company_type": "Manufacturing", "job_publisher": "Geebo", "job_id": "fTe-ruGS7Lnvu3owAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer at Koch Industries in Wichita, KS", "job_apply_link": "https://geebo.com/jobs-online/view/id/1071476665-data-engineer-at-koch-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4403, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1071476665-data-engineer-at-koch-/", "is_direct": true}], "job_description": "Description Koch Minerals & Trading (KM&T) is seeking a motivated and self-driven Data Engineer to join our team in Houston, TX or Wichita, KS. KM&T is a global commodity trading company participating in nearly all commodity markets but focusing on petroleum and energy. This individual will be supporting Traders and Market Analysts by developing systems and data pipelines to handle research, trading, and risk management activities for the Gas, Power and Renewables desk. A successful candidate will have the following characteristics:\nPro-active and capable of working independently to achieve project deadlines Excellent communication and interpersonal skills with the proven ability to work as part of a team Ability to work accurately, efficiently, and independently Meticulous nature, detail-oriented, and producing high quality work Learning and teaching new technologies as required What You Will Do In Your Role The Quantitative Data Developer will work with a team of traders, market analysts and accounting staff to support and improve operations. Specific responsibilities include:\nBuilding and maintaining risk management and trading tools Managing market and fundamental data pipelines in accordance with governance principles Coordinating with development and support teams throughout the company to complete complex projects Creating analytic tools and visualizations to help guide trading decisions Monitoring systems and processes to ensure consistent performance and availability The Experience You Will Bring Requirements:\nBachelor's degree or higher in a quantitative field (examples may include Mathematics, Sciences, Engineering, Economics, Computer Sciences, etc.) Minimum of 3-years of experience as a Quantitative Developer or Data Engineer Python programming experience What Will Put You Ahead Experience with languages such as C#, Java, or R Experience with AWS cloud technologies including Lambda, Glue, Athena, Redshift, EMR Experience with relational databases such as Microsoft SQL server or Postgres Experience loading, transforming, and categorizing data from multiple source systems Knowledge of quantitative financial concepts such as derivative pricing Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter. At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine Learning, Business Intelligence & AnalyticsEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Wichita", "job_state": "KS", "job_country": "US", "job_latitude": 37.687176, "job_longitude": -97.330055, "job_benefits": ["health_insurance", "dental_coverage", "retirement_savings", "paid_time_off"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=fTe-ruGS7Lnvu3owAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Pro-active and capable of working independently to achieve project deadlines Excellent communication and interpersonal skills with the proven ability to work as part of a team Ability to work accurately, efficiently, and independently Meticulous nature, detail-oriented, and producing high quality work Learning and teaching new technologies as required What You Will Do In Your Role The Quantitative Data Developer will work with a team of traders, market analysts and accounting staff to support and improve operations", "Bachelor's degree or higher in a quantitative field (examples may include Mathematics, Sciences, Engineering, Economics, Computer Sciences, etc.)", "Minimum of 3-years of experience as a Quantitative Developer or Data Engineer Python programming experience What Will Put You Ahead Experience with languages such as C#, Java, or R Experience with AWS cloud technologies including Lambda, Glue, Athena, Redshift, EMR Experience with relational databases such as Microsoft SQL server or Postgres Experience loading, transforming, and categorizing data from multiple source systems Knowledge of quantitative financial concepts such as derivative pricing Our goal is for each employee, and their families, to live fulfilling and healthy lives"], "Responsibilities": ["This individual will be supporting Traders and Market Analysts by developing systems and data pipelines to handle research, trading, and risk management activities for the Gas, Power and Renewables desk"], "Benefits": ["Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance", "Any compensation range provided for a role is an estimate determined by available market data", "$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "35201400", "job_onet_job_zone": "2", "job_naics_code": "32519", "job_naics_name": "Other Basic Organic Chemical Manufacturing"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "s_mQdq9dgSld7XGoAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/north-reading/mdm-data-engineer-65800fa6e2a04004aa753573", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/north-reading/mdm-data-engineer-65800fa6e2a04004aa753573", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113747, "job_posted_at_datetime_utc": "2023-12-20T23:09:07.000Z", "job_city": "North Reading", "job_state": "MA", "job_country": "US", "job_latitude": 42.575092, "job_longitude": -71.07867, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=s_mQdq9dgSld7XGoAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:09:07.000Z", "job_offer_expiration_timestamp": 1705532947, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "rt2iY8Ae4XEftyuVAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/connecticut/torrington/mdm-data-engineer-65800f9e89fe4a04c7f33100", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/connecticut/torrington/mdm-data-engineer-65800f9e89fe4a04c7f33100", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113471, "job_posted_at_datetime_utc": "2023-12-20T23:04:31.000Z", "job_city": "Torrington", "job_state": "CT", "job_country": "US", "job_latitude": 41.800304, "job_longitude": -73.12117, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=rt2iY8Ae4XEftyuVAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:04:31.000Z", "job_offer_expiration_timestamp": 1705532671, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "dH_KcoVB9a5PS0A-AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/connecticut/east-granby/mdm-data-engineer-65800f992ee8d604cf848690", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/connecticut/east-granby/mdm-data-engineer-65800f992ee8d604cf848690", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113370, "job_posted_at_datetime_utc": "2023-12-20T23:02:50.000Z", "job_city": "East Granby", "job_state": "CT", "job_country": "US", "job_latitude": 41.941208, "job_longitude": -72.72732, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=dH_KcoVB9a5PS0A-AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:02:50.000Z", "job_offer_expiration_timestamp": 1705532570, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "RrMFxb5UpKriuVqRAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/ashby/mdm-data-engineer-658010052ee8d604cf848fd7", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/ashby/mdm-data-engineer-658010052ee8d604cf848fd7", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703114505, "job_posted_at_datetime_utc": "2023-12-20T23:21:45.000Z", "job_city": "Ashby", "job_state": "MA", "job_country": "US", "job_latitude": 42.677277, "job_longitude": -71.81871, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=RrMFxb5UpKriuVqRAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:21:45.000Z", "job_offer_expiration_timestamp": 1705533705, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "obmiC06jqhsyw-ThAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/mansfield/mdm-data-engineer-65800fa51bb8db04c085dcb4", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/mansfield/mdm-data-engineer-65800fa51bb8db04c085dcb4", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113726, "job_posted_at_datetime_utc": "2023-12-20T23:08:46.000Z", "job_city": "Mansfield", "job_state": "MA", "job_country": "US", "job_latitude": 42.033455, "job_longitude": -71.219055, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=obmiC06jqhsyw-ThAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:08:46.000Z", "job_offer_expiration_timestamp": 1705532926, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Hire Openings", "job_id": "62L9S4jffOS65fHgAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "MDM Data Engineer", "job_apply_link": "https://hireopenings.com/careerlistings/massachusetts/boxford/mdm-data-engineer-65800fa789fe4a04c7f331c8", "job_apply_is_direct": false, "job_apply_quality_score": 0.3725, "apply_options": [{"publisher": "Hire Openings", "apply_link": "https://hireopenings.com/careerlistings/massachusetts/boxford/mdm-data-engineer-65800fa789fe4a04c7f331c8", "is_direct": false}], "job_description": "Objectives of the role\n\u2022 Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual.\n\u2022 Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs.\n\nDaily and Monthly Responsibilities\n\u2022 Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.\n\u2022 Develop Master Data Management (MDM) solutions various master data domains within MassMutual.\n\u2022 Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.\n\u2022 Implement data modeling policies, procedures, processes, and standards, while also contributing feedback.\n\u2022 Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.\n\u2022 Establish data quality benchmarks and develop tools/processes to ensure data accuracy.\n\u2022 Conduct data profiling and perform detailed analysis of source system data.\n\u2022 Collaborate with various departments to comprehend emerging data patterns.\n\u2022 Convert high-level business needs into precise technical specification\n\u2022 Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.\n\nBasic Qualifications\n\u2022 Bachelor\u2019s degree in computer science or engineering.\n\u2022 5+ years of experience with Informatica Power Center.\n5+ years of experience with data analytics, data modeling, and database design.\n\u2022 3+ years of coding and scripting (Python, Java, Scala) and design experience.\n\u2022 3+ years of experience with Informatica MDM platform in customer/party subject area.\n\u2022 Experience with ELT methodologies and tools.\n\u2022 Expertise in tuning and troubleshooting SQL.\n\u2022 Strong data integrity, analytical and multitasking skills.\n\u2022 Experience with Oracle database.\n\u2022 Experience with AWS\n\u2022 Knowledge of basic UNIX commands and shell scripts.\n\u2022 Experience with 3rd party job schedulers like Maestro.\n\u2022 Experience with RESTful APIs.\n\u2022 Experience with near real-time mastering via SIF.\n\u2022 Experience with data profiling tools.\n\u2022 Willingness to provide Level 2 support for batch processing cycles outside business hours.\n\u2022 Excellent communication, problem solving, organizational and analytical skills.\n\u2022 Able to work independently.\n\u2022 Authorized to work in the USA with or without sponsorship.\n\nPreferred Qualifications\n\u2022 Master\u2019s degree in computer science or engineering.\n\u2022 Familiar with agile project delivery process.\n\u2022 Knowledge of SQL and use in data access and analysis.\n\u2022 Ability to manage diverse projects impacting multiple roles and processes.\n\u2022 Able to troubleshoot problem areas and identify data gaps and issues.\n\u2022 Ability to adapt to fast changing environment.\n\u2022 Experience with Python.\n\u2022 Experience with Kafka\n\u2022 Basic knowledge of database technologies (Vertica, Redshift, etc.).\n\u2022 Experience designing and implementing automated ETL processes.\n\n#LI-RK1\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703113753, "job_posted_at_datetime_utc": "2023-12-20T23:09:13.000Z", "job_city": "Boxford", "job_state": "MA", "job_country": "US", "job_latitude": 42.66116, "job_longitude": -70.99673, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=62L9S4jffOS65fHgAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-17T23:09:13.000Z", "job_offer_expiration_timestamp": 1705532953, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree in computer science or engineering", "5+ years of experience with Informatica Power Center", "5+ years of experience with data analytics, data modeling, and database design", "3+ years of coding and scripting (Python, Java, Scala) and design experience", "3+ years of experience with Informatica MDM platform in customer/party subject area", "Experience with ELT methodologies and tools", "Expertise in tuning and troubleshooting SQL", "Strong data integrity, analytical and multitasking skills", "Experience with Oracle database", "Experience with AWS", "Knowledge of basic UNIX commands and shell scripts", "Experience with 3rd party job schedulers like Maestro", "Experience with RESTful APIs", "Experience with near real-time mastering via SIF", "Experience with data profiling tools", "Excellent communication, problem solving, organizational and analytical skills", "Able to work independently", "Authorized to work in the USA with or without sponsorship"], "Responsibilities": ["Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains in MassMutual", "Working on a range of projects including batch pipelines, data modeling, and Master data solutions, you\u2019ll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business needs", "Daily and Monthly Responsibilities", "Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository", "Develop Master Data Management (MDM) solutions various master data domains within MassMutual", "Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration", "Implement data modeling policies, procedures, processes, and standards, while also contributing feedback", "Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information", "Establish data quality benchmarks and develop tools/processes to ensure data accuracy", "Conduct data profiling and perform detailed analysis of source system data", "Collaborate with various departments to comprehend emerging data patterns", "Convert high-level business needs into precise technical specification", "Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}]}
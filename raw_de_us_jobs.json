{"status": "OK", "request_id": "fe3eb8b6-c264-4e7f-8756-50826e8dd9ce", "parameters": {"query": "data engineer jobs in usa", "page": 2, "num_pages": 4, "date_posted": "today"}, "data": [{"employer_name": "Apple Inc.", "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT5BSMIt7m2uiId_3142LNElhU-kvf9h48rIEOM&s=0", "employer_website": "http://www.apple.com", "employer_company_type": "Manufacturing", "job_publisher": "Jobs Trabajo.org", "job_id": "kRSObXGDmRiDPrpwAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer, Data Engineering", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-a0deb03795ed8ff75811d4a39f4d9b9e", "job_apply_is_direct": false, "job_apply_quality_score": 0.4321, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-a0deb03795ed8ff75811d4a39f4d9b9e", "is_direct": false}], "job_description": "Data Science Engineer - Strategic Data Solutions\nAre you passionate about applying your data skills in a real-world tech environment?Join Apple, and help us leave the world better than we found it. Apple\u2019s Strategic Data Solutions (SDS) team is responsible for mitigating fraud, waste and abuse company-wide while optimizing and empowering our customers and internal partners.The SDS software engineering team is building an environment to enable ground breaking data analysis over Petabytes of data. We work side-by-side with machine learning engineers and implement scalable, easy-to-use systems and tools. We are seeking a customer- focused, passionate and driven Software Engineer with experience in building analytic tools and solutions.\nMastery of one of Python, Java, Scala, C++ or equivalent language\n3+ years of experience in software engineering/data science\nExperience building data science or data analysis tools on Hadoop/cloud based systems\nSome experience with Docker, Kubernetes, or cloud platform deployment\nExperience with Relational databases and NoSQL databases\nDemonstrated understanding of the full software development lifecycle\nSolid grasp of computer science fundamentals including data structures and algorithms\nDescription With the expansive data we have, our job is to build meaningful data relationships and engagement experiences for our internal customers. As a Software Engineer on the SDS team, you will work closely with Machine Learning Engineers and other Software Engineers to lead the design and implementation of systems and tools to support the fraud prevention efforts of SDS.You will be:\u2022 Developing and implementing production software for preventing fraud\u2022 Responsible for system architecture design\u2022 Working with external infrastructure teams to drive the development of infrastructure needs\u2022 Innovating by recognizing opportunities for automation and tools improvements\u2022 Responsible for developing and implementing process improvements to bring efficiency and stability to fraud analytics\u2022 Responsible for technical leadership for a team of data scientists\u2022 Lead the team to increase the level of maturity and skill in analytical software development\u2022 Responsible for release engineering\nBS or advanced degree in Computer Science, related field or equivalent experience\nApple is an equal opportunity employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. At Apple, base pay is one part of our total compensation package and is determined within a range. The base pay range for this role is between $161,700 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location. Apple employees also have the opportunity to become an Apple shareholder through participation in Apple\u2019s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple\u2019s Employee Stock Purchase Plan. Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses \u2014 including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program. #", "job_is_remote": false, "job_posted_at_timestamp": 1703122058, "job_posted_at_datetime_utc": "2023-12-21T01:27:38.000Z", "job_city": "Seattle", "job_state": "WA", "job_country": "US", "job_latitude": 47.60614, "job_longitude": -122.33285, "job_benefits": ["health_insurance", "dental_coverage", "retirement_savings"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=kRSObXGDmRiDPrpwAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["We are seeking a customer- focused, passionate and driven Software Engineer with experience in building analytic tools and solutions", "Mastery of one of Python, Java, Scala, C++ or equivalent language", "3+ years of experience in software engineering/data science", "Experience building data science or data analysis tools on Hadoop/cloud based systems", "Some experience with Docker, Kubernetes, or cloud platform deployment", "Experience with Relational databases and NoSQL databases", "Demonstrated understanding of the full software development lifecycle", "Solid grasp of computer science fundamentals including data structures and algorithms", "BS or advanced degree in Computer Science, related field or equivalent experience"], "Responsibilities": ["As a Software Engineer on the SDS team, you will work closely with Machine Learning Engineers and other Software Engineers to lead the design and implementation of systems and tools to support the fraud prevention efforts of SDS", "You will be:\u2022 Developing and implementing production software for preventing fraud\u2022 Responsible for system architecture design\u2022 Working with external infrastructure teams to drive the development of infrastructure needs\u2022 Innovating by recognizing opportunities for automation and tools improvements\u2022 Responsible for developing and implementing process improvements to bring efficiency and stability to fraud analytics\u2022 Responsible for technical leadership for a team of data scientists\u2022 Lead the team to increase the level of maturity and skill in analytical software development\u2022 Responsible for release engineering"], "Benefits": ["The base pay range for this role is between $161,700 and $243,300, and your base pay will depend on your skills, qualifications, experience, and location", "Apple employees also have the opportunity to become an Apple shareholder through participation in Apple\u2019s discretionary employee stock programs", "Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses \u2014 including tuition", "Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "334111", "job_naics_name": "Electronic Computer Manufacturing"}, {"employer_name": "State Street", "employer_logo": "https://upload.wikimedia.org/wikipedia/ru/thumb/7/76/State_Street_Corporation.png/800px-State_Street_Corporation.png", "employer_website": "http://www.statestreet.com", "employer_company_type": "Finance", "job_publisher": "LinkedIn", "job_id": "Qf_Xt4i0qwqJEPfFAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer, Assistant Vice President, Hybrid", "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-assistant-vice-president-hybrid-at-state-street-3772654533", "job_apply_is_direct": false, "job_apply_quality_score": 0.6766, "apply_options": [{"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-assistant-vice-president-hybrid-at-state-street-3772654533", "is_direct": false}, {"publisher": "The Muse", "apply_link": "https://www.themuse.com/jobs/statestreet/data-engineer-assistant-vice-president-hybrid-4ccfcb", "is_direct": true}, {"publisher": "Levels.fyi", "apply_link": "https://www.levels.fyi/jobs?jobId=132854792033575622", "is_direct": false}], "job_description": "Who We Are Looking For\n\nThis is a hands-on Data Engineer position in State Street Global Technology Services. We are looking for candidate with good knowledge on Bigdata technology and strong development experience with Databricks. You will help to migrate existing on-prem applications to cloud (Azure preferred), create and maintain new data applications relying on experience and judgment to plan and accomplish goals, while working with other globally situated team members.\n\nThis role can be performed in a hybrid model, where you can balance work from home and office to match your needs and role requirements.\n\nWhat You Will Be Responsible For\n\nAs Data Engineer you will\n\u2022 Design & Develop custom high throughput and configurable frameworks/libraries\n\u2022 Ability to drive change through collaboration, influence and demonstration of POCs\n\u2022 Responsible for all aspects of the software development lifecycle, including design, coding, integration testing, deployment, and documentation\n\u2022 Work collaboratively within an agile project team\n\u2022 Follow best practices and coding standards\n\u2022 Grow your personal skillset\n\nWhat We Value\n\nThese skills will help you succeed in this role\n\u2022 Experience performing data analysis and data exploration\n\u2022 Experience working in an agile delivery environment\n\u2022 Hands on development experience on Java is big plus\n\u2022 Exposure/understanding of DevOps best practice and CICD (i.e. Jenkins)\n\u2022 Experience working in a multi-developer environment, using version control (i.e. Git)\n\u2022 Strong knowledge on Databricks SQL/Spark/Scala - Data engineering pipeline\n\u2022 Strong Experience in Unix, Python and complex SQL\n\u2022 Strong critical thinking, communication, and problem-solving skills\n\u2022 Strong hands-on experience in troubleshooting Devops pipelines and Azure services\n\nEducation & Preferred Qualifications\n\u2022 Bachelor\u2019s Degree level qualification in a computer or IT related subject\n\u2022 5+ years of overall Bigdata data pipeline experience\n\u2022 2+ years of Databricks hands on experience\n\u2022 2+ years of experience on cloud-based development including Azure Services, Azure Devops, Kubernetes, Docker\n\nAdditional Requirements\n\u2022 Communicate effectively in a professional manner both written and orally\n\u2022 Team player with a positive attitude enthusiasm initiative and self-motivation\n\u2022 Ability to multi-task energetic fast learner & problem solver\n\u2022 Experience of working in the financial industry\n\u2022 Experience with agile development methodology\n\nAre you the right candidate? Yes!\n\nWe truly believe in the power that comes from the diverse backgrounds and experiences our employees bring with them. Although each vacancy details what we are looking for, we don\u2019t necessarily need you to fulfil all of them when applying. If you like change and innovation, seek to see the bigger picture, make data driven decisions and are a good team player, you could be a great fit.\n\nWhy this role is important to us\n\nOur technology function, Global Technology Services (GTS), is vital to State Street and is the key enabler for our business to deliver data and insights to our clients. We\u2019re driving the company\u2019s digital transformation and expanding business capabilities using industry best practices and advanced technologies such as cloud, artificial intelligence and robotics process automation.\n\nWe offer a collaborative environment where technology skills and innovation are valued in a global organization. We\u2019re looking for top technical talent to join our team and deliver creative technology solutions that help us become an end-to-end, next-generation financial services company.\n\nJoin us if you want to grow your technical skills, solve real problems and make your mark on our industry.\n\nAbout State Street\n\nWhat we do. State Street is one of the largest custodian banks, asset managers and asset intelligence companies in the world. From technology to product innovation, we\u2019re making our mark on the financial services industry. For more than two centuries, we\u2019ve been helping our clients safeguard and steward the investments of millions of people. We provide investment servicing, data & analytics, investment research & trading and investment management to institutional clients.\n\nWork, Live and Grow. We make all efforts to create a great work environment. Our benefits packages are competitive and comprehensive. Details vary by location, but you may expect generous medical care, insurance and savings plans, among other perks. You\u2019ll have access to flexible Work Programs to help you match your needs. And our wealth of development programs and educational support will help you reach your full potential.\n\nInclusion, Diversity and Social Responsibility. We truly believe our employees\u2019 diverse backgrounds, experiences and perspectives are a powerful contributor to creating an inclusive environment where everyone can thrive and reach their maximum potential while adding value to both our organization and our clients. We warmly welcome candidates of diverse origin, background, ability, age, sexual orientation, gender identity and personality. Another fundamental value at State Street is active engagement with our communities around the world, both as a partner and a leader. You will have tools to help balance your professional and personal life, paid volunteer days, matching gift programs and access to employee networks that help you stay connected to what matters to you.\n\nState Street is an equal opportunity and affirmative action employer.\n\nSalary Range\n\n$100,000 - $160,000 Annual\n\nThe range quoted above applies to the role in the primary location specified. If the candidate would ultimately work outside of the primary location above, the applicable range could differ.\n\nJob ID: R-742764", "job_is_remote": false, "job_posted_at_timestamp": 1703157618, "job_posted_at_datetime_utc": "2023-12-21T11:20:18.000Z", "job_city": "Princeton", "job_state": "NJ", "job_country": "US", "job_latitude": 40.357296, "job_longitude": -74.66722, "job_benefits": ["health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Qf_Xt4i0qwqJEPfFAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-27T20:19:24.000Z", "job_offer_expiration_timestamp": 1706386764, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["We are looking for candidate with good knowledge on Bigdata technology and strong development experience with Databricks", "Experience performing data analysis and data exploration", "Experience working in an agile delivery environment", "Hands on development experience on Java is big plus", "Exposure/understanding of DevOps best practice and CICD (i.e", "Experience working in a multi-developer environment, using version control (i.e", "Git)", "Strong knowledge on Databricks SQL/Spark/Scala - Data engineering pipeline", "Strong Experience in Unix, Python and complex SQL", "Strong critical thinking, communication, and problem-solving skills", "Strong hands-on experience in troubleshooting Devops pipelines and Azure services", "Communicate effectively in a professional manner both written and orally", "Team player with a positive attitude enthusiasm initiative and self-motivation", "Ability to multi-task energetic fast learner & problem solver", "Experience of working in the financial industry", "Experience with agile development methodology"], "Responsibilities": ["This role can be performed in a hybrid model, where you can balance work from home and office to match your needs and role requirements", "Design & Develop custom high throughput and configurable frameworks/libraries", "Ability to drive change through collaboration, influence and demonstration of POCs", "Responsible for all aspects of the software development lifecycle, including design, coding, integration testing, deployment, and documentation", "Work collaboratively within an agile project team", "Follow best practices and coding standards"], "Benefits": ["Our benefits packages are competitive and comprehensive", "Details vary by location, but you may expect generous medical care, insurance and savings plans, among other perks", "You\u2019ll have access to flexible Work Programs to help you match your needs", "And our wealth of development programs and educational support will help you reach your full potential", "You will have tools to help balance your professional and personal life, paid volunteer days, matching gift programs and access to employee networks that help you stay connected to what matters to you", "$100,000 - $160,000 Annual", "The range quoted above applies to the role in the primary location specified"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "11101100", "job_onet_job_zone": "5", "job_naics_code": "523920", "job_naics_name": "Portfolio Management"}, {"employer_name": "Intel", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Intel_logo_%282006-2020%29.svg/1005px-Intel_logo_%282006-2020%29.svg.png", "employer_website": "http://www.intel.com", "employer_company_type": "Manufacturing", "job_publisher": "Jobs Trabajo.org", "job_id": "mt80oP1fXST1gKY1AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer - Data Engineering -Big Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-3bfe69642012cca80f149b9256bc2069", "job_apply_is_direct": false, "job_apply_quality_score": 0.4321, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-3bfe69642012cca80f149b9256bc2069", "is_direct": false}], "job_description": "Fab Sort Manufacturing (FSM) is responsible for the production of all Intel silicon using some of the world's most advanced manufacturing processes in fabs in Arizona, Ireland, Israel, Oregon and 2 new greenfield sites in Ohio and Germany. 0 strategy, FSM is rapidly expanding its operation to deliver output for both internal and foundry customers with state-of-the-art technologies arriving in high-volume manufacturing at a 2-year cadence going forward. Intel recently created HVM Global Yield organization in FSM to strengthen its yield operation and enable fast-paced yield ramp-up in early HVM phases for each technology in collaboration with Technology Development team and FSM fab managers.\nThis job requisition is to seek Data Science (DS) team engineering roles in FSM HVM Global Yield organization, reporting to Data Science team manager. Selected candidates will work with other members in Global Yield org including Process Integration, Device and Defect engineering teams, fab module/yield teams and TD team members to achieve yield ramp-up and process optimization in early production stage, supporting internal and external customers.\nData Science engineers' responsibilities include (but are not limited to):\n+ Identify valuable data sources and set automated data collection process to build and manage the dataset used for yield analysis.\n+ Processing of structured and unstructured data, and building predictive models and machine learning algorithms.\n+ Work with Yield Modelling team to develop new yield analysis methods and algorithms to deliver world class yield analytics and machine-learning solutions in high-volume manufacturing environment.\n+ Collaborate with Technology Development, Program Managers, Process Integration, Device Integration and Defect teams to identify yield and performance detractors and enhancement opportunities and support fast paced yield ramp-up in high-volume manufacturing phase.\n+ Engineering support for technical interactions with internal and external customers.\nBachelor's degree in science and engineering major.\n+ 3+ years' experience in advanced node semiconductor industry in yield analysis and data science.\n+ 3+ years' experience in program languages and big data to develop a new analysis method and algorithms using large amount of fab data. Expertise in big data analysis and machine-learning.\n+ 3+ years' experience in Device Physics and overall FinFET process flow.\nin science and engineering major.\n+ Experience in working or leading a TFT project.\n+ Experience in serving external Foundry customers through technical interactions.\n+ Experience in new semiconductor technology development.\n+ Basic understanding on module processes including lithography, dry etch, wet etch, CMP, diffusion, implant, thin films and metrology.\n\\#foundry\n\u2022 *As the world's largest chip manufacturer, Intel strives to make every facet of semiconductor manufacturing state-of-the-art -- from semiconductor process development and manufacturing, through yield improvement to packaging, final test and optimization, and world class Supply Chain and facilities support. Employees in the Technology Development and Manufacturing Group are part of a worldwide network of design, development, manufacturing, and assembly/test facilities, all focused on utilizing the power of Moore\u2019s Law to bring smart, connected devices to every person on Earth.\n\u2022 *All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.\n\u2022 *It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation.", "job_is_remote": false, "job_posted_at_timestamp": 1703130258, "job_posted_at_datetime_utc": "2023-12-21T03:44:18.000Z", "job_city": "San Juan", "job_state": "TX", "job_country": "US", "job_latitude": 26.189241, "job_longitude": -98.15529, "job_benefits": ["retirement_savings", "health_insurance", "paid_time_off"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=mt80oP1fXST1gKY1AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor's degree in science and engineering major", "3+ years' experience in advanced node semiconductor industry in yield analysis and data science", "3+ years' experience in program languages and big data to develop a new analysis method and algorithms using large amount of fab data", "Expertise in big data analysis and machine-learning", "3+ years' experience in Device Physics and overall FinFET process flow", "Experience in working or leading a TFT project", "Experience in serving external Foundry customers through technical interactions", "Experience in new semiconductor technology development"], "Responsibilities": ["Selected candidates will work with other members in Global Yield org including Process Integration, Device and Defect engineering teams, fab module/yield teams and TD team members to achieve yield ramp-up and process optimization in early production stage, supporting internal and external customers", "Identify valuable data sources and set automated data collection process to build and manage the dataset used for yield analysis", "Processing of structured and unstructured data, and building predictive models and machine learning algorithms", "Work with Yield Modelling team to develop new yield analysis methods and algorithms to deliver world class yield analytics and machine-learning solutions in high-volume manufacturing environment", "Collaborate with Technology Development, Program Managers, Process Integration, Device Integration and Defect teams to identify yield and performance detractors and enhancement opportunities and support fast paced yield ramp-up in high-volume manufacturing phase", "Engineering support for technical interactions with internal and external customers"], "Benefits": ["*It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "334413", "job_naics_name": "Semiconductor and Related Device Manufacturing"}, {"employer_name": "Chewy", "employer_logo": "https://static1.squarespace.com/static/5e0fb6a85a52fd5fb32a83c1/t/5ea5d85bf9a9ba212f12ce58/1587927131703/Chewy_Logo_RGB_Blue.jpg", "employer_website": "http://www.chewy.com", "employer_company_type": "Retail", "job_publisher": "Geebo", "job_id": "YSufnz5oOO6JlFVSAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer II at Chewy in Dallas, TX", "job_apply_link": "https://geebo.com/jobs-online/view/id/1070861941-data-engineer-ii-at-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4353, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1070861941-data-engineer-ii-at-/", "is_direct": true}], "job_description": "Our Opportunity:\nChewy's Planning and Analytics team has an exciting opportunity for a Data Engineer II to join the pack. With a background in data engineering, data analysis and reporting you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization. What You'll Do:\nDesign, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data & forecasting products by working with analytics and data scientist team members to speed to market and to enrich data and model options. Assist in creating Proof of Concepts and advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Code, test, and document new or modified data systems to create robust and scalable applications for data analytics. Partner with Business Analysts and Solutions Architects to develop technical architectures for strategic enterprise projects and initiatives. Document technical details of your work using industry standard protocols like Jira, Confluence etc. Respect for Teammates - encouraging diverse identities, backgrounds, talent and perspectives. Collaboration - working with all team members, and across teams, to integrate perspectives and encourage contributions. Adaptability - quickly applying feedback and circumstances to team priorities. What You'll Need:\nBachelor of Science or Master's degree in Computer Science, Engineering, Information Systems, Mathematics or related field 7\nyears of enterprise experience as a data engineer and/or software engineer Strong software development skills in Python and Advanced SQL, tuning SQL queries and data pipelines Proven ability to demonstrate developing modern data pipelines and applications for analytics (e.g., BI, reporting, dashboards) and advanced analytics (e.g., machine learning, deep learning) use cases. Knowledge of industry best practices, strong understanding how data is extracted, transformed, scrubbed, and loaded in a large Data Warehouse environment. Ability to effectively operate both independently and as part of a team. Self-motivated with strong problem-solving and self-learning skills. E-com, Retail or startup experience is a plus. Excellent written and oral communication skills and ability to work with development teams. Bonus:\nExposure building dimensional models in the data warehouse Experience with MPP/Columnar databases like Snowflake, Redshift, etc Exposure to data streaming tools and technologies like kafka or similar technologies Experience with developing solutions on cloud computing services and infrastructure with AWS AWS Components like EC2, S3, Athena, Lambda etc Experience with Sagemaker/Sagemaker Studio AWS Certified Developer/AWS Machine Learning 4\nyears of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems 2\nyears of experience working with workflow orchestration tools (Preferred:\nAirflow, Prefect, Luigi, AutoSys) Experience designing and building infrastructure in cloud data environments (Preferred:\nAWS, GCP, Azure).\nSalary Range:\n$100K -- $150K\nMinimum Qualification\nData Science & Machine Learning, Systems Architecture & EngineeringEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Dallas", "job_state": "TX", "job_country": "US", "job_latitude": 32.776665, "job_longitude": -96.79699, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=YSufnz5oOO6JlFVSAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 84, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Bachelor of Science or Master's degree in Computer Science, Engineering, Information Systems, Mathematics or related field 7", "years of enterprise experience as a data engineer and/or software engineer Strong software development skills in Python and Advanced SQL, tuning SQL queries and data pipelines Proven ability to demonstrate developing modern data pipelines and applications for analytics (e.g., BI, reporting, dashboards) and advanced analytics (e.g., machine learning, deep learning) use cases", "Knowledge of industry best practices, strong understanding how data is extracted, transformed, scrubbed, and loaded in a large Data Warehouse environment", "Ability to effectively operate both independently and as part of a team", "Self-motivated with strong problem-solving and self-learning skills", "Excellent written and oral communication skills and ability to work with development teams", "Exposure building dimensional models in the data warehouse Experience with MPP/Columnar databases like Snowflake, Redshift, etc Exposure to data streaming tools and technologies like kafka or similar technologies Experience with developing solutions on cloud computing services and infrastructure with AWS AWS Components like EC2, S3, Athena, Lambda etc Experience with Sagemaker/Sagemaker Studio AWS Certified Developer/AWS Machine Learning 4", "years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems 2"], "Responsibilities": ["With a background in data engineering, data analysis and reporting you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning", "This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity", "You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization", "Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals", "Solve complex data problems to deliver insights that helps our business to achieve their goals", "Create data & forecasting products by working with analytics and data scientist team members to speed to market and to enrich data and model options", "Assist in creating Proof of Concepts and advise, consult, mentor and coach other data and analytic professionals on data standards and practices", "Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team", "Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes", "Code, test, and document new or modified data systems to create robust and scalable applications for data analytics", "Partner with Business Analysts and Solutions Architects to develop technical architectures for strategic enterprise projects and initiatives", "Document technical details of your work using industry standard protocols like Jira, Confluence etc", "Respect for Teammates - encouraging diverse identities, backgrounds, talent and perspectives", "Collaboration - working with all team members, and across teams, to integrate perspectives and encourage contributions", "Adaptability - quickly applying feedback and circumstances to team priorities"], "Benefits": ["$100K -- $150K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "454111", "job_naics_name": "Electronic Shopping"}, {"employer_name": "MassMutual", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/MassMutual_logo.svg/1024px-MassMutual_logo.svg.png", "employer_website": "http://www.vfgnys.com", "employer_company_type": "Finance", "job_publisher": "Jobs Trabajo.org", "job_id": "p-SIJEFbyVvRZMgkAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer, Data Engineering", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-42b8626cab747d76e21c5b14f827bb9d", "job_apply_is_direct": false, "job_apply_quality_score": 0.4508, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-42b8626cab747d76e21c5b14f827bb9d", "is_direct": false}], "job_description": "Quantitative Data Engineer\n\nQuantitative Investment Platforms and Developer Operations Team\n\nFull-time\n\nAs a Data Engineer you will work in a fast paced, innovative, and collaborative environment on exciting technology directives. A Data Engineer embodies a unique combination of both technical and people skills. Both emotional intelligence and the ability to solve complex problems and communicate effectively are key to success. Ideally, you have a strong background in Data Engineering as well as workflow orchestration and CI/CD pipelines. Additionally, it is preferred that you have robust experience in cloud platforms and system architectures.\n\nAs a Quantitative Operations Engineer, you will join the Quantitative Investment & Developer Operations team, part of MassMutual\u2019s Investment Management organization. The Quantitative development team is comprised of highly skilled, financial industry savvy professionals who render collaborative and portfolio risk solutions to our portfolio managers. The team culture is collaborative, cross-functional, and fosters high performance results with an emphasis on encouraging a healthy work/life balance.\n\nOur ideal Quant Data Engineer is someone who is passionate about data and deploying solutions and workflow automations. You enjoy building data projects from the ground up and are equally comfortable working with business partners to understand requirements as you are developing and delivering robust solutions that meet the highest standards. Learning new technologies and working in the cloud excite you. Ingest data from disparate sources into the various data stores.\n\nCleanse and enrich data and apply adequate data quality controls.\n\nProvide insight and direction to guide the future development of MassMutual\u2019s data platform.\n\nDevelop re-usable tools to help streamline the delivery of new projects.\n\nWork in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements.\n\nBachelor\u2019s degree in Computer Science, Finance, Business or a related field.\n\n2+ years of experience in the IT, and/or finance industry\n\nUnderstanding of ETL methodologies and proficient in tools via Python\n\nHands on experience developing with Python and advanced data processing using Python libraries & AWS services.\n\nExperience working in a cloud environment and basic administration (e.g. Experience in modeling and tuning relational databases (SQL Server/PostgreSQL) and NOSQL databases (Mongo)\n\nExperience with GIT and code review/deployment & scheduling part of operations\n\nExperience in relational databases (SQLServer/PostgreSQL) and NOSQL databases (Mongo)\n\nExperience in Agile/SCRUM-specifically in Story/Acceptance criteria development\n\nMaster\u2019s degree in computer science, engineering or a related field\n\n4+ Years of experience in the IT and/or quantitative, investment or finance industry\n\nExperience with troubleshooting and root cause analysis to determine and remediate potential issues\n\nExperience with data reporting (e.g. Micro strategy, Tableau, Looker) or via python libraries\n\nExperience delivering mobile apps and app store ecosystems\n\nA Data-driven mindset and the ability to use quantitative tools and analyze unstructured feedback to inform the development of solutions\n\nCuriosity regarding emerging digital and technology trends can translate into excellent customer experiences\n\nRegular meetings with the Quantitative and ETX project teams\n\nNetworking opportunities including access to Asian, Hispanic/Latinx, African American, women, LGBTQ, Veteran and disability-focused Business Resource Groups\n\nAccess to learning content on Degreed and other informational platforms\n\nMassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. Note: Veterans are welcome to apply, regardless of their discharge status.\n\nIf you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.", "job_is_remote": false, "job_posted_at_timestamp": 1703120689, "job_posted_at_datetime_utc": "2023-12-21T01:04:49.000Z", "job_city": "Boston", "job_state": "MA", "job_country": "US", "job_latitude": 42.36008, "job_longitude": -71.05888, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=p-SIJEFbyVvRZMgkAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Both emotional intelligence and the ability to solve complex problems and communicate effectively are key to success", "Ideally, you have a strong background in Data Engineering as well as workflow orchestration and CI/CD pipelines", "You enjoy building data projects from the ground up and are equally comfortable working with business partners to understand requirements as you are developing and delivering robust solutions that meet the highest standards", "Bachelor\u2019s degree in Computer Science, Finance, Business or a related field", "Understanding of ETL methodologies and proficient in tools via Python", "Hands on experience developing with Python and advanced data processing using Python libraries & AWS services", "Experience working in a cloud environment and basic administration (e.g. Experience in modeling and tuning relational databases (SQL Server/PostgreSQL) and NOSQL databases (Mongo)", "Experience with GIT and code review/deployment & scheduling part of operations", "Experience in Agile/SCRUM-specifically in Story/Acceptance criteria development", "Master\u2019s degree in computer science, engineering or a related field", "4+ Years of experience in the IT and/or quantitative, investment or finance industry", "Experience with troubleshooting and root cause analysis to determine and remediate potential issues", "Experience with data reporting (e.g. Micro strategy, Tableau, Looker) or via python libraries", "Experience delivering mobile apps and app store ecosystems", "A Data-driven mindset and the ability to use quantitative tools and analyze unstructured feedback to inform the development of solutions", "Curiosity regarding emerging digital and technology trends can translate into excellent customer experiences"], "Responsibilities": ["Ingest data from disparate sources into the various data stores", "Cleanse and enrich data and apply adequate data quality controls", "Provide insight and direction to guide the future development of MassMutual\u2019s data platform", "Develop re-usable tools to help streamline the delivery of new projects", "Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Datadog", "employer_logo": "https://meroxa.com/static/dd326ae62365893b92a8c695910f44bd/datadog_full_logo_.png", "employer_website": "https://www.datadog.com", "employer_company_type": "Information", "job_publisher": "Mogul", "job_id": "DUO8HuBnMVx43hyYAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer - Data Science Engineering", "job_apply_link": "https://onmogul.com/jobs/data-engineer-data-science-engineering", "job_apply_is_direct": false, "job_apply_quality_score": 0.537, "apply_options": [{"publisher": "Mogul", "apply_link": "https://onmogul.com/jobs/data-engineer-data-science-engineering", "is_direct": false}, {"publisher": "New York City NY Geebo.com Free Classifieds Ads - Geebo", "apply_link": "https://newyorkcity-ny.geebo.com/jobs-online/view/id/1081387294-data-engineer-data-science-/", "is_direct": true}], "job_description": "About Datadog:\n\nWe're on a mission to build the best platform in the world for engineers to understand and scale their systems, applications, and teams. We operate at high scale\u2014trillions of data points per day\u2014providing always-on alerting, metrics visualization, logs, and application tracing for tens of thousands of companies. Our engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way.\n\nOur engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way. We need you to design and build machine learning-powered products that help our customers learn from their data and make better decisions in real-time.\n\nThe team:\nWe extract and manage data and events from our core products and live systems to make them centrally available for our Data Science team in both batch and real-time ways. We enable Data Scientists to productionize their models and expose their data assets to the rest of the company.\n\nIf you\u2019re excited to work on a fast-moving data engineering team with the best open-source data tools at high scale, we want to meet you.\n\nYou will:\n\u2022 Build distributed, real-time, high-volume data pipelines and work together with others to enable high-scale Data Science\n\u2022 Do it with Spark, Luigi, Kafka and other open-source technologies\n\u2022 Work all over the stack, moving fluidly between programming languages: Scala, Java, Python, Go, and more\n\u2022 Join a tightly knit team solving hard problems the right way\n\u2022 Own meaningful parts of our service, have an impact, grow with the company\n\nRequirements:\n\u2022 You have a BS/MS/PhD in a scientific field or equivalent experience\n\u2022 You have built and operated data pipelines for real customers in production systems\n\u2022 You are fluent in several programming languages (JVM & otherwise)\n\u2022 You enjoy wrangling huge amounts of data and exploring new data sets\n\u2022 You value code simplicity and performance\n\u2022 You want to work in a fast, high growth startup environment that respects its engineers and customers\n\u2022 You are preferably familiar with Spark and/or Hadoop and know how to put machine learning models in production\n\nIs this you? Send your resume and link to your GitHub if available.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "New York", "job_state": "NY", "job_country": "US", "job_latitude": 40.712776, "job_longitude": -74.005974, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=DUO8HuBnMVx43hyYAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["You have a BS/MS/PhD in a scientific field or equivalent experience", "You have built and operated data pipelines for real customers in production systems", "You are fluent in several programming languages (JVM & otherwise)", "You enjoy wrangling huge amounts of data and exploring new data sets", "You value code simplicity and performance", "You want to work in a fast, high growth startup environment that respects its engineers and customers", "You are preferably familiar with Spark and/or Hadoop and know how to put machine learning models in production"], "Responsibilities": ["Build distributed, real-time, high-volume data pipelines and work together with others to enable high-scale Data Science", "Join a tightly knit team solving hard problems the right way", "Own meaningful parts of our service, have an impact, grow with the company"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15111100", "job_onet_job_zone": "5", "job_naics_code": "511210", "job_naics_name": "Software Publishers"}, {"employer_name": "Google", "employer_logo": "https://kgo.googleusercontent.com/profile_vrt_raw_bytes_1587515358_10512.png", "employer_website": "http://www.google.com", "employer_company_type": "Information", "job_publisher": "LinkedIn", "job_id": "Xt9qsrszHYqyt2HBAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer, Google Customer Solutions", "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-google-customer-solutions-at-google-3774344286", "job_apply_is_direct": false, "job_apply_quality_score": 0.6787, "apply_options": [{"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/data-engineer-google-customer-solutions-at-google-3774344286", "is_direct": false}, {"publisher": "The Muse", "apply_link": "https://www.themuse.com/jobs/google/data-engineer-google-customer-solutions", "is_direct": false}, {"publisher": "BeBee", "apply_link": "https://us.bebee.com/job/20231205-6f73f6eeea0c8918e99e2ef5863891bd", "is_direct": false}, {"publisher": "Jobbinghood Job", "apply_link": "https://jobbinghood.com/jobs/listing/data-engineer-iii-google-customer-solutions/", "is_direct": false}, {"publisher": "Jobilize", "apply_link": "https://www.jobilize.com/job/us-ny-new-york-data-engineer-google-customer-solutions-hiring-now", "is_direct": false}, {"publisher": "MNC Jobs 360", "apply_link": "https://mncjobs360.com/job/google-looking-for-data-engineer-iii-google-customer-solutions-at-new-york-ny-usa/", "is_direct": false}], "job_description": "Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Redwood City, CA, USA; New York, NY, USA; San Francisco, CA, USA.Minimum qualifications:\n\u2022 Bachelor\u2019s degree or equivalent practical experience.\n\u2022 Experience designing data models, data warehouses, and using SQL and NoSQL database management systems along with data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow).\n\u2022 Experience in one or more object oriented programming languages (e.g., Java, C++, or Python, etc.).\n\nPreferred qualifications:\n\u2022 Master\u2019s degree in Business, Statistics, Mathematics, Economics, Engineering or Applied Science, or a related field.\n\u2022 2 years of experience managing projects and working with analytics, software coding, or customer-side web technologies.\n\u2022 2 years of experience in a customer-facing role.\n\u2022 Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources.\n\u2022 Experience in distributed data processing and Unix or GNU/Linux systems.\n\nAbout The Job\n\nPLACEHOLDER JOB DESCRIPTION\n\nGoogle Customer Solutions is harnessing the power of artificial intelligence to build intelligent solutions that enable GCS to grow smarter and faster. AI-driven tools and systems can help automate repetitive tasks, improve decision-making processes, and unlock valuable insights from large volumes of data. Embracing AI as a strategic cornerstone for growth will empower GCS to thrive in an increasingly competitive market and unlock new opportunities for success.\n\nAs a Data Engineer, you will take on data challenges in an agile way. In this role, you will use an analytical, data-driven approach to drive a deep understanding of fast changing business. You will build data pipelines and reporting tools that enable engineers, analysts, and data science teams through feature engineering, automated data extracts, wrangling needs, and scaled insights for both the data science team and their users.\n\nWhen our millions of advertisers and publishers are happy, so are we! Our Google Customer Solutions (GCS) team of entrepreneurial, enthusiastic and client-focused members are the \"human face\" of Google, helping entrepreneurs both individually and broadly build their online presence and grow their businesses. We are dedicated to growing the unique needs of advertising companies. Our teams of strategists, analysts, advisers and support specialists collaborate closely to spot and analyze customer needs and trends. In collaboration, we create and implement business plans broadly for all types of businesses.\n\nThe US base salary range for this full-time position is $93,500-$135,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\n\nPlease note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .\n\nResponsibilities\n\u2022 Build data pipelines, reports, and frameworks that enable analysts and other stakeholders across the organization.\n\u2022 Recognize and adopt best practices in developing pipelines, analytical insights, data integrity, test design, analysis, validation, and documentation.\n\u2022 Design and develop scalable and actionable solutions (e.g., dashboards, automated collateral, web applications) that tell a story and provide insights to help advertisers grow.\n\u2022 Work with various stakeholders to understand feature/tooling gaps and innovate on behalf of customers.\n\nGoogle is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .", "job_is_remote": false, "job_posted_at_timestamp": 1703158442, "job_posted_at_datetime_utc": "2023-12-21T11:34:02.000Z", "job_city": "New York", "job_state": "NY", "job_country": "US", "job_latitude": 40.712776, "job_longitude": -74.005974, "job_benefits": ["health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Xt9qsrszHYqyt2HBAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-29T05:50:37.000Z", "job_offer_expiration_timestamp": 1706507437, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Bachelor\u2019s degree or equivalent practical experience", "Experience designing data models, data warehouses, and using SQL and NoSQL database management systems along with data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow)", "Experience in one or more object oriented programming languages (e.g., Java, C++, or Python, etc.)"], "Responsibilities": ["As a Data Engineer, you will take on data challenges in an agile way", "In this role, you will use an analytical, data-driven approach to drive a deep understanding of fast changing business", "You will build data pipelines and reporting tools that enable engineers, analysts, and data science teams through feature engineering, automated data extracts, wrangling needs, and scaled insights for both the data science team and their users", "Build data pipelines, reports, and frameworks that enable analysts and other stakeholders across the organization", "Recognize and adopt best practices in developing pipelines, analytical insights, data integrity, test design, analysis, validation, and documentation", "Design and develop scalable and actionable solutions (e.g., dashboards, automated collateral, web applications) that tell a story and provide insights to help advertisers grow", "Work with various stakeholders to understand feature/tooling gaps and innovate on behalf of customers"], "Benefits": ["The US base salary range for this full-time position is $93,500-$135,000 + bonus + equity + benefits", "Our salary ranges are determined by role, level, and location"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "41903100", "job_onet_job_zone": "4", "job_naics_code": "519130", "job_naics_name": "Internet Publishing and Broadcasting and Web Search Portals"}, {"employer_name": "Meta", "employer_logo": "https://kgo.googleusercontent.com/profile_vrt_raw_bytes_1587515400_10885.png", "employer_website": "https://www.meta.com", "employer_company_type": "Manufacturing", "job_publisher": "ZipRecruiter", "job_id": "3IPjRjihan6_KTEeAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer, Analytics", "job_apply_link": "https://www.ziprecruiter.com/c/Meta/Job/Data-Engineer,-Analytics/-in-Boise,ID?jid=4887b46b650e32cc", "job_apply_is_direct": false, "job_apply_quality_score": 0.7166, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Meta/Job/Data-Engineer,-Analytics/-in-Boise,ID?jid=4887b46b650e32cc", "is_direct": false}, {"publisher": "Data Careers Job Board", "apply_link": "https://terae.getro.com/companies/facebook-2-07d2eb1b-d3ad-4084-93df-74a124ac9233/jobs/30319596-data-engineer-product-analytics", "is_direct": false}, {"publisher": "Jobrapido.com", "apply_link": "https://us.jobrapido.com/jobpreview/3059636096", "is_direct": false}], "job_description": "at Meta in Boise, Idaho, United States\nJob Description\n\nSummary:\n\nMeta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, WhatsApp, and Novi further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. Instructions on how to apply, click \"Apply to Job\" online on this web page.\n\nRequired Skills:\n\nData Engineer, Analytics Responsibilities:\n\n1. Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making\n\n2. Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or AI research domains\n\n3. Collaborate with researchers, engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way\n\n4. Define and manage SLA for all data sets in allocated areas of ownership\n\n5. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve\n\n6. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership\n\n7. Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources\n\n8. Improve data quality by utilizing novel AI and machine learning techniques\n\n9. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\n\n10. Influence product and cross-functional teams to identify data opportunities to drive impact\n\n11. Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors\n\n12. Demonstrate good judgment in selecting methods and techniques for obtaining solutions\n\n13. Telecommute from anywhere in the U.S. permitted\n\n14. $227,360.00/year to $268,000.00/year + bonus + equity + benefits. Individual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable. In addition to base salary, Meta offers benefits. Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits.\n\nMinimum Qualifications:\n\nMinimum Qualifications:\n\n15. Requires a Master's degree in Computer Science, Engineering, Information Systems, Industrial and Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and two years of work experience in the job offered or in a computer-related occupation. Experience requires two years of experience in the following:\n\n16. Object-oriented programming languages\n\n17. Schema design and dimensional data modeling\n\n18. Writing SQL statements\n\n19. Analyzing data to identify deliverables, gaps, and inconsistencies\n\n20. Python\n\nIndustry: Internet\n\nEqual Opportunity:\n\nMeta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.\n\nMeta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.\nTo view full details and how to apply, please login or create a Job Seeker account", "job_is_remote": true, "job_posted_at_timestamp": 1703095560, "job_posted_at_datetime_utc": "2023-12-20T18:06:00.000Z", "job_city": "Boise", "job_state": "ID", "job_country": "US", "job_latitude": 43.615017, "job_longitude": -116.20232, "job_benefits": ["health_insurance"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=3IPjRjihan6_KTEeAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-20T00:00:00.000Z", "job_offer_expiration_timestamp": 1705708800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 227360, "job_max_salary": 268000, "job_salary_currency": "USD", "job_salary_period": "YEAR", "job_highlights": {"Qualifications": ["Requires a Master's degree in Computer Science, Engineering, Information Systems, Industrial and Systems Engineering, Mathematics, Statistics, Data Analytics, Applied Sciences, or a related field and two years of work experience in the job offered or in a computer-related occupation", "Experience requires two years of experience in the following:", "Object-oriented programming languages", "Schema design and dimensional data modeling", "Writing SQL statements", "Analyzing data to identify deliverables, gaps, and inconsistencies", "Meta participates in the E-Verify program in certain locations, as required by law"], "Responsibilities": ["Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making", "Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or AI research domains", "Collaborate with researchers, engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way", "Define and manage SLA for all data sets in allocated areas of ownership", "Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve", "Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership", "Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources", "Improve data quality by utilizing novel AI and machine learning techniques", "Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts", "Influence product and cross-functional teams to identify data opportunities to drive impact", "Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors", "Demonstrate good judgment in selecting methods and techniques for obtaining solutions"], "Benefits": ["$227,360.00/year to $268,000.00/year + bonus + equity + benefits", "Individual pay is determined by skills, qualifications, experience, and location", "Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable", "In addition to base salary, Meta offers benefits", "Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_occupational_categories": ["15-2041.00: Statisticians"], "job_naics_code": "325414", "job_naics_name": "Biological Product (except Diagnostic) Manufacturing"}, {"employer_name": "Harvard University", "employer_logo": "https://ces.fas.harvard.edu/uploads/art/_600x60_fit_center-center_82_none/CES-Logo_RGB_Blocks_Wide.png?mtime=1694446416", "employer_website": "https://www.harvard.edu", "employer_company_type": "Education", "job_publisher": "Jobs Trabajo.org", "job_id": "uDsAex8OXW4hTYxdAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231220-99318dabeaf55bc50c79b0aff0710878", "job_apply_is_direct": false, "job_apply_quality_score": 0.4578, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231220-99318dabeaf55bc50c79b0aff0710878", "is_direct": false}], "job_description": "64216BRAuto req ID:64216BRJob Code:I1458P IT Rprting and Analyt Prof IV Department Office Location:USA - MA - Boston Business Title:Data Engineer, Digital TransformationSub-Unit: Salary Grade (arvard.edu/salary-ranges#ranges) :058Time Status:Full-time Union:00 - Non Union, Exempt or Temporary Additional Qualifications and Skills:Bachelor's degree in mathematics, physics, computer science, engineering, statistics, or an equivalent technical discipline.\n+ Three-five years\u2019 experience working in cloud and on-premise environments with common data processing tools, writing unit tests, performing integration testing, functional and performance testing, automating workflows, and CI/CD using agile best practices.\n+ Minimum of three years\u2019 experience with building data security frameworks in compliance with GDPR and CCPA guidelines.\n+ Minimum of five years\u2019 hands-on experience with distributed data processing tools such as MapReduce, Hadoop, Hive, EMR, Kafka, Spark, etc.\n+ Minimum of five years\u2019 experience working with relational databases (SQL) and non-relational databases such as MongoDB, Redis, Cassandra, etc. with an ability to choose the right database for a use case.\nExtensive experience with performance tuning applications on cloud systems to maximize performance or other data frameworks.\n+ Experience building systems to perform real-time data processing using scalable data streaming frameworks.\n+ Expert level experience with cloud ecosystems (AWS, GCP, Azure).\n+ Experience with descriptive statistics and data analysis.\n+ Strong software development experience in popular object-oriented programming languages (C/C++, Java, Python, Scala, etc.)\n+ Experience with Unix-based systems, including bash scripting.\n+ Additional Information:This role has the possibility of being a remote or hybrid position. We may conduct candidate interviews virtually (phone and/or via Zoom) and/or in-person for this role.\nCulture of Inclusion: The work and well-being of HBS is profoundly strengthened by the diversity of our network and our differences in background, culture, national origin, religion, sexual orientation, and life experiences. Department:Digital TransformationPre-Employment Screening:Criminal, Education, IdentityJob Function:Information Technology School/Unit:Harvard Business School EEO Statement:We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.Minimum of five years\u2019 post-secondary education or relevant work experience\nBe a pioneer in business, education, and global impact by joining the Harvard Business School Digital Transformation team - a \u201cstartup with assets,\u201d where you will have the chance to deploy cutting-edge digital- and emerging-technology education solutions. As a Data Engineer, you will bring to life and implement architecture blueprints, perform data analyses, and help our team by mapping out solutions to some of our complex technical challenges. You'll provide technical expertise, analyses, mitigate risk and offer solutions tailored for our needs. From migrations of existing workloads to building advanced cloud solutions, you'll help shape and build systems to increase agility, improve security, reduce costs and meet utilization targets. You will work closely with the Data Science and Infrastructure Architecture & Platform teams and will focus on creating blueprints and finding insights to nourish a culture of engineering excellence. You will report to the Managing Director, Data, Analytics and AI, Digital Transformation.\nSupport data engineering needs across HBS by providing technical support and training in data frameworks and ways of working. Also, assist with reviewing, refactoring and integrating source code for quality control, in addition to data analyses.\n+ Design and build production-ready applications to reliably process batch and streaming data in a multi-tenancy cloud data platform and provide insights back to business systems based on their needs.\n+ Design and implement core platform functions, tools, and processes using Agile methodology to integrate data from multiple sources and support use cases, ensuring standardization and reusability of work products.\n+ Collaborate with the Group Architecture team to enhance the data platform capabilities, proposing updates to the solution architecture that utilize cloud platform best practices.\n+ Determine the tools and technologies to be used on the Data Platform and investigate new technologies to identify potential benefits and improvements.\n+ Commitment to Equity, Diversity, Inclusion, and Belonging:Harvard University views equity, diversity, inclusion, and belonging as the pathway to achieving inclusive excellence and fostering a campus culture where everyone can thrive. We strive to create a community that draws upon the widest possible pool of talent to unify excellence and diversity while fully embracing individuals from varied backgrounds, cultures, races, identities, life experiences, perspectives, beliefs, and values.Benefits:We invite you to visit Harvard\u2019s Total Rewards website ( arvard.Paid Time Off: 3-4 weeks of accrued vacation time per year (3 weeks for support staff and 4 weeks for administrative/professional staff), 12 accrued sick days per year, 12.5 holidays plus a Winter Recess in December/January, 3 personal days per year (prorated based on date of hire), and up to 12 weeks of paid leave for new parents who are primary care givers.\n+ Comprehensive medical, dental, and vision benefits, disability and life insurance programs, along with voluntary benefits. Child and elder/adult care resources including on campus childcare centers, Employee Assistance Program, and wellness programs related to stress management, nutrition, meditation, and more.\n+ Retirement: University-funded retirement plan with contributions from 5% to 15% of eligible compensation, based on age and earnings with full vesting after 3 years of service.\n+ Tuition Assistance Program: Competitive program including $40 per class at the Harvard Extension School and reduced tuition through other participating Harvard graduate schools.\n+ Tuition Reimbursement: Program that provides 75% to 90% reimbursement up to $5,250 per calendar year for eligible courses taken at other accredited institutions.\n+ Programs and classes at little or no cost, including through the Harvard Center for Workplace Development and LinkedIn Learning.\n+ Commuting and Transportation: Various commuter options handled through the Parking Office, including discounted parking, half-priced public transportation passes and pre-tax transit passes, biking benefits, and more.\n+ Harvard Facilities Access, Discounts and Perks: Access to Harvard athletic and fitness facilities, libraries, campus events, credit union, and more, as well as discounts to various types of services (legal, financial, etc.) Work Format:Hybrid (partially on-site, partially remote) Work Format Details:All remote work must be performed within one of the Harvard Registered Payroll States, which currently includes Massachusetts, Connecticut, Maine, New Hampshire, Rhode Island, Vermont, Georgia, Illinois, Maryland, New Jersey, New York, Virginia, Washington, and California (CA for exempt positions only).", "job_is_remote": false, "job_posted_at_timestamp": 1703100775, "job_posted_at_datetime_utc": "2023-12-20T19:32:55.000Z", "job_city": "Cambridge", "job_state": "MA", "job_country": "US", "job_latitude": 42.373615, "job_longitude": -71.10973, "job_benefits": ["retirement_savings", "health_insurance", "dental_coverage", "paid_time_off"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=uDsAex8OXW4hTYxdAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-27T00:00:00.000Z", "job_offer_expiration_timestamp": 1703635200, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["64216BRAuto req ID:64216BRJob Code:I1458P IT Rprting and Analyt Prof IV Department Office Location:USA - MA - Boston Business Title:Data Engineer, Digital TransformationSub-Unit: Salary Grade (arvard.edu/salary-ranges#ranges) :058Time Status:Full-time Union:00 - Non Union, Exempt or Temporary Additional Qualifications and Skills:Bachelor's degree in mathematics, physics, computer science, engineering, statistics, or an equivalent technical discipline", "Three-five years\u2019 experience working in cloud and on-premise environments with common data processing tools, writing unit tests, performing integration testing, functional and performance testing, automating workflows, and CI/CD using agile best practices", "Minimum of three years\u2019 experience with building data security frameworks in compliance with GDPR and CCPA guidelines", "Minimum of five years\u2019 hands-on experience with distributed data processing tools such as MapReduce, Hadoop, Hive, EMR, Kafka, Spark, etc", "Minimum of five years\u2019 experience working with relational databases (SQL) and non-relational databases such as MongoDB, Redis, Cassandra, etc. with an ability to choose the right database for a use case", "Extensive experience with performance tuning applications on cloud systems to maximize performance or other data frameworks", "Experience building systems to perform real-time data processing using scalable data streaming frameworks", "Expert level experience with cloud ecosystems (AWS, GCP, Azure)", "Experience with descriptive statistics and data analysis", "Strong software development experience in popular object-oriented programming languages (C/C++, Java, Python, Scala, etc.)", "Experience with Unix-based systems, including bash scripting", "Additional Information:This role has the possibility of being a remote or hybrid position", "Minimum of five years\u2019 post-secondary education or relevant work experience", "Be a pioneer in business, education, and global impact by joining the Harvard Business School Digital Transformation team - a \u201cstartup with assets,\u201d where you will have the chance to deploy cutting-edge digital- and emerging-technology education solutions"], "Responsibilities": ["As a Data Engineer, you will bring to life and implement architecture blueprints, perform data analyses, and help our team by mapping out solutions to some of our complex technical challenges", "You'll provide technical expertise, analyses, mitigate risk and offer solutions tailored for our needs", "From migrations of existing workloads to building advanced cloud solutions, you'll help shape and build systems to increase agility, improve security, reduce costs and meet utilization targets", "You will work closely with the Data Science and Infrastructure Architecture & Platform teams and will focus on creating blueprints and finding insights to nourish a culture of engineering excellence", "You will report to the Managing Director, Data, Analytics and AI, Digital Transformation", "Support data engineering needs across HBS by providing technical support and training in data frameworks and ways of working", "Also, assist with reviewing, refactoring and integrating source code for quality control, in addition to data analyses", "Design and build production-ready applications to reliably process batch and streaming data in a multi-tenancy cloud data platform and provide insights back to business systems based on their needs", "Design and implement core platform functions, tools, and processes using Agile methodology to integrate data from multiple sources and support use cases, ensuring standardization and reusability of work products", "Collaborate with the Group Architecture team to enhance the data platform capabilities, proposing updates to the solution architecture that utilize cloud platform best practices", "Determine the tools and technologies to be used on the Data Platform and investigate new technologies to identify potential benefits and improvements"], "Benefits": ["Paid Time Off: 3-4 weeks of accrued vacation time per year (3 weeks for support staff and 4 weeks for administrative/professional staff), 12 accrued sick days per year, 12.5 holidays plus a Winter Recess in December/January, 3 personal days per year (prorated based on date of hire), and up to 12 weeks of paid leave for new parents who are primary care givers", "Comprehensive medical, dental, and vision benefits, disability and life insurance programs, along with voluntary benefits", "Child and elder/adult care resources including on campus childcare centers, Employee Assistance Program, and wellness programs related to stress management, nutrition, meditation, and more", "Retirement: University-funded retirement plan with contributions from 5% to 15% of eligible compensation, based on age and earnings with full vesting after 3 years of service", "Tuition Assistance Program: Competitive program including $40 per class at the Harvard Extension School and reduced tuition through other participating Harvard graduate schools", "Tuition Reimbursement: Program that provides 75% to 90% reimbursement up to $5,250 per calendar year for eligible courses taken at other accredited institutions", "Programs and classes at little or no cost, including through the Harvard Center for Workplace Development and LinkedIn Learning", "Commuting and Transportation: Various commuter options handled through the Parking Office, including discounted parking, half-priced public transportation passes and pre-tax transit passes, biking benefits, and more", "Harvard Facilities Access, Discounts and Perks: Access to Harvard athletic and fitness facilities, libraries, campus events, credit union, and more, as well as discounts to various types of services (legal, financial, etc.)"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "61", "job_naics_name": "Education"}, {"employer_name": "Presidio, Inc.", "employer_logo": "https://www.presidio.com/icheestu/2021/02/Presidio-blue-logo.svg", "employer_website": "http://www.presidio.com", "employer_company_type": "Computer Services", "job_publisher": "Snagajob", "job_id": "uhc0PfY5CbxufgPvAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer - Now Hiring", "job_apply_link": "https://www.snagajob.com/jobs/888113531", "job_apply_is_direct": false, "job_apply_quality_score": 0.6247, "apply_options": [{"publisher": "Snagajob", "apply_link": "https://www.snagajob.com/jobs/888113531", "is_direct": false}], "job_description": "Description\n\nSEIZE THE OPPORTUNITY TO BE A PART OF SOMETHING GREAT!\n\nPresidio is on the leading edge of a technology-driven movement to transform the way business is done, for our customers and our customers' customers. Joining Presidio means immersing yourself in a culture of self-starters, collaborators and innovators who make real, lasting change in the marketplace via cutting-edge technology and business solutions. At Presidio, we know that it's our people that make the connections happen.\n\nWHY YOU SHOULD JOIN US? You will set your career on track for outstanding achievement with a company that knows no limits. Presidio is a leading a global digital services and solutions provider focused on Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions.\n\nTHE ROLE: Data Engineer\n\nJob Summary:\n\nAs a Data Engineer, you will assist in the cloud data science practice data pipeline management and development. The Cloud Data Science practice is rapidly growing and looking to add a qualified experienced Data Engineer to assist in developing new ETL pipelines and reporting.\n\nTravel Requirements:\n\nThis position does not require any travel.\n\nJob Responsibilities:\n\u2022 Participate on data engineering projects on AWS, Azure, GCP, but Focused on AWS\n\u2022 Collaborate with Data Scientists for Machine Learning/AI Projects\n\u2022 Maintain and Improve Current Data Pipelines\n\u2022 Build ETL jobs per architectural guidelines which integrate within the data pipeline.\n\u2022 Troubleshoot and concerns with our data pipelines.\n\u2022 Identify and implement BI tools per Architectural guidelines.\n\u2022 Ability to Articulate a problem and find solutions in a timely manner.\n\nRequired Skills:\n\u2022 Core understanding of Big Data principles and solutions\n\u2022 Deep understanding of real-time and batch-processing pipelines\n\u2022 Understanding of AWS, Azure, and GCP data Architectures\n\u2022 Experience with Tableau, Quicksight, Domo, Lookr, PowerBI or similar Business Intelligence platform\n\u2022 Experience with Apache Spark is not required, but a significant plus.\n\u2022 Experience with CI/CD via git is not required, but a plus.\n\u2022 Must have experience with git interface.\n\u2022 Must understand how to incorporate Data Lake architecture.\n\u2022 Must know how to work with NoSQL databases such as DynanmoDB.\n\u2022 Highly Proficient in SQL a must\n\u2022 Highly Proficient in Python a must\n\nAdditional Desired Experience:\n\u2022 Experience with workflow applications highly sought after, such as AWS Step functions, Airflow, Prefect, Dagster or a similar workflow application\n\u2022 Experience with Infrastructure as Code (IaC) highly sought after, such as Terraform, AWS Cloud Formation or similar tool.\n\u2022 Experience with or Understanding of containers such as Docker and/or Kubernetes is a must.\n\nEducation and Experience:\n\u2022 Bachelors Degree or equivalent experience and/or military experience in computer science, mathematics, statistics or a related field\n\u2022 3+ Year Data Engineering Experience or equivalent training/education\n\u2022 ****\n\nABOUT PRESIDIO\n\nPresidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DEI change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.\n\nPresidio is a global digital services and solutions provider accelerating business transformation through secured technology modernization. Highly skilled teams of engineers and solutions architects with deep expertise across cloud, security, networking and modern data center infrastructure help customers acquire, deploy and operate technology that delivers impactful business outcomes. Presidio is a trusted strategic advisor with a flexible full life cycle model of professional, managed, and support and staffing services to help execute, secure, operationalize and maintain technology solutions. We serve as an extension of our clients' IT teams, providing deep expertise and letting them focus on their core business. Presidio operates in 40+ US offices and offices in Ireland, London, Singapore, and India.\n\nFor more information visit: http://presidio.com\n\u2022 ****\n\nPresidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.\n\nTo read more about discrimination protections under Federal Law, please visit: https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf\n\nIf you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.\n\nPresidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.\n\nRECRUITMENT AGENCIES PLEASE NOTE:\n\nAgencies/3 Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3 Party will be considered a gift and property of Presidio, unless the Agency/3 Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3 Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.\n\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities\n\nThe contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR 60-1.35(c)", "job_is_remote": false, "job_posted_at_timestamp": 1703143084, "job_posted_at_datetime_utc": "2023-12-21T07:18:04.000Z", "job_city": "Hauppauge", "job_state": "NY", "job_country": "US", "job_latitude": 40.825653, "job_longitude": -73.202614, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=uhc0PfY5CbxufgPvAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-02-03T07:17:44.000Z", "job_offer_expiration_timestamp": 1706944664, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 39.83, "job_max_salary": 69.01, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Core understanding of Big Data principles and solutions", "Deep understanding of real-time and batch-processing pipelines", "Understanding of AWS, Azure, and GCP data Architectures", "Experience with Tableau, Quicksight, Domo, Lookr, PowerBI or similar Business Intelligence platform", "Experience with Apache Spark is not required, but a significant plus", "Must have experience with git interface", "Must understand how to incorporate Data Lake architecture", "Must know how to work with NoSQL databases such as DynanmoDB", "Highly Proficient in SQL a must", "Highly Proficient in Python a must", "Bachelors Degree or equivalent experience and/or military experience in computer science, mathematics, statistics or a related field", "3+ Year Data Engineering Experience or equivalent training/education"], "Responsibilities": ["As a Data Engineer, you will assist in the cloud data science practice data pipeline management and development", "The Cloud Data Science practice is rapidly growing and looking to add a qualified experienced Data Engineer to assist in developing new ETL pipelines and reporting", "Participate on data engineering projects on AWS, Azure, GCP, but Focused on AWS", "Collaborate with Data Scientists for Machine Learning/AI Projects", "Maintain and Improve Current Data Pipelines", "Build ETL jobs per architectural guidelines which integrate within the data pipeline", "Troubleshoot and concerns with our data pipelines", "Identify and implement BI tools per Architectural guidelines", "Ability to Articulate a problem and find solutions in a timely manner"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "5415", "job_naics_name": "Computer Services"}, {"employer_name": "PayPal", "employer_logo": "https://www.paypalobjects.com/webstatic/i/logo/rebrand/ppcom.png", "employer_website": "http://www.paypal.com", "employer_company_type": null, "job_publisher": "Austin, TX - Geebo", "job_id": "1cuiQ08dkGzS1pWmAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer (Jr And Mid Level) at PayPal in Austin, TX", "job_apply_link": "https://austin-tx.geebo.com/jobs-online/view/id/1081966754-data-engineer-jr-and-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4362, "apply_options": [{"publisher": "Austin, TX - Geebo", "apply_link": "https://austin-tx.geebo.com/jobs-online/view/id/1081966754-data-engineer-jr-and-/", "is_direct": true}], "job_description": "Job Description At PayPal (NASDAQ:\nPYPL), we believe that every person has the right to participate fully in the global economy. Our mission is to democratize financial services to ensure that everyone, regardless of background or economic standing, has access to affordable, convenient, and secure products and services to take control of their financial lives. We're a purpose-driven company, and our beliefs are the foundation of how we conduct business every day. We're guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Collectively, these values inspire us to work together as One Team with our customers at the center of everything we do, and to take care of ourselves, each other, and the communities in which we live and work. We challenge the status quo, ask questions, and find solutions. Join us as we enable the hopes, dreams, and ambitions of millions of people around the world. The candidate will be part of PayPal Data Engineering Team. The candidate will be responsible for analysis, design, coding, and testing of the application code. The candidate has to interact with global cross-functional teams to address complex business requirements. The role demands the individual to possess technical skills required to perform the job in an effective manner. The individual should be self-motivated, possess creative problem-solving skills and have the ability to handle multiple projects at the same time. The candidate should have passion for data & analytics.\nResponsibilities:\nIn this role, the individual will be part of the Data engineering team and will be responsible for. o Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale. o Planning the execution of the project in an effective and efficient manner. o Approaching the problem, taking into account all possibilities. o Creativity and out of the box thinking is required. o Proactively anticipating problems and appropriately communicating to the team and management in a timely manner. o Being flexible and being able to support all functions of product life cycle when required. o Ability to work in a fast-paced environment Hiring Jr and Mid-Level DATA ENGINEERS! Location:\nAustin, TX Come Join Us! Required Skills:\n3-7 years of experience in the IT industry, experience in Data Technology space is preferred. Advanced Shell or Perl scripting experience or proficiency in any programming language like Java, Python, Scala, C+\nand Spark Working experience in any MPP systems, should have strong SQL programming skills Knowledge of data warehousing concepts Working knowledge on Big Data, Streaming Integrations Excellent written and oral communication skills Strong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusions Expertise in database programming and performance tuning techniques Familiar with data movement techniques and best practices to handle large volumes of data Experience with data warehousing architecture and data modeling best practices Experience with File Systems, server architectures, and distributed systems Strong communication skills and willingness to take initiative to contribute beyond basic responsibilities Working experience in an Agile methodology is highly preferred Knowledge of Hadoop, Spark, HBase and Hive is highly preferred Knowledge and Working experience on Cloud platforms is highly preferred For more than 20 years, PayPal has remained at the forefront of the digital payment revolution. By leveraging technology to make financial services and commerce more convenient, affordable, and secure, our open digital payments platform gives PayPal's 400 million active account holders the confidence to connect and transact in new and powerful ways. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying, or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Austin", "job_state": "TX", "job_country": "US", "job_latitude": 30.267153, "job_longitude": -97.74306, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=1cuiQ08dkGzS1pWmAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "Vanguard", "employer_logo": "https://upload.wikimedia.org/wikipedia/fr/thumb/8/82/The_Vanguard_Group_-_Logo.png/640px-The_Vanguard_Group_-_Logo.png", "employer_website": "http://www.vanguard.com", "employer_company_type": "Finance", "job_publisher": "Jobs Trabajo.org", "job_id": "GVnbaZxyZ-x-qxbeAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer, Data Analytics Engineering", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-13d85ac5d7d681da8b70186e676e3dd6", "job_apply_is_direct": false, "job_apply_quality_score": 0.4592, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-13d85ac5d7d681da8b70186e676e3dd6", "is_direct": false}], "job_description": "Are you looking for an exciting opportunity to work as a Data Engineer and support mission critical functionality? We are looking for someone to join our team who is passionate about Data Engineering and empowering our business with data and insights.\nAs part of the engineering team, you will build solutions utilizing Glue ETL, RedShift, Lambda, SageMaker and other AWS services. Most of our coding is done in Python and Pyspark.\nPython\n+ SQL\n+ Minimum of three years data analytics, programming, database administration, or data management experience.\n+ Undergraduate degree or equivalent combination of training and experience.\nBecause when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. Inclusion Statement\nVanguard\u2019s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: \u201cDo the right thing.\u201d\nWe believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard\u2019s core purpose through our values.\nWhen all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.\nTo take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.\nVanguard has implemented a hybrid working model for the majority of our crew members, designed to capture the benefits of enhanced flexibility while enabling in-person learning, collaboration, and connection. We believe our mission-driven and highly collaborative culture is a critical enabler to support long-term client outcomes and enrich the employee experience.\nVanguard, one of the world's largest investment management companies, serves individual investors, institutions, employer-sponsored retirement plans, and financial professionals. We have a diverse and talented crew with a culture that promotes teamwork, along with an unwavering focus on serving our clients' best interests.\nThis website uses \"cookies\" to distinguish you from other users. A cookie is a small file of letters and numbers placed on your computer or device. This helps us to provide you with a good experience when you browse our website and also allows us to improve our site and services. The cookies are stored locally on your computer or mobile device. Or you can go to our Privacy Policy (anguardjobs.com/site-privacy-policy/) to read more information and learn how to change your preferences.", "job_is_remote": false, "job_posted_at_timestamp": 1703129632, "job_posted_at_datetime_utc": "2023-12-21T03:33:52.000Z", "job_city": "Malvern", "job_state": "PA", "job_country": "US", "job_latitude": 40.036217, "job_longitude": -75.51381, "job_benefits": ["retirement_savings"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=GVnbaZxyZ-x-qxbeAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Minimum of three years data analytics, programming, database administration, or data management experience", "Undergraduate degree or equivalent combination of training and experience"], "Responsibilities": ["As part of the engineering team, you will build solutions utilizing Glue ETL, RedShift, Lambda, SageMaker and other AWS services"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "43911100", "job_onet_job_zone": "4", "job_naics_code": "523920", "job_naics_name": "Portfolio Management"}, {"employer_name": "Flexon Technologies", "employer_logo": null, "employer_website": "https://flexontech.ca", "employer_company_type": null, "job_publisher": "ZipRecruiter", "job_id": "RqoNOVRRGuqVxxNQAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "JPC - 286 - GCP Data engineer with Dataproc + Big Table", "job_apply_link": "https://www.ziprecruiter.com/c/Flexon-Technologies/Job/JPC-286-GCP-Data-engineer-with-Dataproc-+-Big-Table/-in-New-York,NY?jid=be3960a5f6f9e38b", "job_apply_is_direct": false, "job_apply_quality_score": 0.6963, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Flexon-Technologies/Job/JPC-286-GCP-Data-engineer-with-Dataproc-+-Big-Table/-in-New-York,NY?jid=be3960a5f6f9e38b", "is_direct": false}], "job_description": "Overview\n\nPosition: GCP Data engineer with Dataproc + Big Table\n\nLocation: Anywhere in US & Canada\n\nCertifications:\n\nGCP Cloud Certification is Must\n\nGoogle Cloud Certified Professional Data Engineer\n\nGoogle Cloud Certified Professional Cloud Architect\n\nTechnical Skills:\n\u2022 Experience with private, hybrid or public cloud technology\n\u2022 Possess Strong Cloud Implementation Experience in Cloud architecture/design, compute/storage services.\n\u2022 Understand multi-Cloud/multi-zone based designs.\n\u2022 Compute: Docker Apps, Lambda/ serverless, UserData customization\n\u2022 Preferred: Cluster/HA based parallel design/multi-region approach\n\u2022 Cloud automation: Ansible, YAML, CloudFormation\n\u2022 Containers: Docker; Preferred: Container orchestration - Mesosphere, Kubernetes, ECS\n\u2022 Familiarity with functional operations of server, storage, and network functions.\n\u2022 Experience migrating Windows or Linux\n\u2022 Virtualization experience (VMware, Xen, HyperV)\n\u2022 Databases understanding: Oracle, DB2,MySql; Preferred: NoSql DB, MongoDB, Cassandra, DynamoDB\n\u2022 Program Management experience from Planning to execution\n\u2022 Having experience of Managing teams of moderate size\n\u2022 Strong communications skill and multiple stakeholder's management\n\u2022 Experience in implementing cloud solutions.\n\nResponsibilities:\n\u2022 Working with the GCP team for various GCP Cloud programs including Certifications, Partner engineering, Marketplace, and other Cloud platform solutions.\n\u2022 Educating GCP / ISVs customers on best practices\n\u2022 Stakeholders management / Communication: Google, ISVs, Project Teams\n\u2022 Program Management and Execution\n\u2022 Managing the teams on different projects\n\u2022 Reporting and Project Communications\n\u2022 Advocating the customer's perspective during product and architecture planning.\n\u2022 Create and maintain a library of reusable container images\nAwareness of Business P&L and Goals aligned to technology and revenue\n\nhttps://www.flexontechnologies.com/copy-of-flex-ehr-3\n\nFlexon Technologies is a leading end-to-end technology solutions provider to IT and Non-IT industry. We specialize in providing \"Total Solutions\" encompassing technology and services combined with unparalleled domain knowledge that gives our clients a distinct advantage.\n\nWe offer integrated business solutions, enabling clients to optimize their business with greater efficiency, and increased responsiveness. Our offerings are designed to cater to the entire range of clients' technology needs. We deliver end-to-end solutions that can build, manage and support our customers' IT systems across the entire value chain infrastructure, applications and business processes. The range of our offerings extends to software (including systems and application software development, implementation, maintenance and frameworks), IT architecture, network consulting, Staffing etc. These technology offerings backed by the domain solutions and knowledge to ensure maximum business alignment, allowing you to derive maximum benefits out of the IT investments.", "job_is_remote": false, "job_posted_at_timestamp": 1703095200, "job_posted_at_datetime_utc": "2023-12-20T18:00:00.000Z", "job_city": "New York", "job_state": "NY", "job_country": "US", "job_latitude": 40.712776, "job_longitude": -74.005974, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=RqoNOVRRGuqVxxNQAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-20T00:00:00.000Z", "job_offer_expiration_timestamp": 1705708800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["GCP Cloud Certification is Must", "Experience with private, hybrid or public cloud technology", "Possess Strong Cloud Implementation Experience in Cloud architecture/design, compute/storage services", "Understand multi-Cloud/multi-zone based designs", "Compute: Docker Apps, Lambda/ serverless, UserData customization", "Cloud automation: Ansible, YAML, CloudFormation", "Familiarity with functional operations of server, storage, and network functions", "Experience migrating Windows or Linux", "Virtualization experience (VMware, Xen, HyperV)", "Program Management experience from Planning to execution", "Having experience of Managing teams of moderate size", "Strong communications skill and multiple stakeholder's management", "Experience in implementing cloud solutions"], "Responsibilities": ["Working with the GCP team for various GCP Cloud programs including Certifications, Partner engineering, Marketplace, and other Cloud platform solutions", "Educating GCP / ISVs customers on best practices", "Stakeholders management / Communication: Google, ISVs, Project Teams", "Program Management and Execution", "Managing the teams on different projects", "Reporting and Project Communications", "Advocating the customer's perspective during product and architecture planning", "Create and maintain a library of reusable container images"]}, "job_job_title": null, "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_occupational_categories": ["15-1143.00: Computer Network Architects"]}, {"employer_name": "Children's Hospital Los Angeles", "employer_logo": null, "employer_website": null, "employer_company_type": null, "job_publisher": "Geebo", "job_id": "ITy_3qc5S4dENaPmAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer at Children's Hospital Los Angeles in Glendale, CA", "job_apply_link": "https://geebo.com/jobs-online/view/id/1062594853-data-engineer-at-children-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4506, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1062594853-data-engineer-at-children-/", "is_direct": true}], "job_description": "Job Description NATIONAL LEADERS IN PEDIATRIC CARE Ranked No. 5 in the nation by U.S. News & World Report, Children's Hospital Los Angeles (CHLA) provides the best care in California. Here world-class experts in medicine, education and research work together to deliver family-centered care half a million times each year. From primary to complex critical care, more than 350 programs and services are offered, each one specially designed for children. The CHLA of the future is brighter than can be imagined. Investments in technology, research and innovation will create care that is personal, convenient and empowering. Our scientists will work with clinical experts to take laboratory discoveries and create treatments that are a perfect match for every patient. And together, CHLA team members will turn health care into health transformation. Join a hospital where the work you do will matter--to you, to your colleagues, and above all, to our patients and families. The work will be challenging, but always rewarding. It's Work That Matters. Overview Data Engineer I is responsible for administrating, managing, and scaling the Enterprise Data Lake (EDL), expanding the EDL to utilize Cloud resources, and supporting data pipelines, products, and visualization. Minimum Qualifications/Work\nExperience:\n3\nyears' data engineer experience in various big data and data lake technologies, e.g., Cloudera, Hortonworks, Databricks, etc. Proficiency in SQL, Linux, shell-script programming, Spark, Python and Tableau Experience with AD, Hadoop authentication/authorization and security frameworks is preferred Education/Licensure/Certification:\nBachelor's Degree in computer science, Information Systems, or equivalent. Graduate Degree preferred.\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine Learning, Systems Architecture & EngineeringEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Glendale", "job_state": "CA", "job_country": "US", "job_latitude": 34.14251, "job_longitude": -118.25507, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=ITy_3qc5S4dENaPmAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["years' data engineer experience in various big data and data lake technologies, e.g., Cloudera, Hortonworks, Databricks, etc", "Bachelor's Degree in computer science, Information Systems, or equivalent"], "Responsibilities": ["The work will be challenging, but always rewarding", "Overview Data Engineer I is responsible for administrating, managing, and scaling the Enterprise Data Lake (EDL), expanding the EDL to utilize Cloud resources, and supporting data pipelines, products, and visualization"], "Benefits": ["$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "Diverse Lynx", "employer_logo": "https://c.smartrecruiters.com/sr-company-images-prod-aws-dc5/5525418ee4b08c20c6910cd4/default_social_logo/300x300?r=s3-eu-central-1&_1534874151146", "employer_website": "http://www.diverselynx.com", "employer_company_type": null, "job_publisher": "Jobs Trabajo.org", "job_id": "31FcQMXuMzGHU6IeAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer - Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-5039c2fe6cf05ffe8a902a285fc20c79", "job_apply_is_direct": false, "job_apply_quality_score": 0.4424, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-5039c2fe6cf05ffe8a902a285fc20c79", "is_direct": false}], "job_description": "Job Title: Data Engineer\nLocation: Bay Area, CA(Sunnyvale)\nDuration: Contract / FullTime Position\n\nJob Description:\nWe're seeking data engineers to work on scalable data workflows to enrich knowledge graph. As an expert in developing software to manage large, dynamic datasets, you'll be building and optimizing pipelines for data ingestion, cleaning, transformation and evaluation to support a rapidly scaling organization.\n\nKey Qualifications\n~2+ years of experience as a Software Engineer processing large-scale datasets\n~ Excellent programming skills - e.g. Python, Go, Java, Scala.\n~ Excellent problem-solving, analytic, and debugging skills\n~ Solid computer science and systems foundations; ability to quickly learn new domains\n~ Proven software development skills in UNIX-type OS (e.g. Linux, Mac OS)\n~ Experience working with large data sets and pipelines, ideally using the Apache software stack (e.g. Spark, HBase)\n~ Experience with continuous integration and continuous development solutions (e.g. Jenkins, etc.)\n~ Experience with cloud-native deployment is a good plus (e.g. Kubernetes)\n~ Good communication skills and teamwork\n~ Passion for building great products\n~ Experience in tooling and streamlining workflows in complex processes\n\nDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.\n#J-18808-Ljbffr", "job_is_remote": false, "job_posted_at_timestamp": 1703117566, "job_posted_at_datetime_utc": "2023-12-21T00:12:46.000Z", "job_city": "San Francisco", "job_state": "CA", "job_country": "US", "job_latitude": 37.77493, "job_longitude": -122.41942, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=31FcQMXuMzGHU6IeAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["~2+ years of experience as a Software Engineer processing large-scale datasets", "~ Excellent programming skills - e.g. Python, Go, Java, Scala", "~ Excellent problem-solving, analytic, and debugging skills", "~ Solid computer science and systems foundations; ability to quickly learn new domains", "~ Proven software development skills in UNIX-type OS (e.g. Linux, Mac OS)", "~ Experience working with large data sets and pipelines, ideally using the Apache software stack (e.g", "Spark, HBase)", "~ Experience with continuous integration and continuous development solutions (e.g", "Jenkins, etc.)", "~ Experience with cloud-native deployment is a good plus (e.g", "Kubernetes)", "~ Good communication skills and teamwork", "~ Passion for building great products", "~ Experience in tooling and streamlining workflows in complex processes"], "Responsibilities": ["We're seeking data engineers to work on scalable data workflows to enrich knowledge graph", "As an expert in developing software to manage large, dynamic datasets, you'll be building and optimizing pipelines for data ingestion, cleaning, transformation and evaluation to support a rapidly scaling organization"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "PACCAR", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/PACCAR-logo.svg/2560px-PACCAR-logo.svg.png", "employer_website": "http://www.paccar.com", "employer_company_type": "Manufacturing", "job_publisher": "Talent.com", "job_id": "w1LPJcjUfwL_UkkwAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "It intern summer 2024", "job_apply_link": "https://www.talent.com/view?id=39c5d578f7a8", "job_apply_is_direct": false, "job_apply_quality_score": 0.5973, "apply_options": [{"publisher": "Talent.com", "apply_link": "https://www.talent.com/view?id=39c5d578f7a8", "is_direct": false}, {"publisher": "Washington Jobs - Tarta.ai", "apply_link": "https://washington.tarta.ai/j/wOl8b4wBcRajQcTjXqUB1223-it-intern-summer-2024-in-renton-washington-at-paccar", "is_direct": false}, {"publisher": "Jobilize", "apply_link": "https://www.jobilize.com/job/us-wa-renton-it-intern-summer-2024-project-management-job-paccar-winch", "is_direct": false}], "job_description": "About PACCAR\n\nPACCAR, Inc. is a Fortune 500 company established in 1905 and is recognized as a global leader in the commercial vehicle, financial, and customer service fields.\n\nPACCAR is a global technology leader in the design, manufacture, and customer support of high-quality light-, medium- and heavy-duty trucks under its internationally recognized brands Kenworth, Peterbilt and DAF nameplates.\n\nPACCAR designs and manufactures advanced diesel engines and provides customized financial services, information technology and truck parts related to its principal business.\n\nYour next career move is waiting at PACCAR whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services.\n\nJoin the PACCAR team today!\n\nDivision Information\n\nPACCAR's Information Division (ITD), located in Renton, WA, utilizes cutting-edge technology to provide systems development, consulting, voice and data communications services to the entire corporation, which has high visibility in the technology sector.\n\nThe Role\n\nDoes empowering teams to makedatadriven decisions excite you? Do you wake up in the morning wondering what possibilities could be unlocked with moredata?\n\nPACCAR is looking for adataengineer to join the team.DataEngineering focuses on making possible fast, accurate, and reliable access todata.\n\nWe builddatapipelines, manage adatalake, and support the production use of ourdata. We advocate for leading datapractices and make sure that our business users can makedatadriven decisions.\n\nAbout the Team\n\nAs a Data Engineer Intern, you will work with a highly celebrative team of data engineers with different backgrounds. This data engineering team focuses on solving PACCAR's most important challenges by making data available to our data science and analytics teams.\n\nAs part of a diversified global Fortune 500 company, the data engineering team works on a huge variety of projects, from building data ingestion pipelines to data quality products, data integration services and data platforms.\n\nCome join this dynamic, growing, and pioneering team!\n\nJob Responsibilities\n\u2022 Build and support automateddatapipelines from a wide range ofdatasource\n\u2022 Working on ways to automated and improve development and release processes\n\u2022 Develop solutions to measure, improve, and monitordataquality based on business requirements.\n\u2022 Proactively support product health by building solutions that are automated, scalable, and sustainable - be relentlessly focused on minimizing defects and technical debt\n\nRequired Experience & Skills\n\u2022 Basic proficiency with a dialect of ANSI SQL and Python\n\u2022 Working knowledge of RDBMS databases such as MS SQL Server, Oracle, PostgreSQL preferred.\n\u2022 Experience with participating in projects in a highly collaborative, multi-discipline team environment.\n\nEducation\n\nBachelors' degree in Computer Science, Informatics, or a related field required\n\nBenefits of working at PACCAR\n\nAs a U.S. PACCAR intern, you have a full range of benefit options including :\n\u2022 401k with up to a 5% company match\n\u2022 Sick Leave\n\u2022 Medical, dental, and vision plans for you and your family\n\u2022 Flexible spending accounts (FSA) and health savings account (HSA)\n\u2022 Life and accidental death and dismemberment insurance\n\u2022 EAP services including wellness plans, estate planning, financial counseling and more\n\u2022 Global Fortune 500 company with a wide array of growth, training, and development opportunities\n\u2022 Work alongside experienced goal-oriented colleagues recognized as experts in their field\n\nDiversity & Inclusion\n\nPACCAR has success with diverse teams of employees working together to achieve excellent results. Having a diverse and inclusive work environment ensures PACCAR has the talent needed to conduct business today and in the future by leveraging different backgrounds, skills, and viewpoints.\n\nPACCAR increases awareness through the efforts of global company-wide Diversity Councils and various employee resource groups that support initiatives & activities including multi-cultural events, outreach, mentorship programs, and more.\n\nPACCAR has been awarded the Top Company for Women to Work for in Transportation since 2018 by the Women in Trucking Association and Newsweek 2023 America's Greatest Workplaces for Women.\n\nFor more information, please visit our Diversity and Inclusion Commitment page : Diversity and Inclusion (paccar.com)\n\nAdditional Information\n\nPACCAR is an Equal Opportunity Employer / Protected Veteran / Disability. At PACCAR, we value talent and promote growth and development.\n\nWe carefully consider numerous compensation factors including your education, training, or experience. If hired, you will be required to provide proof of authorization to work in the United States.\n\nApplicants and employees for this position will not be sponsored for work authorization, including, but not limited to H-1B visas, now or in the future.\n\nThe salaries for intern positions are as follows :\n\u2022 Undergraduate Enrollment - $25 / hour\n\u2022 Graduate Enrollment - $30 / hour\n\nLast updated : 2023-12-21", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Renton", "job_state": "WA", "job_country": "US", "job_latitude": 47.479694, "job_longitude": -122.207924, "job_benefits": ["health_insurance", "dental_coverage", "retirement_savings"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=w1LPJcjUfwL_UkkwAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-03-10T00:00:00.000Z", "job_offer_expiration_timestamp": 1710028800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 30, "job_max_salary": 30, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Basic proficiency with a dialect of ANSI SQL and Python", "Experience with participating in projects in a highly collaborative, multi-discipline team environment", "Bachelors' degree in Computer Science, Informatics, or a related field required", "If hired, you will be required to provide proof of authorization to work in the United States"], "Responsibilities": ["Build and support automateddatapipelines from a wide range ofdatasource", "Working on ways to automated and improve development and release processes", "Develop solutions to measure, improve, and monitordataquality based on business requirements", "Proactively support product health by building solutions that are automated, scalable, and sustainable - be relentlessly focused on minimizing defects and technical debt"], "Benefits": ["As a U.S. PACCAR intern, you have a full range of benefit options including :", "401k with up to a 5% company match", "Sick Leave", "Medical, dental, and vision plans for you and your family", "Flexible spending accounts (FSA) and health savings account (HSA)", "Life and accidental death and dismemberment insurance", "EAP services including wellness plans, estate planning, financial counseling and more", "Global Fortune 500 company with a wide array of growth, training, and development opportunities", "Undergraduate Enrollment - $25 / hour", "Graduate Enrollment - $30 / hour"]}, "job_job_title": null, "job_posting_language": "en", "job_onet_soc": "11103100", "job_onet_job_zone": "4", "job_occupational_categories": ["41-9031.00"], "job_naics_code": "336111", "job_naics_name": "Automobile Manufacturing"}, {"employer_name": "NATIONAL GRID CO USA (NE POWER)", "employer_logo": null, "employer_website": null, "employer_company_type": null, "job_publisher": "Whitman, MA - Geebo", "job_id": "-NVAzz40Cq-7KyQWAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Senior Data Engineer", "job_apply_link": "https://whitman-ma.geebo.com/jobs-online/view/id/862783738-senior-data-engineer-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4311, "apply_options": [{"publisher": "Whitman, MA - Geebo", "apply_link": "https://whitman-ma.geebo.com/jobs-online/view/id/862783738-senior-data-engineer-/", "is_direct": true}], "job_description": "About usEvery day we deliver safe and secure energy to homes, communities, and businesses.\nWe are there when people need us the most.\nWe connect people to the energy they need for the lives they live.\nThe pace of change in society and our industry is accelerating and our expertise and track record puts us in an unparalleled position to shape the sustainable future of our industry.\nTo be successful we must anticipate the needs of our customers, reducing the cost of energy delivery today and pioneering the flexible energy systems of tomorrow.\nThis requires us to deliver on our promises and always look for new opportunities to grow, both ourselves and our business.\nNational Grid is hiring a Senior Data Engineer for our Waltham, MA.\nJob PurposeNational Grid is a gas and electric utility that brings energy to life for 7 million customers across Massachusetts, New York, and Rhode Island through a network of 100,000\nmiles of wires and pipes.\nThe company is aggressively transforming into a 21st century data-driven energy manager for its customers.\nWe are building a team that will be responsible for Data Engineering and MLOps.\nThe team will develop data pipelines, take data science prototype models to production, monitor operations and standup the necessary infrastructure in Azure to perform these functions.\nRead on if you are excited about using best-in-class tools and methodologies to help shape the future of Data Engineering and MLOps at National Grid.\nKey AccountabilitiesWork with data scientists to specify data requirements for data science projects.\nWork with data owners to understand the data schema, the nature of data flow within the enterprise, and develop data dictionaries.\nDevelop a deep understanding of the ingested data from a business perspective.\nDesign robust, scalable, and maintainable data ingestion ETL/ELT pipelines.\nChoose appropriate data models and ensure that ingested data is high quality.\nWrite high quality code that has high test coverage.\nParticipate in code reviews to help improve code quality.\nSupervisory/Interpersonal- Experience RequiredExperience in project management and stakeholder-based delivery.\nQualificationsBachelor's degree in a computer science or another quantitative field.\n5\nyears of experience in a Data Engineer role Minimum of 3-5 years' experience in advanced data processing and software development is desired.\nExperience in deploying pipelines using Airflow.\nProficient/expert level hands-on knowledge of shell scripting, Python, SQL, Azure, and AWS.\nExperience in extracting data using REST APIs and from databases (Oracle, MSSQL)Ideally, experience in administering Windows and/or Linux servers.\nBe intellectually curious and enjoy learningMore InformationThis position has a career path which provides for advancement opportunities within and across bands as you develop and evolve in the position; gaining experience, expertise and acquiring and applying technical skills.\nInternal candidates will be assessed and provided offers against the minimum qualifications of this role and their individual experience.\nNational Grid is an equal opportunity employer that values a broad diversity of talent, knowledge, experience and expertise.\nWe foster a culture of inclusion that drives employee engagement to deliver superior performance to the communities we serve.\nNational Grid is proud to be an affirmative action employer.\nWe encourage minorities, women, individuals with disabilities and protected veterans to join the National Grid team.\n.\nEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Whitman", "job_state": "MA", "job_country": "US", "job_latitude": 42.080658, "job_longitude": -70.9356, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=-NVAzz40Cq-7KyQWAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Key AccountabilitiesWork with data scientists to specify data requirements for data science projects", "QualificationsBachelor's degree in a computer science or another quantitative field", "Experience in deploying pipelines using Airflow", "Proficient/expert level hands-on knowledge of shell scripting, Python, SQL, Azure, and AWS", "Experience in extracting data using REST APIs and from databases (Oracle, MSSQL)Ideally, experience in administering Windows and/or Linux servers", "Be intellectually curious and enjoy learning"], "Responsibilities": ["The team will develop data pipelines, take data science prototype models to production, monitor operations and standup the necessary infrastructure in Azure to perform these functions", "Read on if you are excited about using best-in-class tools and methodologies to help shape the future of Data Engineering and MLOps at National Grid", "Work with data owners to understand the data schema, the nature of data flow within the enterprise, and develop data dictionaries", "Develop a deep understanding of the ingested data from a business perspective", "Design robust, scalable, and maintainable data ingestion ETL/ELT pipelines", "Choose appropriate data models and ensure that ingested data is high quality", "Write high quality code that has high test coverage", "Participate in code reviews to help improve code quality", "Supervisory/Interpersonal- Experience RequiredExperience in project management and stakeholder-based delivery"], "Benefits": ["Estimated Salary: $20 to $28 per hour based on qualifications"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4"}, {"employer_name": "Oracle", "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png", "employer_website": "http://www.oracle.com", "employer_company_type": "Information", "job_publisher": "Jobs Trabajo.org", "job_id": "Pd9uwY1qYydZtx3NAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Engineer - Data Engineer", "job_apply_link": "https://us.trabajo.org/job-1275-20231221-601e9e4c1df4dfdd3f93c84c5eec21ee", "job_apply_is_direct": false, "job_apply_quality_score": 0.4489, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-1275-20231221-601e9e4c1df4dfdd3f93c84c5eec21ee", "is_direct": false}], "job_description": "Must have the ability to obtain federal security clearance necessary for this role which requires being a US citizen**\nBuilding off our Cloud momentum, Oracle has formed a new organization - Oracle Health Applications & Infrastructure. This team will focus on product development and product strategy for Oracle Health while building out a complete platform supporting modernized, automated healthcare. This is a net new line of business, constructed with an entrepreneurial spirit that promotes an energetic and creative environment. We are unencumbered and will need your contribution to make it a world-class engineering center with a focus on excellence.Identify and help document for the Data Center Engineering knowledge base.\nAssist with process improvements and standard methodologies.\nEducation Qualifications: 4-year degree or higher education, computer science or technology, or equivalent work experience.\nReceipt of the appropriate government security clearance card applicable for your position.\nDue to the client contract you will be assigned, this position requires you to be a U.The night shifts are four 10 hour shifts from 7pm to 5am Monday evenings to Tuesday mornings thru Thursday evenings to Friday mornings. The Data Center Engineering team is a small, close-knit team that comes from a wide range of backgrounds and experiences. Oracle Health Mission Statement:\nOracle Health is putting humans at the heart of the conversation and what the healthcare experience needs to look like - for patients, providers, payers, and the population. This will allow a patient to get point of care from anyone, anywhere or any device by providing the practitioner medical information leveraging global data.\nLife at Oracle and Equal Opportunity\nAn Oracle career can span industries, roles, Countries and cultures, giving you the opportunity to flourish in new roles and innovate, while blending work life in. Oracle has thrived through 40+ years of change by innovating and operating with integrity while delivering for the top companies in almost every industry.\nOracle offers a highly competitive suite of Employee Benefits designed on the principles of parity, consistency, and affordability. The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more. We also encourage our employees to engage in the culture of giving back to the communities where we live and do business.\nAt Oracle, we believe that innovation starts with diversity and inclusion and to create the future we need talent from various backgrounds, perspectives, and abilities. We ensure that individuals with disabilities are provided reasonable accommodation to successfully participate in the job application, interview process, and in potential roles to perform crucial job functions.\nPerform performance trend analysis and manage the server/network capacity.\n+ React to potential problems using automation, scheduling, and monitoring tools -- escalating to management where appropriate.\n+ Participate in configuration and implement technical solutions to enhance and/or troubleshoot the system.\n+ Replacing failed server components, upgrading servers to meet the demands of applications, and proactively maintaining hardware to ensure efficient performance for its entire lifecycle.\n+ Responding to client emergencies in Oracle Health\u2019s data centers as the Data Center Engineering team is the very last line of defense for clients before the server hardware itself.\n+ The Data Center Engineering team is the bridge between Oracle Health Application & Infrastructure and third-party contractors and vendor companies who interact with the data centers.\n+ Maintaining a spare parts inventory with Data Center Capacity and Asset Management.\n08 per hour; Oracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle\u2019s differing products, industries and lines of business.\nOracle offers a comprehensive benefits package which includes the following:\nMedical, dental, and vision insurance, including expert medical opinion\nShort term disability and long term disability\nSupplemental life insurance (Employee/Spouse/Child)\nHealth care and dependent care Flexible Spending Accounts\nPre-tax commuter and parking benefits\nFlexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position. 11 paid holidays\nEmployee Stock Purchase Plan\nFinancial planning and group legal\nAn Oracle career can span industries, roles, Countries and cultures, giving you the opportunity to flourish in new roles and innovate, while blending work life in. Oracle has thrived through 40+ years of change by innovating and operating with integrity while delivering for the top companies in almost every industry.\nIn order to nurture the talent that makes this happen, we are committed to an inclusive culture that celebrates and values diverse insights and perspectives, a workforce that inspires thought leadership and innovation.\nOracle offers a highly competitive suite of Employee Benefits designed on the principles of parity, consistency, and affordability. The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more. We also encourage our employees to engage in the culture of giving back to the communities where we live and do business.\nAt Oracle, we believe that innovation starts with diversity and inclusion and to create the future we need talent from various backgrounds, perspectives, and abilities. We ensure that individuals with disabilities are provided reasonable accommodation to successfully participate in the job application, interview process, and in potential roles. Oracle is an Equal Employment Opportunity Employer . All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans\u2019 status, or any other characteristic protected by law. Oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law.\n\u2022 **", "job_is_remote": false, "job_posted_at_timestamp": 1703129811, "job_posted_at_datetime_utc": "2023-12-21T03:36:51.000Z", "job_city": "Raleigh", "job_state": "NC", "job_country": "US", "job_latitude": 35.77959, "job_longitude": -78.638176, "job_benefits": ["retirement_savings", "health_insurance", "dental_coverage"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Pd9uwY1qYydZtx3NAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Must have the ability to obtain federal security clearance necessary for this role which requires being a US citizen**", "Education Qualifications: 4-year degree or higher education, computer science or technology, or equivalent work experience"], "Responsibilities": ["Identify and help document for the Data Center Engineering knowledge base", "Assist with process improvements and standard methodologies", "This will allow a patient to get point of care from anyone, anywhere or any device by providing the practitioner medical information leveraging global data", "Perform performance trend analysis and manage the server/network capacity", "React to potential problems using automation, scheduling, and monitoring tools -- escalating to management where appropriate", "Participate in configuration and implement technical solutions to enhance and/or troubleshoot the system", "Replacing failed server components, upgrading servers to meet the demands of applications, and proactively maintaining hardware to ensure efficient performance for its entire lifecycle", "Responding to client emergencies in Oracle Health\u2019s data centers as the Data Center Engineering team is the very last line of defense for clients before the server hardware itself", "The Data Center Engineering team is the bridge between Oracle Health Application & Infrastructure and third-party contractors and vendor companies who interact with the data centers", "Maintaining a spare parts inventory with Data Center Capacity and Asset Management"], "Benefits": ["The overall package includes certain core elements such as Medical, Life Insurance, access to Retirement Planning, and much more", "Medical, dental, and vision insurance, including expert medical opinion", "Short term disability and long term disability", "Supplemental life insurance (Employee/Spouse/Child)", "Health care and dependent care Flexible Spending Accounts", "Pre-tax commuter and parking benefits", "Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position", "11 paid holidays", "Employee Stock Purchase Plan"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "511210", "job_naics_name": "Software Publishers"}, {"employer_name": "The Cigna Group", "employer_logo": "https://cigna.gg/static/www-cigna-gg/img/cigna-rebrand-logo-blue-with-green.png", "employer_website": "https://www.thecignagroup.com", "employer_company_type": "Finance", "job_publisher": "Cigna - The Cigna Group", "job_id": "CJdR7_aAck1-TIhMAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Senior AI Engineer - Hybrid", "job_apply_link": "https://jobs.thecignagroup.com/us/en/job/23009983/Senior-AI-Engineer-Hybrid", "job_apply_is_direct": false, "job_apply_quality_score": 0.9022, "apply_options": [{"publisher": "Cigna - The Cigna Group", "apply_link": "https://jobs.thecignagroup.com/us/en/job/23009983/Senior-AI-Engineer-Hybrid", "is_direct": false}, {"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/senior-ai-engineer-hybrid-at-the-cigna-group-3787352183", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=4075e2d110391dbd", "is_direct": false}, {"publisher": "National Labor Exchange Veterans Jobs", "apply_link": "https://veterans.usnlx.com/st-louis-mo/senior-ai-engineer-hybrid/EBED4B6A7FE248C1BDD228F64903044D/job/?vs=28", "is_direct": false}, {"publisher": "Cigna Jobs", "apply_link": "https://cigna.dejobs.org/st-louis-mo/senior-ai-engineer-hybrid/EBED4B6A7FE248C1BDD228F64903044D/job/?vs=28", "is_direct": false}, {"publisher": "SimplyHired", "apply_link": "https://www.simplyhired.com/job/dYnXd59qrd8NODRfQv-pSdScNpWrlR6rBCc7Z-ays3S4_JtSXduYHQ", "is_direct": false}, {"publisher": "Cigna - Talentify", "apply_link": "https://cigna.talentify.io/job/senior-ai-engineer-saint-louis-missouri-cigna-23009983", "is_direct": false}, {"publisher": "Jooble", "apply_link": "https://jooble.org/jdp/-1610515414124069202/Senior-engineer-Saint-Louis%2C-MO", "is_direct": false}], "job_description": "The job profile for this position is Machine Learning Senior Advisor, which is a Band 4 Senior Contributor Career Track Role.\n\nExcited to grow your career?\n\nWe value our talented employees, and whenever possible strive to help one of our associates grow professionally before recruiting new talent to our open positions. If you think the open position you see is right for you, we encourage you to apply!\n\nOur people make all the difference in our success.\n\nAs a Senior AI Engineer, you will be joining a team transforming healthcare and improving the lives and vitality of the millions of members we serve. We leverage cutting edge Artificial Intelligence (AI) and Machine Learning (ML) algorithms to develop solutions for automated document processing and customer service chat bots. We are looking for Sr AI Engineers with strong engineering, full stack expertise to build the best fit solutions leveraging Large Language Models (LLMs) and Generative AI solutions. The work you do will impact millions of customers, members, and employers that rely on Cigna every day. Extreme focus on speed to market and getting Products and Services in the hands of customer and passion to transform healthcare is key to the success of this role.\n\nResponsibilities\n\u2022 Build scalable software solutions using LLM\u2019s and other ML models to solve challenges in healthcare\n\u2022 Build enterprise grade AI solutions with focus on privacy, security, fairness.\n\u2022 Work with Product Development as a Generative Artificial Intelligence (AI) subject matter expert and architect and develop scalable, resilient, ethical AI solutions\n\u2022 Strong engineering skills to design the output from the AI with nodes and nested nodes in JSON or array, HTML formats as required \u2013 This is critical so that the AI output can be consumed as is and displayed on the dashboard for accelerated development cycles.\n\u2022 Build extensible API Integrations, low code UI/UX solutions, with extremely short cycle times, to extract information from sources, integrate with GPT4, receive insights and make them available in intuitive, high performing dashboards\n\u2022 Build solutions that align with responsible AI practices.\n\u2022 Envision the solution outcomes to solve for the business problem with actionable insights and design viable solutions to meet the outcomes.\n\u2022 Understand how AI is interpreting the data set and use that understanding to build prompts that lead to expected outcomes\n\u2022 Architect and develop software or infrastructure for scalable, distributed systems and with machine learning technologies.\n\u2022 Work with frameworks(Tensorflow, PyTorch) and open source platforms like Hugging Face to deliver the best solutions\n\u2022 Optimize existing generative AI models for improved performance, scalability, and efficiency.\n\u2022 Develop and maintain AI pipelines, including data preprocessing, feature extraction, model training, and evaluation.\n\u2022 Develop clear and concise documentation, including technical specifications, user guides, and presentations, to communicate complex AI concepts to both technical and non-technical stakeholders.\n\u2022 Contribute to the establishment of best practices and standards for generative AI development within the organization.\n\nQualifications:\n\u2022 Degree in Computer Science, Artificial Intelligence, or a related field.\n\u2022 5 years of Full stack engineering expertise with languages like C#, Python and Proficiency in designing architecture, building API Integrations, configuring and deploying cloud services, setting up authentication, monitoring and logging\n\u2022 Experience in implementing enterprise systems in production setting for AI, computer vision, natural language processing. Exposure to self-supervised learning, transfer learning, and reinforcement learning is a plus.\n\u2022 Experience with information storage/retrieval using vector databases like pinecone.\n\u2022 Strong understanding and exposure in natural language generation or Gen AI like transformers, LLM\u2019s, text embedding\u2019s.\n\u2022 Experience with designing scalable software systems for classification, text extraction/summary, data connectors for different formats(pdf, csv, doc, etc)\n\u2022 Experience with machine learning libraries and frameworks such as PyTorch or TensorFlow, Hugging Face, Lang chain, Llama Index.\n\u2022 3 years of experience in a technical leadership role leading project teams and setting technical direction.\n\u2022 3 years of experience working in a complex, matrixed organization involving cross-functional or cross-business projects.\n\u2022 Programming experience in C/C++, Java, Python.\n\u2022 Strong knowledge of data structures, algorithms, and software engineering principles.\n\u2022 Familiarity with cloud-based platforms and services, such as AWS, GCP, or Azure.\n\u2022 Excellent problem-solving skills, with the ability to think critically and creatively to develop innovative AI solutions.\n\u2022 Strong communication skills, with the ability to effectively convey complex technical concepts to a diverse audience.\n\u2022 Possess a proactive mindset, with the ability to work independently and collaboratively in a fast-paced, dynamic environmen\n\u2022 This is a hybrid role and will require the ability to work in-person\n\nIf you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload.\n\nQualified applicants will be considered without regard to race, color, age, disability, sex, childbirth (including pregnancy) or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.\n\nPlease note that you must meet our posting guidelines to be eligible for consideration. Policy can be reviewed at this link.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Washington", "job_state": "DC", "job_country": "US", "job_latitude": 38.907192, "job_longitude": -77.03687, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=CJdR7_aAck1-TIhMAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": null, "job_offer_expiration_timestamp": null, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 36, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Degree in Computer Science, Artificial Intelligence, or a related field", "5 years of Full stack engineering expertise with languages like C#, Python and Proficiency in designing architecture, building API Integrations, configuring and deploying cloud services, setting up authentication, monitoring and logging", "Experience in implementing enterprise systems in production setting for AI, computer vision, natural language processing", "Experience with information storage/retrieval using vector databases like pinecone", "Strong understanding and exposure in natural language generation or Gen AI like transformers, LLM\u2019s, text embedding\u2019s", "Experience with designing scalable software systems for classification, text extraction/summary, data connectors for different formats(pdf, csv, doc, etc)", "Experience with machine learning libraries and frameworks such as PyTorch or TensorFlow, Hugging Face, Lang chain, Llama Index", "3 years of experience in a technical leadership role leading project teams and setting technical direction", "3 years of experience working in a complex, matrixed organization involving cross-functional or cross-business projects", "Programming experience in C/C++, Java, Python", "Strong knowledge of data structures, algorithms, and software engineering principles", "Familiarity with cloud-based platforms and services, such as AWS, GCP, or Azure", "Excellent problem-solving skills, with the ability to think critically and creatively to develop innovative AI solutions", "Strong communication skills, with the ability to effectively convey complex technical concepts to a diverse audience", "Possess a proactive mindset, with the ability to work independently and collaboratively in a fast-paced, dynamic environmen", "This is a hybrid role and will require the ability to work in-person", "If you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload"], "Responsibilities": ["Extreme focus on speed to market and getting Products and Services in the hands of customer and passion to transform healthcare is key to the success of this role", "Build scalable software solutions using LLM\u2019s and other ML models to solve challenges in healthcare", "Build enterprise grade AI solutions with focus on privacy, security, fairness", "Work with Product Development as a Generative Artificial Intelligence (AI) subject matter expert and architect and develop scalable, resilient, ethical AI solutions", "Strong engineering skills to design the output from the AI with nodes and nested nodes in JSON or array, HTML formats as required \u2013 This is critical so that the AI output can be consumed as is and displayed on the dashboard for accelerated development cycles", "Build extensible API Integrations, low code UI/UX solutions, with extremely short cycle times, to extract information from sources, integrate with GPT4, receive insights and make them available in intuitive, high performing dashboards", "Build solutions that align with responsible AI practices", "Envision the solution outcomes to solve for the business problem with actionable insights and design viable solutions to meet the outcomes", "Understand how AI is interpreting the data set and use that understanding to build prompts that lead to expected outcomes", "Architect and develop software or infrastructure for scalable, distributed systems and with machine learning technologies", "Work with frameworks(Tensorflow, PyTorch) and open source platforms like Hugging Face to deliver the best solutions", "Optimize existing generative AI models for improved performance, scalability, and efficiency", "Develop and maintain AI pipelines, including data preprocessing, feature extraction, model training, and evaluation", "Develop clear and concise documentation, including technical specifications, user guides, and presentations, to communicate complex AI concepts to both technical and non-technical stakeholders", "Contribute to the establishment of best practices and standards for generative AI development within the organization"]}, "job_job_title": "Engineer", "job_posting_language": "en", "job_onet_soc": "15111100", "job_onet_job_zone": "5", "job_occupational_categories": ["Data & Analytics"], "job_naics_code": "524113", "job_naics_name": "Direct Life Insurance Carriers"}, {"employer_name": "Amazon", "employer_logo": "https://visualhierarchy.co/blog/wp-content/uploads/2018/04/amazon.jpg", "employer_website": "http://www.amazon.com", "employer_company_type": "Retail", "job_publisher": "ZipRecruiter", "job_id": "wdW6cQk03QZwlXekAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Legal Data Engineer, Legal Data & Business Intelligence", "job_apply_link": "https://www.ziprecruiter.com/c/Amazon/Job/Legal-Data-Engineer,-Legal-Data-&-Business-Intelligence/-in-Seattle,WA?jid=7ccc3578979fc7e9", "job_apply_is_direct": false, "job_apply_quality_score": 0.6608, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Amazon/Job/Legal-Data-Engineer,-Legal-Data-&-Business-Intelligence/-in-Seattle,WA?jid=7ccc3578979fc7e9", "is_direct": false}, {"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/legal-data-engineer-legal-data-business-intelligence-at-amazon-3781745824", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=097a9e072038863e", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/legal-data-engineer-legal-data-and-business-intelligence-amazon-com-services-llc-JV_IC1150505_KO0,56_KE57,80.htm?jl=1009030752687", "is_direct": false}, {"publisher": "Getwork", "apply_link": "https://getwork.com/details/84025c43feca3bf1960261f4ace28427", "is_direct": false}, {"publisher": "Jora", "apply_link": "https://us.jora.com/job/Legal-Data-Engineer-24aed831f2c8454ad9f5182a2887fafb", "is_direct": false}, {"publisher": "Resume-Library.com", "apply_link": "https://www.resume-library.com/job/view/137179794/legal-data-engineer%2C-legal-data-%26-business-intelligence", "is_direct": true}, {"publisher": "EduCativ", "apply_link": "https://educativ.net/jobs/job/820923-legal-data-engineer-legal-data-business-intelligence-seattle/", "is_direct": false}], "job_description": "Legal Technologies is looking for a Data Engineer to join our team. Legal Technologies is a software development team that builds tools for Amazon Legal to automate and manage litigation, regulatory contacts, contracts, and other legal work. The data engineer will creates data pipelines, manage the data warehouse, and support business intelligence engineers who build dashboards and reports for Amazon Legal.\n\nThe ideal candidate will have experience with data warehousing, ETL, and infrastructure. The candidate will be responsible for on-boarding new data sources and evolving the current architecture and data model. You will also have outstanding business acumen and an ability to work with a variety of teams with different technical abilities. You should be a self-starter comfortable with ambiguity, have a strong attention to detail, and be driven by a desire to innovate. While the data engineer is not directly responsible for building reports, they will support business intelligence engineers and business analysts who present data and metrics at all levels of the business.\n\nKey job responsibilities\n- Support a data warehouse providing secure access to confidential departmental data across all areas of legal practice.\n- Use AWS tools for data intake, processing, storage, and analytics.\n- Meet with business customers to understand how they use data, and provide input and recommendations on technical issues to BI engineers and business analysts.\n- Ensures completeness and compatibility of the technical infrastructure to support system performance, availability and architecture requirements.\n- Write complete SQL queries, and tune SQL queries for various data requests.\n- Responsible for data design, data extracts and transforms, data quality monitoring, enhancements and ongoing support.\n\nAbout the team\nThe Legal Data & Business Intelligence is a small team supporting the Legal department, as well as reporting on legal processes that affect all of Amazon. Our focus is on capturing and presenting data, and enabling other teams to report on that data. We are part of a software development org, and work closely with the devs who build legal tools.\n\nWe are open to hiring candidates to work out of one of the following locations:\n\nSeattle, WA, USA", "job_is_remote": false, "job_posted_at_timestamp": 1703103660, "job_posted_at_datetime_utc": "2023-12-20T20:21:00.000Z", "job_city": "Seattle", "job_state": "WA", "job_country": "US", "job_latitude": 47.60614, "job_longitude": -122.33285, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=wdW6cQk03QZwlXekAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-20T00:00:00.000Z", "job_offer_expiration_timestamp": 1705708800, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["You should be a self-starter comfortable with ambiguity, have a strong attention to detail, and be driven by a desire to innovate"], "Responsibilities": ["The candidate will be responsible for on-boarding new data sources and evolving the current architecture and data model", "Support a data warehouse providing secure access to confidential departmental data across all areas of legal practice", "Use AWS tools for data intake, processing, storage, and analytics", "Meet with business customers to understand how they use data, and provide input and recommendations on technical issues to BI engineers and business analysts", "Ensures completeness and compatibility of the technical infrastructure to support system performance, availability and architecture requirements", "Write complete SQL queries, and tune SQL queries for various data requests", "Responsible for data design, data extracts and transforms, data quality monitoring, enhancements and ongoing support"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15119900", "job_onet_job_zone": "4", "job_occupational_categories": ["23-2011.00: Paralegals and Legal Assistants"], "job_naics_code": "454111", "job_naics_name": "Electronic Shopping"}, {"employer_name": "Salesforce", "employer_logo": "https://careers.salesforce.com/images/logo.svg", "employer_website": "http://www.salesforce.com", "employer_company_type": "Information", "job_publisher": "Geebo", "job_id": "zszYzSp8RZpIK1UfAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Associate Data Engineer, Enterprise Data Warehouse at Salesforce in Dallas, TX", "job_apply_link": "https://geebo.com/jobs-online/view/id/1051796261-associate-data-engineer-enterprise-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4542, "apply_options": [{"publisher": "Geebo", "apply_link": "https://geebo.com/jobs-online/view/id/1051796261-associate-data-engineer-enterprise-/", "is_direct": true}], "job_description": "Job Details Business Technology blazes the trail of enterprise IT. Built on the foundation of our core values, we own more than the traditional IT components with a heavy focus on working closely with our business partners for amazing outcomes. Our goal is to deliver technology that is centered around our business and our collective success. We oversee technology strategy, Salesforce on Salesforce, customer and partner enablement, applications engineering, infrastructure, collaboration, enterprise operations, architecture, and program enablement. We own the world's foremost Salesforce implementation and enable our global Ohana to do their best work by leveraging our platform. The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, Customer success and Employee Success. This platform also includes management of a Big Data Hadoop platform to store unstructured application log data that is generated by the Salesforce product offerings, this data is predominately used by the data science teams. Team also leads all aspects of rolling out Salesforce's analytics tool Wave to internal business partners. The Salesforce IT EDW team is looking for an experienced BI designer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com. This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.\nResponsibilities:\n? Experience in designing and development of analytical solutions including Salesforce analytics solution - Wave and Tableau ? Contribute towards designing data models, ETL mappings and associated objects of analytical solutions. ? Review solution design with Lead members and ensure that the defined EDW standards and framework are followed. ? Review and validate logical and physical design to ensure alignment with the defined solution architecture. ? Create/review technical documentation for all new and modified objects. ? Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly. ? Follow standard practises for QA in the organization ? Support QA, UAT and performance testing phases of the development cycle. ? Understand and incorporate required security framework in the developed data model and ETL objects. ? Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes. ? Evaluate, determine root cause and resolve production issues. ? Work closely with other IT development groups to deliver coordinated software solutions Required Skills:\n? 1+year of experience working as a BI technical resource in a customer-focused IT EDW team ? 2+years of experience in the data warehousing domain as a technical resource ? Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools. ?Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture. ? Good knowledge of SQL and relational database models. ? Hands-on experience creating Unix shell scripts ? Data visualization tool experience like Tableau ? Good understanding of data modeling ? Basic working experience in an agile environment ? Excellent team player able to work with virtual and global across functional teams at all levels. ? Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines. ? Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences. ? Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role. Desired Skills:\n? Salesforce, Data Warehousing, customer success and Data Analysis ? Working experience on Data science or Python scripting is a plus ? Working experience of snowflake ? Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Dallas", "job_state": "TX", "job_country": "US", "job_latitude": 32.776665, "job_longitude": -96.79699, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=zszYzSp8RZpIK1UfAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 24, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel", "1+year of experience working as a BI technical resource in a customer-focused IT EDW team ?", "2+years of experience in the data warehousing domain as a technical resource ?", "Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools", "Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture", "Good knowledge of SQL and relational database models", "Hands-on experience creating Unix shell scripts ?", "Data visualization tool experience like Tableau ?", "Good understanding of data modeling ?", "Basic working experience in an agile environment ?", "Excellent team player able to work with virtual and global across functional teams at all levels", "Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines", "Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences", "Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role", "Salesforce, Data Warehousing, customer success and Data Analysis ?", "Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements", "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"], "Responsibilities": ["Contribute towards designing data models, ETL mappings and associated objects of analytical solutions", "Review solution design with Lead members and ensure that the defined EDW standards and framework are followed", "Review and validate logical and physical design to ensure alignment with the defined solution architecture", "Create/review technical documentation for all new and modified objects", "Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly", "Follow standard practises for QA in the organization ?", "Support QA, UAT and performance testing phases of the development cycle", "Understand and incorporate required security framework in the developed data model and ETL objects", "Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes", "Evaluate, determine root cause and resolve production issues", "Work closely with other IT development groups to deliver coordinated software solutions Required Skills:"], "Benefits": ["$80K -- $100K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "511210", "job_naics_name": "Software Publishers"}, {"employer_name": "Insight Global", "employer_logo": "https://images.squarespace-cdn.com/content/5f7f984c3ca20d1d55b276f7/1619793549819-L54X3W9Z5RD277UPNH9S/IGLogoPublic.png?content-type=image%2Fpng", "employer_website": "http://www.insightglobal.com", "employer_company_type": "Staffing", "job_publisher": "Jobs Trabajo.org", "job_id": "RHu0HzNwDwxUdafxAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Lead SR. Data Engineer", "job_apply_link": "https://us.trabajo.org/job-640-20231221-783b1f2ef6a2190ac91ea7585e529c93", "job_apply_is_direct": false, "job_apply_quality_score": 0.4387, "apply_options": [{"publisher": "Jobs Trabajo.org", "apply_link": "https://us.trabajo.org/job-640-20231221-783b1f2ef6a2190ac91ea7585e529c93", "is_direct": false}, {"publisher": "Jobilize", "apply_link": "https://www.jobilize.com/job/us-oh-new-albany-lead-sr-data-engineer-remote-insight-global-hiring", "is_direct": false}], "job_description": "A client of Insight Global is seeking a Sr. Data Engineer to join their team. The primary responsibility of Senior Data Management Engineer is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self-service data pipelines, assist in operationalizing the AI / ML Engineering solutions. This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards.\n\nThis role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project.\n\nResponsibilities:\n\u2022 Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation\n\u2022 Create and maintain the data ingestion, quality testing and audit framework\n\u2022 Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE.\n\u2022 Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M\n\u2022 Setup and evangelize the metadata driven approach to data pipelines to promote self service\n\u2022 Setup and continuously improve the data quality and audit monitoring as well as alerting\n\u2022 Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design.\n\u2022 Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices\n\u2022 Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects\n\u2022 Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data\n\u2022 Respond SLA driven production data quality or pipeline issues\n\u2022 Work in a fast-paced Agile/Scrum environment\n\u2022 Identify and assist with implementation of DevOps practices in support of fully automated deployments\n\u2022 Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines\n\u2022 Follow the Industry standard data security practices and evangelize the same across the team.\n\nMust Haves:\n\u2022 5+ years of experience in an Enterprise Data Management or Data Engineering role\n\u2022 3+ of hands-on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake\n\u2022 5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks\n\u2022 Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, SQL Server, MySQL, Teradata\n\u2022 Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab\n\u2022 Preferably experienced in SLA driven Production Data Pipeline or Quality support\n\u2022 Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc.\n\u2022 Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)\n\u2022 Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE\n\u2022 Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls\n\u2022 Ability to implement Agile methodologies and work in an Agile DevOps environment\n\u2022 Bachelor's degree in computer science or engineering or mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization\n\u2022 Experience designing and building complex data pipelines in an agile environment\n\u2022 Expertise on data analysis and wrangling using SQL, python, data bricks\n\u2022 Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions\n\u2022 Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms", "job_is_remote": false, "job_posted_at_timestamp": 1703118897, "job_posted_at_datetime_utc": "2023-12-21T00:34:57.000Z", "job_city": "New Albany", "job_state": "OH", "job_country": "US", "job_latitude": 40.08216, "job_longitude": -82.81009, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=RHu0HzNwDwxUdafxAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["5+ years of experience in an Enterprise Data Management or Data Engineering role", "3+ of hands-on experience in building metadata driven data pipelines using Azure Data Factory, Databricks / Spark for Cloud Datalake", "5+ years hands on experience with using one or more of the following for data analysis and wrangling Databricks, Python / PySpark, Jupyter Notebooks", "Expert level SQL knowledge on databases such as but not limited to Snowflake, Netezza, Oracle, SQL Server, MySQL, Teradata", "Experience working in a multi developer environment and hands on experience in using either azure devops or gitlab", "Preferably experienced in SLA driven Production Data Pipeline or Quality support", "Experience or strong understanding of the traditional enterprise ETL platforms such as IBM Datastage, Informatica, Pentaho, Ab Initio etc", "Functional knowledge of some of the following technologies - Terraform, Azure CLI, PowerShell, Containerization (Kubernetes, Docker)", "Functional knowledge of one or more Reporting tools such as PowerBI, Tableau, OBIEE", "Team player with excellent communication skills, ability to communicate with the customer directly and able to explain the status of the deliverables in scrum calls", "Ability to implement Agile methodologies and work in an Agile DevOps environment", "Bachelor's degree in computer science or engineering or mathematics or related field and 5+ years of experience in various cloud technologies within a large-scale organization", "Experience designing and building complex data pipelines in an agile environment", "Expertise on data analysis and wrangling using SQL, python, data bricks", "Experience with modern cloud development and design concepts; software development lifecycle; multi-developer code versioning and conflict resolution; planning, design, and problem resolution enterprise data applications / solutions", "Demonstrated ability in developing a culture that embraces innovation, and challenges existing paradigms"], "Responsibilities": ["The primary responsibility of Senior Data Management Engineer is to build data pipelines, model and prepare data, perform complex data analysis to answer Business questions, build and automate data pipeline and quality framework to enable and promote self-service data pipelines, assist in operationalizing the AI / ML Engineering solutions", "This role is expected to lead and guide other team members and evangelize the design patterns as well as coding standards", "This role plays an active part in our Data Modernization project to migrate the from on-prem platforms such as IBM Netezza to cloud project", "Team up with the engineering teams and enterprise architecture (EA) to define standards, design patterns, accelerators, development practices, DevOps and CI/CD automation", "Create and maintain the data ingestion, quality testing and audit framework", "Conduct complex data analysis to answer the queries from Business Users or Technology team partners either directly from Analysts or stemmed from one of the Reporting tools suchs PowerBI, Tableau, OBIEE", "Build and automate the data ingestion, transformation and aggregation pipelines using Azure Data Factory, Databricks / Spark, Snowflake, Kafka as well as Enterprise Scheduler tools such as CA Workload automation or Control M", "Setup and evangelize the metadata driven approach to data pipelines to promote self service", "Setup and continuously improve the data quality and audit monitoring as well as alerting", "Constantly evaluate the process automation options and collaborate with engineering as well as architecture to review the proposed design", "Demonstrate mastery of build and release engineering principles and methodologies including source control, branch management, build and smoke testing, archiving and retention practices", "Adhere to and enhance and document the design principles, best practices by collaborating with Solution and in some cases Enterprise Architects", "Participate in and support the Data Academy and Data Literacy program to train the Business Users and Technology teams on Data", "Respond SLA driven production data quality or pipeline issues", "Work in a fast-paced Agile/Scrum environment", "Identify and assist with implementation of DevOps practices in support of fully automated deployments", "Document the Data Flow Diagrams, Data Models, Technical Data Mapping and Production Support Information for Data Pipelines", "Follow the Industry standard data security practices and evangelize the same across the team"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "561311", "job_naics_name": "Employment Placement Agencies"}, {"employer_name": "R1 RCM", "employer_logo": "https://cdn2.hubspot.net/hubfs/4941928/R1%20Logo%C2%AE%20Blue%202935C.png", "employer_website": "http://www.r1rcm.com", "employer_company_type": "Office Administration", "job_publisher": "ZipRecruiter", "job_id": "dRWgE3jmUmsqk9T-AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Staff Data Engineer", "job_apply_link": "https://www.ziprecruiter.com/c/R1-RCM/Job/Staff-Data-Engineer/-in-Boise,ID?jid=a3333bd95126a95c", "job_apply_is_direct": false, "job_apply_quality_score": 0.6218, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/R1-RCM/Job/Staff-Data-Engineer/-in-Boise,ID?jid=a3333bd95126a95c", "is_direct": false}], "job_description": "at R1 RCM in Boise, Idaho, United States\nJob Description\n\nThe Sr. Staff Data Software Engineer is a senior most technical member in the engineering team and applies advanced technical knowledge, broad knowledge of data management best practices in the context of big data engineering, problem solving, and creativity to build and maintain data software products that achieve technical, business, and customer experience goals. This role requires the ability and willingness to mentor and develop junior data platform software engineers, and act as a thought leader and trusted subject-matter expert in area of big data and data platform technical domain.\n\nResponsibilities:\n\n+ Be a subject matter expert on orchestration, data model, and big data processing for the teams.\n\n+ Directly works with data platform senior leadership in ideating and creating a roadmap for the data platform to be a modern, highly scalable, and highly resilient platform.\n\n+ Works with product management, business stakeholders, and application architects to understand software requirements and helps estimate epics, features, and stories.\n\n+ Experiments with cutting-edge technologies that can help to modernize the data platform architecture and address technical challenges as business and data grows.\n\n+ Provides thoughtful recommendations in sessions development team members in larger data platform engineering team to structure solution source code and implementation approaches - emphasizing the need to optimize code that follows engineering best practices, and maximizes maintainability, testability, and performance.\n\n+ Ensures appropriate design patterns are applied to system, data architectures and implementations.\n\n+ Provides skillful communication and respectful listening - conveying logical and structured thoughts, truthfulness, empathy, confidence, and friendliness.\n\n+ Applies consistent levels of strategic thinking, judgment, decision making, attention to detail, teamwork, organization, innovation, and initiative.\n\n+ Willingly and enthusiastically mentors' teams with design review sessions the teams have created.\n\n+ Evaluates understands and recommends new technology, languages, or development practices that have benefits for implementing.\n\nPreferred Qualifications\n\n+ Deep expertise in building configuration based modern data pipeline automation using big data technologies such as Scala AND Apache Spark.\n\n+ Experience building solutions using modern noSQL data stores such as MongoDB, Couchbase\n\n+ Experience with event streaming technol\nTo view full details and how to apply, please login or create a Job Seeker account", "job_is_remote": false, "job_posted_at_timestamp": 1703096280, "job_posted_at_datetime_utc": "2023-12-20T18:18:00.000Z", "job_city": "Boise", "job_state": "ID", "job_country": "US", "job_latitude": 43.615017, "job_longitude": -116.20232, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=dRWgE3jmUmsqk9T-AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-19T00:00:00.000Z", "job_offer_expiration_timestamp": 1705622400, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Responsibilities": ["This role requires the ability and willingness to mentor and develop junior data platform software engineers, and act as a thought leader and trusted subject-matter expert in area of big data and data platform technical domain", "Be a subject matter expert on orchestration, data model, and big data processing for the teams", "Directly works with data platform senior leadership in ideating and creating a roadmap for the data platform to be a modern, highly scalable, and highly resilient platform", "Works with product management, business stakeholders, and application architects to understand software requirements and helps estimate epics, features, and stories", "Experiments with cutting-edge technologies that can help to modernize the data platform architecture and address technical challenges as business and data grows", "Provides thoughtful recommendations in sessions development team members in larger data platform engineering team to structure solution source code and implementation approaches - emphasizing the need to optimize code that follows engineering best practices, and maximizes maintainability, testability, and performance", "Ensures appropriate design patterns are applied to system, data architectures and implementations", "Applies consistent levels of strategic thinking, judgment, decision making, attention to detail, teamwork, organization, innovation, and initiative", "Willingly and enthusiastically mentors' teams with design review sessions the teams have created", "Evaluates understands and recommends new technology, languages, or development practices that have benefits for implementing"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_occupational_categories": ["15-2041.00: Statisticians"], "job_naics_code": "561110", "job_naics_name": "Office Administrative Services"}, {"employer_name": "iHerb.com", "employer_logo": "https://images.crunchbase.com/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/bcd0rdavg1nsilncplmf", "employer_website": "http://www.iherb.com", "employer_company_type": "Retail", "job_publisher": "Beavercreek, OR - Geebo", "job_id": "C-Jbj61CPOVCqe8rAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineer - AWS at iHerb.COM in Remote", "job_apply_link": "https://beavercreek-or.geebo.com/jobs-online/view/id/1080708174-data-engineer-aws-at-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.5158, "apply_options": [{"publisher": "Beavercreek, OR - Geebo", "apply_link": "https://beavercreek-or.geebo.com/jobs-online/view/id/1080708174-data-engineer-aws-at-/", "is_direct": true}], "job_description": "Job Description We're looking for a Senior Data Engineer to work on our massive data processing pipeline and lead our data lake and data warehouse building, to help us deliver more insights and scale our data infrastructure. You will have a chance to contribute to the company's evolving culture, bring innovative approaches and learn from your talented colleagues. Responsibilities Designs and develops programs and tools to support data ingestion, curation and provisioning of complex enterprise data to achieve analytics & reporting on our current technology stack Designs and builds data extracts, integrations and transformations Provides successful deployment and provisioning of data solutions to required environments Designs and builds data architecture and applications that successfully enable speed, quality and efficient pipelines Interacts with cross-functional customers and development team to gather and define requirements. Develops understanding of the data and builds business acumen. Reviews discrepancies in requirements and resolves with stakeholders. Identifies and recommends appropriate continuous improvement opportunities and ensures integrations are automated and have proper exception handling. Key team member of project team designing and deploying a ground up cloud data pipeline Qualifications Bachelor or Master s degree in technical discipline such as Computer Science, Information Systems or another technical field People person, team player with a strong can-do mentality 5 years as a Data Engineer on a data and analytics team Proficient in data modelling principles Proficiency in Snowflake and other data warehousing solutions (Redshift / BigQuery) Proficiency in Databricks Experience with building data stream pipelines and ETL using:\nSpark, Elastic Advanced experience with AWS cloud services Advanced knowledge in Python Advanced working SQL experience as well as performance tuning Experience with data pipeline workflow management tools such as:\nAirflow, Astronomer Knowledge and ability to write, test, and debug APIs Experience working with agile methodologies and working in cross-functional teams Must be proactive, demonstrate initiative and be a logical thinker Must be an inquisitive learner and have a thirst for improvement Ability to understand and apply customer requirements, including drawing out unforeseen implications and making design recommendations. Strong facilitation and consensus building skills. Strong oral and written communication skills; Ability to communicate by simplifying complexity Ability to provide work guidance to junior level developers Enjoys higher learning and keeps track of industry best practices and trends and through acquired knowledge, takes advantage of process and system improvement opportunities Be proactive requiring minimal supervision, be strong time management and work organization skills Have an ability prioritize workload and handle multiple tasks and at times meet tight deadlines. Strong problem-solving and analytical skills Has worked on data quality improvement projects such as Master Data Management Highly Desired AWS certifications (any):\nAWS Certified Solutions Architect - Associate/Professional AWS Certified Developer - Associate/Professional AWS Certified DevOps Engineer AWS Certified Solutions Architect AWS Certified Data Analytics AWS Certified Security - Specialty AWS Certified Cloud Practitioner.\nSalary Range:\n$100K -- $150K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Beavercreek", "job_state": "OR", "job_country": "US", "job_latitude": 45.28793, "job_longitude": -122.53534, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=C-Jbj61CPOVCqe8rAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": true}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {"Qualifications": ["Key team member of project team designing and deploying a ground up cloud data pipeline Qualifications Bachelor or Master s degree in technical discipline such as Computer Science, Information Systems or another technical field People person, team player with a strong can-do mentality 5 years as a Data Engineer on a data and analytics team Proficient in data modelling principles Proficiency in Snowflake and other data warehousing solutions (Redshift / BigQuery) Proficiency in Databricks Experience with building data stream pipelines and ETL using:", "Spark, Elastic Advanced experience with AWS cloud services Advanced knowledge in Python Advanced working SQL experience as well as performance tuning Experience with data pipeline workflow management tools such as:", "Airflow, Astronomer Knowledge and ability to write, test, and debug APIs Experience working with agile methodologies and working in cross-functional teams Must be proactive, demonstrate initiative and be a logical thinker Must be an inquisitive learner and have a thirst for improvement Ability to understand and apply customer requirements, including drawing out unforeseen implications and making design recommendations", "Strong facilitation and consensus building skills", "Strong oral and written communication skills; Ability to communicate by simplifying complexity Ability to provide work guidance to junior level developers Enjoys higher learning and keeps track of industry best practices and trends and through acquired knowledge, takes advantage of process and system improvement opportunities Be proactive requiring minimal supervision, be strong time management and work organization skills Have an ability prioritize workload and handle multiple tasks and at times meet tight deadlines", "AWS Certified Solutions Architect - Associate/Professional AWS Certified Developer - Associate/Professional AWS Certified DevOps Engineer AWS Certified Solutions Architect AWS Certified Data Analytics AWS Certified Security - Specialty AWS Certified Cloud Practitioner", "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"], "Responsibilities": ["Responsibilities Designs and develops programs and tools to support data ingestion, curation and provisioning of complex enterprise data to achieve analytics & reporting on our current technology stack Designs and builds data extracts, integrations and transformations Provides successful deployment and provisioning of data solutions to required environments Designs and builds data architecture and applications that successfully enable speed, quality and efficient pipelines Interacts with cross-functional customers and development team to gather and define requirements", "Develops understanding of the data and builds business acumen", "Reviews discrepancies in requirements and resolves with stakeholders", "Identifies and recommends appropriate continuous improvement opportunities and ensures integrations are automated and have proper exception handling"], "Benefits": ["$100K -- $150K"]}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "454111", "job_naics_name": "Electronic Shopping"}, {"employer_name": "Jenius Bank", "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQwDHvr5CmxHZWM_xQiOUk3Sp3fSNFEmLUdvK0R&s=0", "employer_website": null, "employer_company_type": null, "job_publisher": "LinkedIn", "job_id": "Sg7i32qaVI-hzdEAAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "QA Automation Engineer - Data (Hybrid)", "job_apply_link": "https://www.linkedin.com/jobs/view/qa-automation-engineer-data-hybrid-at-jenius-bank-3740222577", "job_apply_is_direct": false, "job_apply_quality_score": 0.782, "apply_options": [{"publisher": "LinkedIn", "apply_link": "https://www.linkedin.com/jobs/view/qa-automation-engineer-data-hybrid-at-jenius-bank-3740222577", "is_direct": false}, {"publisher": "Careers At SMBC", "apply_link": "https://careers.smbcgroup.com/job/Scottsdale-Software-Engineer-QA-Data-%28Hybrid%29-AZ-85255/1087145600/", "is_direct": false}, {"publisher": "Salary.com", "apply_link": "https://www.salary.com/job/sumitomo-mitsui-banking-corporation-smbc/qa-automation-engineer-data-hybrid/j202311211243411486107", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=18d9b441c3b73fe6", "is_direct": false}, {"publisher": "Milwaukee Jobs", "apply_link": "https://www.milwaukeejobs.com/job/detail/77522035/Software-Engineer-Data-Hybrid?keywords=Planning%2Bor%2BProject%2BManagement%2BInformation%2BTechnology%2BSpecialist", "is_direct": false}, {"publisher": "Monster", "apply_link": "https://www.monster.com/job-openings/software-engineer-data-hybrid-scottsdale-az--9091c6c4-22f5-46ea-94f6-6827dfee7d02", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/software-engineer-data-hybrid-sumitomo-mitsui-banking-corporation-JV_IC1133911_KO0,29_KE30,65.htm?jl=1009021441058", "is_direct": false}, {"publisher": "Ladders", "apply_link": "https://www.theladders.com/job-listing/qa-automation-engineer-data-hybrid-sumitomo-mitsui-banking-corporation-smbc-scottsdale-az-_v2_-7-3389790573.html", "is_direct": false}], "job_description": "Join us on our mission to create a completely new, 100% digital bank that truly serves customers' best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).\n\nThe anticipated salary range for this role is between $78,000.00 and $130,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees.\n\nWe work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the second largest bank in Japan and the 12th largest bank in the world with operations in over forty countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.\n\nIt is the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the 'patient capital' necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.\n\nSummary\n\nWe are seeking a QA Automation Engineer - Data to provide continuous automated and manual testing of data sets for use in internal data systems and for delivery from internal systems to users within Jenius Bank. Data sets can be in several formats, depending on supplier systems and client requirements, including JSON, XML, CSV or RDF. The QA Automation Engineer - Data works in the technology team with the data engineers.\n\nPrincipal Duties And Responsibilities\n\u2022 Partner with Data Engineering development teams to enable code delivery, automated testing, and assurance of product reliability.\n\u2022 Create, communicate, and enforce data quality management policies, processes, and procedures.\n\u2022 Create effective test plans and data sets related to functional testing and end-to-end testing for ETL, database and reports.\n\u2022 Review requirements, specifications, and technical documentation to provide meaningful feedback.\n\u2022 Articulate test results, progress, and milestones to leadership and development teams\n\u2022 Create jobs and scripts to automatically test the quality of data throughout our data warehouse environments.\n\u2022 Create, document, and execute test cases to support product releases.\n\nPosition Specifications\n\u2022 Education: Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field\n\u2022 Highly proficient in Python and SQL\n\u2022 4+ years of experience in a Quality Assurance / Automation Engineering role\n\u2022 3+ years experience with creation, execution, and maintenance of automation frameworks, scripts in either Python\n\u2022 3+ years experience with SQL\n\u2022 Experience in DevOps and CI/CD related technologies\n\u2022 Preferred experience in a public cloud environment, preferably GCP\n\u2022 Preferred experience in testing data pipelines built in Python.\n\u2022 Preferred experience in Data governance and Metadata Management\n\u2022 Preferred experience working in a Big Data environment, dealing with large diverse data sets.\n\u2022 Have understanding of Spark/Pyspark to test using Notebooks\n\u2022 Ability to work independently, solve problems, update the stake holders.\n\u2022 Analyze, design, develop and deploy solutions as per business requirements.\n\u2022 Ability to identify defects in edge-case scenarios.\n\u2022 Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives.\n\u2022 Excellent analytical, problem solving, and troubleshooting skills in large data warehousing environments.\n\nEOE STATEMENT\n\nWe are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law.\n\nCCPA DISCLOSURE\n\nPersonal Information Collection Notice: This notice contains information under the California Consumer Privacy Act (CCPA) about the categories of personal information (PI) of California residents that Manufacturers Bank collects and the business or commercial purpose(s) for which the PI may be used. We do not sell PI. More information about our collection and use of PI may be found in our CCPA Privacy Policy at https://www.manufacturersbank.com/CCPA-Privacy. Persons with disabilities may contact our Customer Contact Center toll-free at (877) 560-9812 to request the information in this Notice in an alternative format.", "job_is_remote": false, "job_posted_at_timestamp": 1703153210, "job_posted_at_datetime_utc": "2023-12-21T10:06:50.000Z", "job_city": "Scottsdale", "job_state": "AZ", "job_country": "US", "job_latitude": 33.494877, "job_longitude": -111.92168, "job_benefits": ["health_insurance", "retirement_savings"], "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=Sg7i32qaVI-hzdEAAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-02-13T16:44:15.000Z", "job_offer_expiration_timestamp": 1707842655, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 48, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": true, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {"Qualifications": ["Education: Bachelor's Degree in Computer Science, Information Technology, Engineering, or related field", "Highly proficient in Python and SQL", "4+ years of experience in a Quality Assurance / Automation Engineering role", "3+ years experience with creation, execution, and maintenance of automation frameworks, scripts in either Python", "3+ years experience with SQL", "Experience in DevOps and CI/CD related technologies", "Have understanding of Spark/Pyspark to test using Notebooks", "Ability to identify defects in edge-case scenarios", "Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives", "Excellent analytical, problem solving, and troubleshooting skills in large data warehousing environments"], "Responsibilities": ["We are seeking a QA Automation Engineer - Data to provide continuous automated and manual testing of data sets for use in internal data systems and for delivery from internal systems to users within Jenius Bank", "Partner with Data Engineering development teams to enable code delivery, automated testing, and assurance of product reliability", "Create, communicate, and enforce data quality management policies, processes, and procedures", "Create effective test plans and data sets related to functional testing and end-to-end testing for ETL, database and reports", "Review requirements, specifications, and technical documentation to provide meaningful feedback", "Articulate test results, progress, and milestones to leadership and development teams", "Create jobs and scripts to automatically test the quality of data throughout our data warehouse environments", "Create, document, and execute test cases to support product releases", "Analyze, design, develop and deploy solutions as per business requirements"], "Benefits": ["The anticipated salary range for this role is between $78,000.00 and $130,000.00", "The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire", "The role may also be eligible for an annual discretionary incentive award", "In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees"]}, "job_job_title": "Automation engineer", "job_posting_language": "en", "job_onet_soc": "15119900", "job_onet_job_zone": "4"}, {"employer_name": "Avanade Inc", "employer_logo": "https://www.avanade.com/-/media/logo/share-avanade-logo.jpg?la=en-AU&vs=1", "employer_website": "http://www.avanade.com", "employer_company_type": "Consulting", "job_publisher": "ZipRecruiter", "job_id": "6GrInV0JdnNP8bi9AAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Data Engineering Manager", "job_apply_link": "https://www.ziprecruiter.com/c/Avanade-Inc/Job/Data-Engineering-Manager/-in-Indianapolis,IN?jid=da8a6642c01e7223", "job_apply_is_direct": false, "job_apply_quality_score": 0.6777, "apply_options": [{"publisher": "ZipRecruiter", "apply_link": "https://www.ziprecruiter.com/c/Avanade-Inc/Job/Data-Engineering-Manager/-in-Indianapolis,IN?jid=da8a6642c01e7223", "is_direct": false}, {"publisher": "The Muse", "apply_link": "https://www.themuse.com/jobs/avanade/data-engineering-manager-2e447a", "is_direct": false}, {"publisher": "LocalJobs.com", "apply_link": "https://www.localjobs.com/job/indianapolis-in-data-engineering-manager", "is_direct": false}, {"publisher": "Indeed", "apply_link": "https://www.indeed.com/viewjob?jk=42e3e89e4f23f866", "is_direct": false}, {"publisher": "Glassdoor", "apply_link": "https://www.glassdoor.com/job-listing/data-engineering-manager-avanade-JV_IC1145705_KO0,24_KE25,32.htm?jl=1008984000772", "is_direct": false}, {"publisher": "Ladders", "apply_link": "https://www.theladders.com/job/data-engineering-manager-avanade-indianapolis-in_65819813", "is_direct": false}, {"publisher": "Talent.com", "apply_link": "https://www.talent.com/view?id=69104e1614c4", "is_direct": false}, {"publisher": "BeBee", "apply_link": "https://us.bebee.com/job/20231212-08d846c445df869f77af0268ae105328", "is_direct": false}], "job_description": "Country: United States Cities: Chicago, Cleveland, Detroit, Indianapolis, Milwaukee, Minneapolis, St. Louis Area of expertise: Analytics Job Description Data Engineering Manager Our talented Data & AI Practice is made up of globally recognized experts - and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues, let's talk.\n\nTake your skills to a new level and launch a career where you can truly do what matters. Come join us As a Data Engineering Manager within our Data & AI Business Technology Group you'll leverage a client-centric approach to translate our client's business strategy into solutions and services that lead to successful, impactful outcomes.You'll draw on your considerable experience in bringing data and statistics to life to solve sometimes complex problems, and you're comfortable looking after several projects at once. You're able to make your own decisions while at the same time supporting more junior team members.\n\nTogether we do what matters. What you'll do As a Manager, Data Engineering, you know the importance of data to the business. You design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making.\n\nYou make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis Build the building blocks for transforming enterprise data solutions Design and build modern data pipelines, data streams, and data service Application Programming Interfaces (APIs) Craft the architectures, data warehouses and databases that support access, AI and Data Science capabilities and bring them to life through modern visualization tools As a senior member of the Avanade technical community, you will provide coaching and mentoring to motivate and inspire more junior colleagues from within your team and across the organization Skills and experiences Mapping data and analytics Data profiling, cataloguing and mapping to enable the design and build of technical data flows Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration Knowledge of multiple Azure data services including Azure Data Bricks and Synapse Experience in preparing data for and building pipelines and architecture within the Azure ecosystem Experience in designing and delivering large-scale Data Platform solutions using relevant approaches (Lakehouse, Data Mesh, Cloud Scale Analytics, etc.) About you Characteristics that can spell success for this role: Experience in business processing mapping of data and analytics solutions Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows Experience of at least one software development methodology Knowledge of Microsoft Data and Analytics technologies Experience managing teams, as well as large-scale and complex programs and projects Enjoy your career Some of the best things about working at Avanade People-first culture that supports innovationand encourages people to move forward Turn your ideas to human impact by cultivating a team that willhelp clients unlock what's next Opportunities to innovate in the Microsoft platform while charting your own career path On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose Real-time access to technical and skilled resources globally Find out more about some of our benefits here. A great place to work As you bring your skills and abilities to Avanade, you'll get distinctive experiences, limitless learning, and ambitious growth in return.As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You'll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise.\n\nYou get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It's all here, so take a closer look! We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees.\n\nVisit our Inclusion & Diversity page. Create a future for our people that focuses on Expanding your thinking Experimenting courageously Learning and pivoting Inspire greatness in our people by Empowering every voice Encouraging boldness Celebrating progress Accelerate the impact of our people by Amazing the client Prioritizing what matters Acting as one Learn more To learn more about the types of projects our Data & AI team works on check out these case studies: thyssenkrupp Materials Services uses data to help strike a delicate operational balance What matters to SSE Renewables is innovating for a sustainable future Hachette UK enables employees with machine learning-driven contract searches Marston Holdingsidentifies significant cloud savings Interested in knowing what's going on inside Avanade? Check out our blogs: Avanade Insights - exchange ideas that drive tomorrow's innovation Inside Avanade - explore what life is like working at Avanade Apply now Share this job: Share Facebook Twitter About Avanade Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem.\n\nEvery day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers. We have been recognized as Microsoft's Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.\n\nWe are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential. Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation.\n\nLearn more at www.avanade.com.", "job_is_remote": false, "job_posted_at_timestamp": 1703101920, "job_posted_at_datetime_utc": "2023-12-20T19:52:00.000Z", "job_city": "Indianapolis", "job_state": "IN", "job_country": "US", "job_latitude": 39.768402, "job_longitude": -86.158066, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=6GrInV0JdnNP8bi9AAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2024-01-19T00:00:00.000Z", "job_offer_expiration_timestamp": 1705622400, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": null, "experience_mentioned": true, "experience_preferred": false}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": false, "degree_preferred": false, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": null, "job_max_salary": null, "job_salary_currency": null, "job_salary_period": null, "job_highlights": {}, "job_job_title": "Engineering manager", "job_posting_language": "en", "job_onet_soc": "11302100", "job_onet_job_zone": "4", "job_occupational_categories": ["15-1141.00: Database Administrators"], "job_naics_code": "541613", "job_naics_name": "Marketing Consulting Services"}, {"employer_name": "Cognizant", "employer_logo": "https://www.cognizant.com/content/dam/cognizant/en_us/dotcom/logos/COG-Logo-2022.svg", "employer_website": "http://www.cognizant.com", "employer_company_type": "Computer Services", "job_publisher": "Nashville, TN - Geebo", "job_id": "DfgsSQKmvdOLqQ2mAAAAAA==", "job_employment_type": "FULLTIME", "job_title": "Azure Data Engineer at Cognizant in Nashville, TN", "job_apply_link": "https://nashville-tn.geebo.com/jobs-online/view/id/1066089664-azure-data-engineer-at-/", "job_apply_is_direct": true, "job_apply_quality_score": 0.4486, "apply_options": [{"publisher": "Nashville, TN - Geebo", "apply_link": "https://nashville-tn.geebo.com/jobs-online/view/id/1066089664-azure-data-engineer-at-/", "is_direct": true}], "job_description": "Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks You must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future Job Title - Azure Data Engineer Location - Hartford, CT (Remote until COVID) Roles/\nResponsibilities:\nDesigning and coding Hadoop applications to analyze data collections. Develop and unit test ETL/ ELT data pipelines using ADF V2 and other components like Azure SQL, Azure SQL DW, ADLS Gen2 etc. Maintain and manage code with Azure DevOps. Implement secure integration with data sources using Azure Key Vault, Managed Identity, Private Links etc. Create data pipeline involving diverse systems such Azure SQL DB, Azure SQL DW, ADLS Gen2, Azure Databricks, On premise compute. Engage with onsite-offshore team for daily activities Status reporting - weekly and monthly basis. Document functional and technical design. Required\nQualifications:\nBachelor's degree in Software Engineering or Computer Science. 2-3 Experience working on ETL/ELT tools. Understanding and experience in Azure and Azure Data ecosystem Good to have programming language experience with .NET/Python or Scala/ Spark. 2-3 years' experience working with ADF. 2-3 years' experience working with Azure Data Lake. 2-3 years' experience working with Azure SQL DB or Azure Synapse. Experience in Machine Learning Studio, Stream Analytics, Event/IoT Hubs, and Cosmos. Nice to have Databricks experience. Nice to have Azure Logic Apps, Azure Blog Storage experience. Nice to have data migration experience from on prem to cloud. Knowledge of ADF Fault Tolerance, load balancing, security controls and distributed processing capabilities. 5-8 Years of work experience in ETL tools from designing and implementing data ingestion and processing pipelines using any big data technologies Minimum 3\nyears of experience in designing and implementing a fully operational solution on Snowflake Data Warehouse Excellent understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies Should be having good presentation and communication skills, both written and verbal Ability to problem solve and able to convert the requirements to design Well versed in RDBMS databases Good communication skill Person must have good data analytical skills Hands on python programming is a plus Good knowledge and extensive experience in SQL Should have experience working with any of the cloud and able to load data from cloud storage\nSalary Range:\n$100K -- $150K\nMinimum Qualification\nData Science & Machine Learning, Systems Architecture & Engineering, Software DevelopmentEstimated Salary: $20 to $28 per hour based on qualifications.", "job_is_remote": false, "job_posted_at_timestamp": 1703116800, "job_posted_at_datetime_utc": "2023-12-21T00:00:00.000Z", "job_city": "Nashville", "job_state": "TN", "job_country": "US", "job_latitude": 36.162663, "job_longitude": -86.7816, "job_benefits": null, "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+jobs+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+jobs+in+usa&htidocid=DfgsSQKmvdOLqQ2mAAAAAA%3D%3D", "job_offer_expiration_datetime_utc": "2023-12-28T00:00:00.000Z", "job_offer_expiration_timestamp": 1703721600, "job_required_experience": {"no_experience_required": false, "required_experience_in_months": 60, "experience_mentioned": true, "experience_preferred": true}, "job_required_skills": null, "job_required_education": {"postgraduate_degree": false, "professional_certification": false, "high_school": false, "associates_degree": false, "bachelors_degree": false, "degree_mentioned": true, "degree_preferred": true, "professional_certification_mentioned": false}, "job_experience_in_place_of_education": false, "job_min_salary": 20, "job_max_salary": 28, "job_salary_currency": "USD", "job_salary_period": "HOUR", "job_highlights": {}, "job_job_title": "Data engineer", "job_posting_language": "en", "job_onet_soc": "15113200", "job_onet_job_zone": "4", "job_naics_code": "541511", "job_naics_name": "Custom Computer Programming Services"}]}